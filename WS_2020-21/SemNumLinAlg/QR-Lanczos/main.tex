\documentclass{article}

\input{../../../Fundament-LaTeX/packages.tex}
\input{../../../Fundament-LaTeX/macros.tex}
\input{../../../Fundament-LaTeX/environments.tex}

\parindent 0pt

\begin{document}
\title
{
	Eigenwertberechnung mithilfe des Lanczos-Verfahrens
}
\author
{
	Göth Christian
	\and
	Moik Matthias
	\and
	Sallinger Christian
}
\date{\today}
\maketitle

\newpage
\tableofcontents
\newpage


\section{Einleitung}

Das folgende Projekt beschäftigt sich mit der numerischen Berechnung von Eigenwerten mithilfe des Lanczos-Verfahrens. Zur Motivation dieser Berechnung betrachten wir das Eigenwertproblem des negativen Laplace-Operators mit homogenene Neumann-Randbedingungen auf einem zweidimensionalen Gebiet $\Omega$.

\subsection{Problemstellung}

Wir wiederholen zunächst die Definition eines Eigenwertproblems.

\begin{definition}
	Das Paar $(\lambda, u) \in \C \times H^2(\Omega) \setminus \{0\}$ heisst ein Eigenpaar, wenn es eine Lösung des Eigenwertproblems
	\begin{align}
	    \begin{cases}
	    -\Delta u = \lambda u & \text{in } \Omega, \\
	    \frac{\partial u}{\partial \nu} = 0 & \text{auf}~ \partial\Omega,
	    \end{cases}
	    \label{neumann}
	\end{align}
	ist, wobei $\nu$ der äußere Normalenvektor an $\partial\Omega$ sei.
\end{definition}

Um die aus dem numerischen Verfahren gewonnen Eigenwerte mit den tatsächlichen Eigenwerten vergleichen zu können, wählen wir als Gebiet das Rechteck $\Omega := (0,a) \times (0,b)$, auf dem sich diese analytisch berechnen lassen.

\subsection{Analytische Lösung für ein Rechteck}

Die Eigenwerte und Eigenfunktionen des oben genannten Problems werden zunächst mit einem Separationsansatz berechnet.

\begin{theorem}
	Die Eigenfunktionen für das Eigenwertproblem \ref{neumann} auf dem Gebiet $\Omega := (0,a) \times (0,b)$ sind gegeben durch
	\begin{align*}
		u_{n,m}(x,y) = \cos(\frac{n \pi}{a}x)\cos(\frac{m \pi}{b}y), \quad n,m \in \N
	\end{align*}
	und die zugehörigen Eigenwerte durch
	\begin{align*}
		\lambda_{n,m} = \pi^2(\frac{n^2}{a^2} + \frac{m^2}{b^2}), \quad n,m \in \N.
	\end{align*}
\end{theorem}

\begin{proof}
	Wir machen einen Separationsansatz. Sei $u(x,y) = v(x)w(y)$, die Differentialgleichung ergibt dann
	\begin{align*}
		- \Delta u(x,y) = -v''(x) w(y) - v(x) w''(y) = \lambda v(x)w(y)
	\end{align*}
	Multiplizieren mit $1/\left(v(x)w(y)\right)$ liefert
	\begin{align*}
		-\frac{v''(x)}{v(x)} - \frac{w''(y)}{w(y)} &= \lambda \\
		\Leftrightarrow \quad -\frac{v''(x)}{v(x)} &= \lambda + \frac{w''(y)}{w(y)}
	\end{align*}
	Da die linke Seite nur von $x$ abhängt und die rechte nur von $y$, müssen beide konstant sein. Es gibt also ein $\kappa_{x}^2$, sodass
	\begin{align*}
		-\frac{v''(x)}{v(x)} = \kappa_{x}^2, \quad \Forall x \in (0,a) \\
		\Rightarrow v''(x) = -\kappa_{x}^2 v(x) \\
		\Rightarrow v(x) = c_1 \sin(\kappa_{x} x) + c_2 \cos(\kappa_{x} x)
	\end{align*}

	Analog erhält man $w(y) = c_3 \sin(\kappa_{y} y) + c_4 \cos(\kappa_{y} y)$, wobei $\lambda = \kappa_{x}^2 + \kappa_{y}^2$ gelten muss.

	Nun lassen wir noch die Neumann-Randbedingungen einfließen. Mit $\nabla u(x,y) = (v'(x)w(y), v(x)w'(y))$ erhalten wir für den linken Rand
	\begin{align*}
		\frac{\partial u}{\partial \nu}(0,y) = - v'(0)w(y) = \kappa_{x}(- c_1 \cos(\kappa_{x} \cdot 0) + c_2 \underbrace{\sin(\kappa_{x} \cdot 0)}_{=0})w(y) \stackrel{!}{=} 0
	\end{align*}
	Es muss also $c_1 = 0$ gelten. Analog schließt man auch $c_3 = 0$.

	Für den rechten Rand gilt nun
	\begin{align*}
		\frac{\partial u}{\partial \nu}(a,y) = v'(a)w(y) = \kappa_{x} c_2 \sin(\kappa_{x} \cdot a)w(y) \stackrel{!}{=} 0 \\
		\Rightarrow \kappa_x = \frac{n \pi}{a}, \quad n \in \N
	\end{align*}

	Wiederum analog schließt man $\kappa_y = \frac{m \pi}{b}, m \in \N$. Durch die Wahl $c_2 = c_4 := 1$ erhalten wir also die Eigenfunktionen $u_{n,m}(x,y) = \cos(\frac{n \pi}{a}x)\cos(\frac{m \pi}{b}y)$ mit Eigenwerten $\lambda_{n,m} = \frac{n^2 \pi^2}{a^2} + \frac{m^2 \pi^2}{b^2}$.
\end{proof}

\subsection{Numerischer Lösungsansatz}

Das Eigenwertproblem \ref{neumann} kann durch Multiplizieren mit $v \in H^1(\Omega)$ und integrieren auf schwache Form gebracht werden (beim Anwenden des Satzes von Gauß verschwindet der Randterm aufgrund der homogenen Randbedingungen). Gesucht sind also Lösungen $(\lambda, u) \in \C \times H^1(\Omega) \setminus \{0\}$ von

\begin{align*}
	\Int[\Omega]{\nabla u \nabla v}{x} = \lambda \Int[\Omega]{uv}{x}, \quad \Forall v \in H^1(\Omega)
\end{align*}

Dieses Problem kann mittels Finite-Elemente-Methoden auf ein endlichdimensionales verallgemeinertes Eigenwertproblem der Form

\begin{align} \label{discretization}
	A x_h = \lambda_h B x_h
\end{align}

gebracht werden, mit $A, B \in \R^{N \times N}$ und symmetrisch.

\begin{lemma}
	Mit $\rho_h \in \R$, welches kein Eigenwert ist, und $\lambda_h = \frac{1}{\mu_h} + \rho_h$ ist das verallgemeinerte Problem \ref{discretization} äquivalent  zu
	\begin{align} \label{equivdiscret}
		(A - \rho_h B)^{-1} B x_h = \mu_h x_h
	\end{align}
\end{lemma}

\begin{proof}
	\begin{align*}
		A x = \lambda B x =& A x = (\frac{1}{\mu} + \rho) B x \\
		\Leftrightarrow (A - \rho B)x =& \frac{1}{\mu} B x \\
		\Leftrightarrow \mu x =& (A - \rho B)^{-1}B x
	\end{align*}
\end{proof}

\begin{remark}
	Die Matrix $(A - \rho_h B)^{-1} B$ im Eigenwertproblem \ref{equivdiscret} ist zwar Produkt zweier symmetrischer Matrizen, aber i. A. nicht mehr symmetrisch. Dies wird im Laufe des Projektes noch wichtig sein, da das Lanczos-Verfahren nur für symmetrische Matrizen funktioniert.
\end{remark}

Die Diskretisierung durch Finite-Elemente-Methoden liefert uns Matrizen $A$ und $B$ mit sehr großer Dimension $N$. Daher ist es sinnvoll, ein Verfahren zu entwickeln, das uns für sehr große Matrizen eine gute Approximation zumindest an die kleinsten Eigenwerte liefert. Das Lanczos-Verfahren wird dies unter Zuhilfenahme des QR-Verfahrens bewerkstelligen.

\section{Das QR-Verfahren}

\subsection{Idee und Algorithmus}

Das QR-Verfahren ist eine Methode zur Eigenwertbestimmung einer Matrix $A \in \K^{n \times n}$ mit verschiedenen Eigenwerten $\lambda_1, ..., \lambda_n$. Dabei wird mithilfe der QR-Zerlegung iterativ eine Folge $(A^{(t)})_{t \in \N}$ definiert. Diese ist durch

\begin{equation}
	A^{(0)} \coloneqq A, \quad A^{(t+1)} \coloneqq R^{(t)}Q^{(t)}
\end{equation}

gegeben. Die Folge $(A^{(t)})_{t \in \N}$ konvergiert dann gegen eine obere Dreiecksmatrix, die die gleichen Eigenwerte wie $A$ besitzt.

\floatname{algorithm}{Algorithmus}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}

	\caption{QR-Verfahren}
	\label{QR-Verfahren}
	\algorithmicrequire{ $A \in \K^{n\times n}$}
	\begin{algorithmic}[1]
		\While{Abbruchbedingung nicht erfüllt}
		\State Berechne $A = QR$
		\State $A \coloneqq RQ$
		\EndWhile
	\end{algorithmic}
	\algorithmicensure{ $A$ als Approximation einer oberen Dreiecksmatrix mit gleichen Eigenwerten wie $A$ anfangs hatte}
\end{algorithm}


Die Konvergenz des Algoritmus kann gilt unter gewissen Voraussetzungen. Wir Zitieren hier nur einen Satz und verweisen für den Beweis auf \cite{Nannen-Skript}, Satz 9.15.

\begin{theorem}[Konvergenz des QR-Verfahrens]
	\label{Konv_QR}
	Sei $A \in \K^{n\times n}$ eine Matrix mit Eigenwerten $\lambda_1,\dots,\lambda_n \in \K$ und $|\lambda_1| > |\lambda_2| > \dots > |\lambda_n| > 0$. Dann konvergieren die Hauptdiagonaleinträge der Matrizen $A^{(t)}$, die in Algorithmus \ref{QR-Verfahren} definiert sind, gegen die Eigenwerte von $A$.
\end{theorem}


\subsection{Beschleunigung der Konvergenz}

Wir betrachten die Matrizenfolge $(M^{(t)})_{t \in \N}$, die im Beweis von Satz \ref{Konv_QR} vorkommt. Diese haben die Form

\begin{equation*}
	M^{(t)} =
	\left( \begin{array}{rrrr}
		1 & 0 & \dots & 0 \\
		m_{21} \left(\frac{\lambda_2}{\lambda_1}\right)^t & 1 & \ddots \vdots \\
		\vdots & \ddots & \ddots & 0 \\
		m_{n1} \left(\frac{\lambda_n}{\lambda_1}\right)^t & \dots & m_{n(n-1)} \left(\frac{\lambda_n}{\lambda_{n-1}}\right)^t & 1 \\
	\end{array}\right).
\end{equation*}

Da die Eigenwerte der größe nach sortiert sind, erkennt man, dass diese Matrix gegen die Einheitsmatrix konvergiert. Dies zieht schlussendlich die Konvergenz von $(A^{(t)})_{t\in \N}$ gegen eine obere Dreiecksmatrix nach sich.
Die Einträge konvergieren schneller gegen $0$, wenn die Brüche kleiner sind. Das versuchen wir zu erreichen, in dem wir einen Shift durchführen.\\
Für einen Eigenwert $\lambda$ einer Matrix $A$ gilt

\begin{equation*}
	(A - \lambda \id) = 0 \Leftrightarrow ((A - \rho \id) - (\lambda -\rho) \id) = 0 \Leftrightarrow (\lambda-\rho) \text{ ist Eigenwert von }  (A - \rho \id).
\end{equation*}

Die Wahl des Shifts als $a^{(t)}_{nn}$ macht also Sinn, da dieser Eintrag für $t \rightarrow \infty$ gegen $\lambda_n$ konvergiert. Dieser Shift wird auch Rayleigh-Quotienten-Shift genannt. Nach der QR-Zerlegung von $A^{(t)}$ setzt man $A^{(t+1)} \coloneqq R^{(t)}Q^{(t)} + \rho \id$, um wieder eine Matrix zu erhalten, die die gleichen Eigenwerte wie $A^{(t)}$ und somit auch wie $A$ besitzt.
Je nachdem, wie genau $\lambda_n$ approximiert werden soll, wählt man eine Toleranz, die von den Einträgen $a^{(t)}_{ni}, \, i = 1,\dots, n-1$ unterschritten werden muss, bevor analoge Schritte für den zweitkleinsten Eigenwert durchgeführt werden. Dessen Näherung steht in $a^{(t)}_{(n-1)(n-1)}$.

\floatname{algorithm}{Algorithmus}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}

	\caption{QR-Verfahren mit Rayleigh-Quotienten-Shift}
	\label{QR-Verfahren_shifts_rayleigh}
	\algorithmicrequire{ $A \in \K^{n\times n}$}
	\begin{algorithmic}[1]
		\For{$i = n,\dots, 2$}
			\While{$|a_{i,i-1}| > tol$}
				\State $\rho = a_{i,i}$
				\State Berechne $A - \rho \id = QR$
				\State $A = RQ + \rho \id$
			\EndWhile
		\EndFor
	\end{algorithmic}
	\algorithmicensure{ $A$ als Approximation einer oberen Dreiecksmatrix mit gleichen Eigenwerten wie $A$ anfangs hatte}
\end{algorithm}

Eine weitere Möglichkeit, den Shift zu wählen, ist sich die Eigenwerte $\rho_1, \rho_2$ der $2 \times 2$ Matrix rechts unten von $A^{(t)}$ zu berechnen und anschließend den Eigenwert, der betragsmäßig näher an $a^{(t)}_{nn}$ liegt, zu wählen. Die Konvergenzgeschwindigkeit wird dadurch nochmals verbessert. Vergleiche dazu \cite{Nannen-Skript}. Dieser Shift wird Wilkinson-Shift genannt.

In unserer Implementierung des QR-Verfahrens verwenden wir die letzte angeführte Methode.

\begin{algorithm}
	\label{QR-Verfahren_shifts}
	\caption{Verbessertes QR-Verfahren}
	\algorithmicrequire{ $A \in \K^{n\times n}$}
	\begin{algorithmic}[1]
		\For{$i = n,\dots, 2$}
		\While{Abbruchbedingung nicht erfüllt}
		\State Berechne die Eigenwerte $\rho_1$, $\rho_2$ von $\left(\begin{array}{rr}
			a&b\\
			c&d\\
		\end{array}\right)$
		\If{$|\rho_1 - a_{ii}| < |\rho_2 - a_{ii}|$}
		\State $\rho = \rho_1$
		\Else
		\State $\rho = \rho_2$
		\EndIf
		\State Berechne $A - \rho \id = QR$
		\State $A = RQ + \rho \id$
		\EndWhile
		\EndFor
	\end{algorithmic}
	\algorithmicensure{ $A$ als Approximation einer oberen Dreiecksmatrix mit gleichen Eigenwerten wie $A$ anfangs hatte}
\end{algorithm}

Eine weitere Beschleunigung erreichen wir bei der QR-Zerlegung, wenn die Matrix $A$ Hessenbergform hat.

\subsection{Die QR-Zerlegung für Hessenbergmatrizen}
\begin{algorithm}
	\label{QR-Zerlegung_Hessenberg}
	\caption{QR-Zerlegung für Hessenbergmatrizen}
	\algorithmicrequire{ $A \in \K^{n\times n}$}
	\begin{algorithmic}[1]
		\State $Q = \id \in \K^{n\times n}$
		\For{$i = 1,\dots,n-1$}
		\State $\left(\begin{array}{c}
    c_i \\
    s_i \\
\end{array}\right)
=
\frac{1}{\sqrt{|a_{i,i}|^2 + |a_{i+1,i}|^2}}
\left(\begin{array}{c}
a_{i,i} \\
a_{i+1,i} \\
\end{array}\right)$
		\For{$j = i,\dots,n$}
		\State $\left(\begin{array}{c}
		a_{i,j} \\
		a_{i+1,j}\\
		\end{array}\right)
		=
		\left(\begin{array}{cc}
		\overline{c_i} & \overline{s_i} \\
		-s_i & c_i \\
		\end{array}
		\right)
		\left(\begin{array}{c}
		a_{i,j} \\
		a_{i+1,j}\\
		\end{array}\right)$
		\EndFor
		\For{$i= 1,\dots,n$}
		\State $\left(\begin{array}{c}
		q_{i,j} \\
		q_{i+1,j}\\
		\end{array}\right)
		=
		\left(\begin{array}{cc}
		\overline{c_i} & \overline{s_i} \\
		-s_i & c_i \\
		\end{array}
		\right)
		\left(\begin{array}{c}
		q_{i,j} \\
		q_{i+1,j}\\
		\end{array}\right)$
		\EndFor
		\EndFor
	\end{algorithmic}
	\algorithmicensure{ $A$ wird mit oberer Dreiecksmatrix (=R) überschrieben, Q ist das Produkt der Givens-Rotationen und $\overline{Q}^\top$ ist die gewünschte unitäre Matrix}
\end{algorithm}
\subsection{Implementierung und Vergleich der verschiedenen Methoden}


\section{Das Lanczos-Verfahren}

Wie in der Einleitung erwähnt, liefert uns das dort beschriebene Eigenwertproblem Matrizen mit sehr großer Dimension. Außerdem ist man in der Praxis häufig nur an wenigen extremalen Eigenwerten interessiert. Das QR-Verfahren ist für solche Probleme also nicht geeignet. Im Folgenden werden wir deshalb das Lanczos-Verfahren kennenlernen.

\subsection{Idee}

Wir zeigen zunächst ein theoretisches Resultat, welches eine Darstellung der Eigenwerte für hermitesche Matrizen liefert. Man sieht leicht, dass es sich hierbei um eine Verallgemeinerung des Satzes von Rayleigh handelt.

\begin{lemma}
	Sei $A \in \C^{N \times N}$ hermitesch bezüglich des Skalarproduktes $(\cdot, \cdot)$, $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_N$ die Eigenwerte von $A$ (gemäß Vielfachheit gezählt) und $u_1, \dots, u_N$ die zugehörigen normierten Eigenvektoren. Dann gilt
	\begin{align}\label{lambdamax}
		\lambda_1 = \max_{v \in \C^N \setminus \{0\}} \frac{(Av,v)}{(v,v)}, \quad \lambda_k = \max_{\substack{v \in \C^N \setminus \{0\}\\
    (u_j,v) = 0, j= 1,\dots, k-1}} \frac{(Av,v)}{(v,v)}, \quad k= 2, \dots, N
	\end{align}
	und
	\begin{align}\label{lambdamaxmin}
		\lambda_k = \max_{\substack{S \subset \C^N\\
    \dim S = k}} \min_{v \in S \setminus \{0\}} \frac{(Av,v)}{(v,v)}.
	\end{align}

	Für $v \neq 0$ nennt man $\frac{(Av,v)}{(v,v)}$ den Rayleigh-Quotienten von $v$.
\end{lemma}

\begin{proof}
	Sei $v \in \C^N \setminus \{0\}$ beliebig. Da $u_1, \dots, u_N$ eine Orthonormalbasis des $\C^N$ bilden, gilt $ v = \sum_{j=1}^{N} (u_j,v) u_j$ und damit
	\begin{align*}
		(Av,v) = \sum_{j=1}^{N} (u_j,v) (\lambda_j u_j,v) = \sum_{j=1}^{N} \lambda_j |(u_j,v)|^2 \leq \lambda_1 \sum_{j=1}^{N} |(u_j,v)|^2 = \lambda_1 (v,v).
	\end{align*}
	Dividieren durch $(v,v)$ liefert $\lambda_1 \geq \frac{(Av,v)}{(v,v)}$ für alle $v \in \C^N \setminus \{0\}$ und für $v = u_1$ gilt sogar Gleichheit.

	Für ein anderes $k = 2,\dots,N$ können wir analog ein beliebiges $v \in \C^N \setminus \{0\}$ wählen, nun aber mit der Bedingung $(u_j,v) = 0, j=1,\dots,k$ betrachten. In der Summe $\sum_{j=1}^{N} \lambda_j |(u_j,v)|^2$ fallen somit die ersten $k-1$ Summanden weg und wir können mit $\lambda_k$ nach oben abschätzen. Gleichheit gilt für den jeweiligen Eigenvektor $u_k$.

	Für die Gleichheit in \ref{lambdamaxmin} gilt ähnlich zu oben für $v \in \tilde{S} := \textbf{span}\{v_1, \dots, v_k\}$, dass
	\begin{align*}
		(Av,v) = \sum_{j=1}^{k} \lambda_j |(v,u_j)|^2 \geq \lambda_k (v,v)
	\end{align*}

	und daraus $\lambda_k \leq \min_{v \in \tilde{S}} \frac{(Av,v)}{(v,v)}$. Für $v = u_k$ gilt wiederum Gleichheit.

	Betrachten wir nun einen anderen Unterraum $S \neq \tilde{S}$ mit $\dim S = k$. Dann gibt es ein $v_0 \in S \setminus \{0\}$ mit $(v_0,u_j) = 0$ für $j=1,\dots,k$. Für dieses $v_0$ gilt $(Av_0,v_0) \leq \lambda_k (v_0,v_0)$. Das heißt, für beliebige solche $S$ ist $\min_{v \in S \setminus \{0\}} \frac{(Av,v)}{(v,v)} \leq \lambda_k$, womit die Gleichheit gezeigt ist.
\end{proof}

Wir wollen das Verfahren der Vektoriteration zur Bestimmung des größten Eigenwertes einer hermiteschen Matrix wiederholen. Diese erzeugt ausgehend von einem Startvektor $x^{(0)} \in \K^{n}$ eine Folge von Iterierten $x^{(m)} := \frac{Ax^{(m-1)}}{\|Ax^{(m-1)}\|}$ für $m \in \N$. Definiert man die Approximationen $\lambda^{(m)}$ an den größten Eigenwert $\lambda_1$ durch den Rayleigh-Quotienten von $x^{(m-1)}$, so konvergiert $\lambda^{(m)}$ quadratisch gegen $\lambda_1$. (vgl. Nannen-Skript)

\begin{definition}
	Sei $v_0 \in \K^{N}$ und $A \in \K^{N \times N}$. Dann bezeichnet
	\begin{align*}
		\mathcal{K}_m(A,v_0) := \textbf{span}\{v_0, Av_0, \dots, A^{m-1}v_0\}, \quad m \in \N
	\end{align*}
	den Krylov-Raum von $A$ und $v_0$. Es bezeichne $\mathcal{P}_m: \K^N \to \mathcal{K}_m$ die orthogonale Projektion auf $\mathcal{K}_m$.
\end{definition}

Beim Lanczos-Verfahren wird nun als Näherung an den größten Eigenwert das Maximum des Rayleigh-Quotienten über dem Krylov-Raum
\begin{align*}
	\mathcal{K}_m(A,x^{(0)}) = \textbf{span}\{x^{(0)}, x^{(1)}, \dots , x^{(m-1)} \}
\end{align*}
verwendet, also

\begin{align*}
	\lambda_1^{(m)} := \max_{x \in \mathcal{K}_m \setminus \{0\}} \frac{(Ax,x)}{(x,x)}.
\end{align*}

Da $x^{(t-1)} \in \mathcal{K}_m$, gilt
\begin{align*}
	\lambda^{(m)} \leq \lambda_1^{(m)} \leq \lambda_1
\end{align*}

und $\lambda_1^{(m)}$ ist somit eine mindestens so gute Approximation von $\lambda_1$ wie $\lambda^{(m)}$.

Betrachten wir die Abbildung

\begin{align*}
	\mathcal{A}_m: \mathcal{K}_m &\to \mathcal{K}_m \\
	x &\mapsto \mathcal{P}_m Ax,
\end{align*}

so sehen wir, dass diese selbstadjungiert ist. Es gilt für $x,y \in \mathcal{K}_m$

\begin{align*}
	(\mathcal{A}_mx, y) = (\mathcal{A}_m\mathcal{P}_m x, y) = x^*\mathcal{P}_m A \mathcal{P}_m y = (x, \mathcal{A}_m y),
\end{align*}

da $\mathcal{P}_m$ und $A$ hermitesch sind. Also besitzt $ \mathcal{A}_m$ nur reelle Eigenwerte $\lambda_1(\mathcal{A}_m) \geq \lambda_2(\mathcal{A}_m) \geq \dots \geq \lambda_m(\mathcal{A}_m)$. Für den größten Eigenwert gilt

\begin{align*}
	\lambda_1(\mathcal{A}_m) = \max_{x \in \mathcal{K}_m \setminus \{0\}} \frac{(x, \mathcal{A}_m x)}{(x,x)} = \max_{x \in \mathcal{K}_m \setminus \{0\}} \frac{(\mathcal{P}_m x, A x)}{(x,x)} = \max_{x \in \mathcal{K}_m \setminus \{0\}} \frac{(x, A x)}{(x,x)} = \lambda_1^{(m)}.
\end{align*}

Der größte Eigenwert $\lambda_1(\mathcal{A}_m)$ von $\mathcal{A}_m$ ist also nach unseren Überlegungen in ref*** eine gute Approximation an den größten Eigenwert $\lambda_1$ von $A$. Es liegt nahe, für alle $j \leq m$ auch $\lambda_j(\mathcal{A}_m)$ als Approximation von $\lambda_j$ zu betrachten. Dass dies eine gute Wahl ist, werden wir im übernächsten Unterabschnitt zeigen.

\subsection{Herleitung des Verfahrens}

Wie wir eben gesehen haben, ist es also Ziel, die Eigenwerte der Abbildung $\mathcal{A}_m$ zu berechnen. Hierfür benötigen wir eine Repräsentation von $\mathcal{A}_m$ durch eine Matrix $T_m$ bezüglich einer Orthonormalbasis $\{v_0, \dots, v_{m-1}\}$ des Krylov-Raums. Für die Einträge von $T_m$ gilt
\begin{align*}
	(T_m)_{ij} &= (v_i, \mathcal{A}_m v_j) = v_i^*Av_j \\
	\Rightarrow T_m &= V_m^*AV_m.
\end{align*}

Der Vorteil dieser Darstellung ist, dass die Eigenwerte von $T_m$ jenen von $\mathcal{A}_m$ entsprechen. Wenn wir $m$ nicht zu groß gewählt haben, können wir diese mit dem aus Kapitel 2 bekannten QR-Verfahren mit wenig Aufwand berechnen.

Eine Möglichkeit zur Bestimmung einer Orthonormalbasis liefert das CG-Verfahren, auf welches an dieser Stelle nicht genauer eingegangen werden soll. Aus diesem ergibt sich der Ansatz, $T_m$ als eine Tridiagonalmatrix (also auch Hessenbergmatrix) der Form

\begin{align}\label{trimatrix}
\begin{pmatrix}
	\gamma_0 & \delta_0 & 0 & \hdots & 0 \\
	\delta_0 & \gamma_1 & \delta_1 &  & \vdots \\
	0 & \delta_1 & \ddots & \ddots & 0 \\
	\vdots &  & \ddots & \ddots & \delta_{m-2} \\
	0 & \hdots & 0 & \delta_{m-2} & \gamma_{m-1}
\end{pmatrix}
\end{align}

aufzustellen. Die Gleichung **ref** schreiben wir um als $AV_m = V_m T_m$, was spaltenweise zu folgenden Gleichungen führt:
\begin{align*}
	A v_0 &= \gamma_0 v_0 + \delta_0 v_1 \\
	A v_j &= \delta_{j-1} v_{j-1} + \gamma_j v_j + \delta_j v_{j+1}
\end{align*}

Da es sich bei $\{v_0, \dots, v_{m-1}\}$ um ein Orthogonalsystem handelt, liefert Multiplizieren der $j$-ten Gleichung von links mit $v_j^*$, dass $\gamma_j = v_j^*Av_j$ für $j = 0, \dots, m-2$. Somit können wir die $j$-te Gleichung rekursiv nach $v_{j+1}$ auflösen:

\begin{align*}
	v_{j+1} = \begin{cases}
		\frac{\overbrace{(A-\gamma_0)v_0}^{=:w_0}}{\delta_0} &,j = 0 \\
		\frac{\overbrace{(A-\gamma_j)v_j - \delta_{j-1} v_{j-1}}^{=:w_j}}{\delta_j} &,j \geq 1
	\end{cases}
\end{align*}

Hierbei setzen wir $\delta_j := \|w_j\|$ falls $\|w_j\| \neq 0$. Andernfalls bricht die Iteration ab. Daraus ergibt sich nun der Algorithmus für das Lanczos-Verfahren.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}
	\label{Lanczos-Verfahren}
	\caption{Lanczos-Verfahren}
	\algorithmicrequire{ $A \in \K^{n\times n}$ hermitesche Matrix, zufälliger normierter Startvektor $v_0 \in \K^n$}
	\begin{algorithmic}[1]
		\State $\gamma_0 = v_0^*Av_0$
		\State $w_0 = (A-\gamma_0 I)v_0$
	  \State $\delta_0 = \|w_0\|_2$
		\While{$\delta_j \neq 0$}
		\State $v_{j+1} = w_j/\delta_j$
		\State $j = j+1$
		\State $\gamma_j = v_j^*Av_j$
		\State $w_j = (A-\gamma_j I)v_j - \delta_{j-1}v_{j-1}$
		\State $\delta_j = \|w_j\|_2$
		\EndWhile
		\State Berechne mittels QR-Verfahren Eigenwerte von $T_j \in \K^{(j+1) \times (j+1)}$ aus \ref{trimatrix} (Hessenbergmatrix)
	\end{algorithmic}
	\algorithmicensure{Approximation an die Eigenwerte von $\mathcal{A}_j$}
\end{algorithm}


\subsection{Konvergenz der Eigenwerte von hermiteschen Matrizen}

In diesem Abschnitt beweisen wir, dass die durch das Lanczos-Verfahren erhaltenen Eigenwerte $\lambda_1^{(m)},\dots,\lambda_m^{(m)}$ gegen Eigenwerte $\lambda_1,\dots,\lambda_m$ der hermiteschen Matrix $A \in \K^{n \times n}$ konvergieren.

Der Argumentation ist dabei in großen Teilen angelehnt an \cite{Eigenwertprobleme}, Abschnitt 4.1.

Wir werden ein Lemma benötigen, welches Elemente des Krylov-Raums in Zusammenhang mit Polynomen charakterisiert.

\begin{lemma}
	\label{v0_Polynom_Darstellung}
	Sei $\Pi_m$ der Raum der Polynome in einer Veränderlichen mit maximalem Grad $m$. Dann ist $v \in \mathcal{K}_m(A,v_0) \subset \K^N$ genau dann, wenn ein Polynom $p \in \Pi_{m-1}$ existiert mit $v = p(A)v_0$. \newline
	Ist $A$ diagonalisierbar mit Eigenwerten $\lambda_1, \dots, \lambda_N$ und zugehörigen Eigenvektoren $u_1, \dots, u_N$, dann existiert eine eindeutige Darstellung $v_0 = \sum_{j=1}^{N} \alpha_j u_j$ und es gilt
	\begin{align*}
		v \in \mathcal{K}_m \Leftrightarrow \Exists p \in \Pi_{m-1}: v = \sum_{j=1}^{N} p(\lambda_j)\alpha_j u_j.
	\end{align*}
\end{lemma}

\begin{proof}
	Wir zeigen zunächst die erste Aussage. Nach Definition des Krylov-Raums gilt
	\begin{align}
		v \in \mathcal{K}_m \Leftrightarrow& \Exists a_0, \dots a_{m-1} \in \K: v = \sum_{i=0}^{m-1} a_i C^i v_0 = (\sum_{i=0}^{m-1} a_i C^i)v_0\\
		\Leftrightarrow& \Exists p(x) = \sum_{i=0}^{m-1} a_i x^i \in \Pi_{m-1}: v = p(C)v_0.
		\label{eq:Krylov}
	\end{align}

	Da die Eigenvektoren eine Basis des $\K^N$ bilden, folgt die eindeutige Darstellung von $v_0$. Setzen wir diese in die erste Zeile von \ref{eq:Krylov} ein und verwenden die Tatsache, dass $C^i u_j = \lambda_j^i u_j$ für $j=1,\dots, N $ und $i = 0, \dots, m-1$, erhalten wir auch die zweite Aussage.
\end{proof}


\begin{definition}
	Für $m \in \N$ sind die Chebyshev-Polynome $T_m \in \Pi_m$ definiert druch
	\begin{align}\label{chebyshev}
		T_m(x) := \frac{1}{2}((x+ \sqrt{x^2 - 1})^m + (x- \sqrt{x^2 - 1})^m), \quad x \in \R.
	\end{align}
\end{definition}

Es gibt weitere Definitionen der Chebyshev-Polynome. Die Bekannteste ist
\begin{align}\label{chebyalt}
	T_m(x) := \cos(m \arccos x), \quad x \in [-1,1],
\end{align}

welche aus der Identität $T_m(\cos \varphi) = \cos (m \varphi)$ für $\varphi \in \R$ folgt.

Aus dieser Darstellung folgt auch mittels Additionstheorem die 2-Term-Rekursion

\begin{align*}
	T_0(x) = 1, \quad T_1(x) = x, \quad T_{n}(x) = 2x T_{n-1}(x) - T_{n-2}(x), \quad n \geq 2,
\end{align*}

welche zeigt, dass es sich bei $T_m$ tatsächlich um ein Polynom vom Grad $m$ handelt.

\begin{lemma}
	\label{lem:polminmax}
	Sei $[a,b]$ ein nicht-leeres Intervall in $\R$ und sei $c \geq b$. Dann gilt mit $\gamma := 1 + 2 \frac{c-b}{b-a} > 0$
	\begin{align}\label{polminmax}
		\min_{\substack{p \in \Pi_m \\
		p(c) = 1}} \max_{x \in [a,b]} |p(x)| \leq \frac{1}{|T_m(\gamma)|} \leq 2 (\gamma + \sqrt{\gamma^2 -1})^{-m}.
	\end{align}
\end{lemma}

\begin{proof}
	Wir verwenden die affin lineare Abbildung $\Phi: [a,b] \to [-1,1]$ mit
	\begin{align*}
		\Phi(x) = 1 + 2 \frac{x-b}{b-a}, \quad x \in [a,b].
	\end{align*}
	Wir definieren
	\begin{align*}
		\hat{p} := \frac{T_m \circ \Phi}{|T_m(\Phi(c))|}
	\end{align*}
	und können die Darstellung von $T_m$ aus \ref{chebyalt} verwenden (da $\Phi$ nach $[-1,1]$ abbildet), aus der
	\begin{align*}
		\max_{x \in [a,b]}|\hat{p}(x)| \leq \frac{1}{|T_m(\gamma)|}
	\end{align*}
	folgt und daraus auch die erste Ungleichung.

	Die zweite Ungleichung folgt aus der Definition der Chebyshev-Polynome \ref{chebyshev} und der Tatsache, dass $\gamma \geq 1$ und deshalb $|T_m(\gamma)| = T_m(\gamma) \geq \frac{1}{2} (\gamma+ \sqrt{\gamma^2-1})^m$ gilt.
\end{proof}

Mit den bis hier getroffenen Vorbereitungen können wir nun die Konvergenz des Projektionsverfahrens nachweisen.

\begin{theorem}[Konvergenz der Eigenwerte hermitscher Matrizen]
	Sei $A \in \K^{n \times n}$ eine hermitesche Matrix mit paarweise verschiedenen Eigenwerten $\lambda_1 > \lambda_2 > \dots > \lambda_n$ und der zugehörigen Orthonormalbasis aus Eigenvektoren $u_1,\dots,u_n$. Für $1 \le m < n$ werden die Eigenwerte der linearen Abbildung $\mathcal{A}_m: \mathcal{K}_m(A,v_0)\rightarrow \mathcal{K}_m(A,v_0)$, die durch $v \rightarrowtail \mathcal{P}_mAv$ gegeben ist,  mit $\lambda_1^{(m)} \ge \lambda_2^{(m)} \ge \dots \ge \lambda_m^{(m)}$ bezeichnet. Dabei ist $v_0$ ein beliebiger Startvektor, der nicht orthogonal zu den ersten $m-1$ Eigenvektoren von $A$ ist. Dann gilt

	\begin{equation}
		\label{konvergenz Eigenwerte}
		0 \le \lambda_i - \lambda_i^{(m)} \le (\lambda_i -\lambda_n) (\tan\theta_i)^2 \kappa_i^{(m)} \left(\frac{1}{T_{m-i}(\gamma_i)}\right)^2, \quad i=1,\dots,m-1
	\end{equation}

	wobei
	\begin{equation*}
		\tan \theta_i \coloneqq \frac{\norm{(\id - \mathcal{P}_{u_i})v_0}}{\norm{ \mathcal{P}_{u_i}v_0}}, \quad \gamma_i \coloneqq 1+2 \frac{\lambda_i-\lambda_{i+1}}{\lambda_{i+1} -\lambda_n}
	\end{equation*}

	und
	\begin{equation*}
		\kappa_1^{(m)} \coloneqq 1, \quad \kappa_i^{(m)} \coloneqq \left(\prod_{j=1}^{i-1} \frac{\lambda_j^{(m)} - \lambda_n}{\lambda_j^{(m)} - \lambda_i}\right)^2, \quad i = 2,\dots,m.
	\end{equation*}

	\begin{proof}

	Seien $v_1, \dots, v_m$ Orthonormalbasisvektoren von $\mathcal{K}_m$ und $V \in \K^{n\times m}$ die Matrix, die diese Vektoren als Spalten besitzt. Dann ist

	\begin{equation*}
		T_m \coloneqq V^* A V \in \K^{m\times m}
	\end{equation*}

	die Matrixdarstellung der Abbildung $\mathcal{A}_m$ bezüglich der Orthonormalbasis $\{v_1,\dots,v_m\}$.

	Diese Matrix ist offensichtlich hermitesch. Somit können die Eigenwerte $\lambda_1^{(m)}, \dots, \lambda_m^{(m)}$ mithilfe von \ref{lambdamaxmin} berechnet werden.

	Es gilt also

	\begin{equation}
		\label{Eigenwert_Ungl}
		\begin{aligned}
			\lambda_k^{(m)} &= \max_{\substack{S \subseteq \C^m\\ \dim S = k}} \min_{v\in S\setminus\{0\}} \frac{(T_m v,v)}{(v,v)} \\
			&= \max_{\substack{S \subseteq \C^m\\ \dim S = k}} \min_{v\in S\setminus\{0\}} \frac{(AVv,Vv)}{(Vv,Vv)}\\
			&= \max_{\substack{S \subseteq \mathcal{K}_m\\ \dim S = k}} \min_{w\in S\setminus\{0\}} \frac{(Aw,w)}{(w,w)}\\
			 &\le \max_{\substack{S \subseteq \C^n\\ \dim S = k}} \min_{w\in S\setminus\{0\}} \frac{(Aw,w)}{(w,w)} = \lambda_k
		\end{aligned}
	\end{equation}

	Somit ist die erste Ungleichung erfüllt.\\

	Wir beweisen nun die zweite Ungleichung zuerst für den Fall $i = 1$.
	Sei $v_0 = \sum_{j=1}^{n}\alpha_j u_j$. Wir bemerken, dass $\alpha_1, \dots, \alpha_{m-1}$ nicht verschwinden, da $v_0$ nicht orthogonal zu diesen Eigenvektoren gewählt wurde.\\

	Mit analogen Umformungen wie in \ref{Eigenwert_Ungl}, dem Satz von Pythagoras und Lemma \ref{v0_Polynom_Darstellung} gilt somit
	\begin{equation}
		\label{Ungl i=1}
		\begin{aligned}
			\lambda_1 - \lambda_1^{(m)} &= \lambda_1 - \max_{v \in \mathcal{K}_m \setminus\{0\}} \frac{(Av,v)}{(v,v)} = \min_{v \in \mathcal{K}_m\setminus\{0\}} \lambda_1 - \frac{(Av,v)}{(v,v)} = \min_{v \in \mathcal{K}_m\setminus\{0\}} \frac{\lambda_1(v,v) - (Av,v)}{(v,v)} \\ &= \min_{p\in \Pi_{m-1}\setminus \{0\}} \frac{\lambda_1 \norm{\sum_{j=1}^{n}p(\lambda_j)\alpha_j u_j}^2 - (\sum_{j=1}^{n}p(\lambda_j)\alpha_j A u_j,\sum_{j=1}^{n}p(\lambda_j)\alpha_j u_j)}{\norm{\sum_{j=1}^{n}p(\lambda_j)\alpha_j u_j}^2} \\
			&= \min_{p\in \Pi_{m-1}\setminus \{0\}} \frac{\lambda_1 \sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2 - \sum_{j=1}^{n} \lambda_j |p(\lambda_j)\alpha_j|^2 }{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			&=  \min_{p\in \Pi_{m-1}\setminus \{0\}} \frac{\sum_{j=2}^{n} (\lambda_1 - \lambda_j)|p(\lambda_j)\alpha_j|^2}{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			&\le (\lambda_1 - \lambda_n) \frac{\sum_{j=2}^{n} |\alpha_j|^2}{|\alpha_1|^2} \min_{p\in \Pi_{m-1}\setminus \{0\}} \max_{j=2,\dots,n} \left(\frac{|p(\lambda_j)|}{|p(\lambda_1)|}\right)^2.
		\end{aligned}
	\end{equation}

	Wir bezeichnen mit $\mathcal{P}_{u_i}, \, i = 1,\dots, m-1$ die Projektion auf den Vektor $u_i$. Damit gilt
		\begin{equation*}
			(\tan\theta_1)^2 = \frac{\norm{(\id - \mathcal{P}_{u_1})v_0}^2}{\norm{ \mathcal{P}_{u_1}v_0}^2} = \frac{\norm{v_0 - \alpha_1 u_1}^2}{\norm{\alpha_1 u_1}^2} = \frac{\sum_{j=2}^{n} |\alpha_j|^2}{|\alpha_1|^2}
		\end{equation*}
	und mithilfe der Ungleichung \ref{polminmax} mit $a = \lambda_n, b= \lambda_2$ und $c= \lambda_1$

		\begin{equation*}
			\min_{p\in \Pi_{m-1}\setminus \{0\}} \max_{j=2,\dots,n} 		\left(\frac{|p(\lambda_j)|}{|p(\lambda_1)|}\right)^2 \le \min_{p\in \Pi_{m-1}\setminus \{0\}} \max_{\lambda \in [a,b]} \left(\frac{|p(\lambda)|}{|p(\lambda_1)|}\right)^2 \le \left(\frac{1}{T_{m-1}(\gamma_1)}\right)^2.
		\end{equation*}
	Daraus folgt die gewünschte Ungleichung und \ref{konvergenz Eigenwerte} ist für $i = 1$ erfüllt.

	Betrachten wir nun den Fall $i>1$.
	Bezeichne mit $u_j^{(m)}, \, j = 1,\dots, m$ die Eigenvektoren zu den Eigenwerten $\lambda_j^{(m)} , \, j = 1,\dots, m$. Mit \ref{lambdamax} erhalten wir dann

	\begin{equation*}
		\lambda_i^{(m)} = \max_{\substack{v\in \C^n\setminus \{0\} \\ (u_j^{(m)},v) = 0, j = 1,\dots,i-1}} \frac{(A^{(m)}v,v)}{(v,v)} = \max_{\substack{v\in \C^n\setminus \{0\} \\ (u_j^{(m)},v) = 0, j = 1,\dots,i-1}} \frac{(AVv,Vv)}{(Vv,Vv)} = \max_{\substack{w\in \mathcal{K}_m\setminus \{0\} \\ (Vu_j^{(m)},w) = 0, j = 1,\dots,i-1}} \frac{(Aw,w)}{(w,w)}
	\end{equation*}


	Definiere  $\tilde{u}_j^{(m)}$ als den normierten Vektor $Vu_j$. Da $A^{(m)}$ hermitesch ist, gilt für alle $i,j = 1,\dots, m$

	\begin{equation*}
		\lambda^{(m)}_i (u_i^{(m)}, u_j^{(m)}) = (A^{(m)}u_i^{(m)}, u_j^{(m)}) = (u_i^{(m)}, A^{(m)}u_j^{(m)}) = \lambda^{(m)}_j (u_i^{(m)}, u_j^{(m)}).
	\end{equation*}

	Also sind Eigenvektoren zu verschiedenen Eigenwerten orthogonal. Wegen $ 0 = (u_i, u_j) = (Vu_i, Vu_j)$ sind auch die $\tilde{u}_j^{(m)}$ orthogonal und somit eine Basis von $\mathcal{K}_m$. Sei nun $w\in \mathcal{K}_m$ gegeben durch $w = \sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k \tilde{u}_k^{(m)}$. Dann gilt für alle $j < i$
	\begin{equation*}
		0 \stackrel{!}{=} (w,\tilde{u}_j^{(m)}) = (\sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k \tilde{u}_k^{(m)},\tilde{u}_j^{(m)}) = \sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k (\tilde{u}_k^{(m)},\tilde{u}_j^{(m)}) = p(\lambda_j^{(m)}) \beta_j.
	\end{equation*}

	Es gibt also zwei Möglichkeiten, diese Gleichheit zu erfüllen. Entweder ist $p(\lambda_j^{(m)}) = 0$ oder $\beta_j = 0$. Wir betrachten zuerst den Fall, dass  $\beta_j \neq 0$ für alle $j = 1,\dots, i-1$ gilt.

	Da dann die $\lambda_j^{(m)}, \, j = 1,\dots,i-1$ Nullstellen von $p$ sind, können wir

	\begin{equation*}
		p(\lambda) = \prod_{j = 1}^{i-1} \frac{\lambda_j^{(m)} -\lambda}{\lambda_j^{(m)} - \lambda_i} q(x)
	\end{equation*}
	schreiben, wobei $q \in \Pi_{m-i}$.\\

	Nach analogen Umformungen wie in \ref{Ungl i=1} gilt

	\begin{equation*}
		\begin{aligned}
			\lambda_i - \lambda_i^{(m)} &= \min_{\substack{p\in \Pi_{m-1}\setminus \{0\} \\ p(\lambda_j^{(m)}) = 0, j = 1,\dots,i-1}} \frac{\sum_{j=1}^{n} (\lambda_i - \lambda_j)|p(\lambda_j)\alpha_j|^2}{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			&= \min_{\substack{p\in \Pi_{m-1}\setminus \{0\} \\ p(\lambda_j^{(m)}) = 0, j = 1,\dots,i-1}} \frac{\sum_{\substack{j=1, j\neq i}}^{n} (\lambda_i - \lambda_j)|p(\lambda_j)\alpha_j|^2}{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			& \le (\lambda_i - \lambda_n) \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2} \min_{\substack{p\in \Pi_{m-1}\setminus \{0\} \\ p(\lambda_j^{(m)}) = 0, j = 1,\dots,i-1}} \max_{j=i+1,\dots,n} \left(\frac{|p(\lambda_j)|}{|p(\lambda_i)|}\right)^2 \\
			&= (\lambda_i - \lambda_n) \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2} \min_{q\in \Pi_{m-i}\setminus \{0\}} \max_{j=i+1,\dots,n} \left(\left|\prod_{k = 1}^{i-1} \frac{\lambda_k^{(m)} -\lambda_j}{\lambda_k^{(m)} - \lambda_i}\right| \left|\frac{q(\lambda_j)}{q(\lambda_i)}\right|\right)^2 \\
			&\le (\lambda_i - \lambda_n) \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2} \left|\prod_{k = 1}^{i-1} \frac{\lambda_k^{(m)} -\lambda_n}{\lambda_k^{(m)} - \lambda_i}\right|^2 \min_{q\in \Pi_{m-i}\setminus \{0\}} \max_{j=i+1,\dots,n} \left(\left|\frac{q(\lambda_j)}{q(\lambda_i)}\right|\right)^2 \\
		\end{aligned}
	\end{equation*}

	Wiederum ist
	\begin{equation*}
		(\tan\theta_i)^2 = \frac{\norm{(\id - \mathcal{P}_{u_i})v_0}^2}{\norm{ \mathcal{P}_{u_i}v_0}^2} = \frac{\norm{v_0 - \alpha_i u_i}^2}{\norm{\alpha_i u_i}^2} = \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2}
	\end{equation*}

	und  mit $a = \lambda_n, b= \lambda_{i+1}$, $c = \lambda_i$ in Lemma \ref{lem:polminmax}, erhalten wir die Behauptung.\\

	Wenn nun $\beta_j$ existieren, die gleich 0 sind, kann man die Bediungung $p(\lambda_j^{(m)}) = 0$ weglassen und das damit berechnete Minimum wird dadurch nicht größer, also kann die gleiche Abschätzung verwendet werden.
	Also gilt \ref{konvergenz Eigenwerte} für alle $i = 1, \dots, m-1$.
	\end{proof}
\end{theorem}

Die mit $m$ exponentielle Konvergenz der Eigenwerte ist nun ersichtlich, da mit \ref{polminmax}
\begin{equation*}
	\left(\frac{1}{T_{m-i}(\gamma_i)}\right)^2 \le 4 \left(\gamma_i + \sqrt{\gamma_i^2 -1}\right)^{-2(m-i)}
\end{equation*}
gilt. Die Konvergenzgeschwindigkeit nimmt in Richtung der kleineren Eigenwerte ab. Wir bemerken noch, dass bei der Approximation von $\lambda_i$ bei Eigenwerten $\lambda_i \approx \lambda_j, \, j < i$, das Produkt $\kappa^{(m)}$ groß werden kann, da der Nenner wegen $\lambda^{(m)}_j - \lambda_i$ klein wird. Dies hat eine schlechtere Approximation zur Folge.


\subsection{Implementierung}

\subsection{Ergebnisse}

\newpage

%Literaturverzeichnis

\end{document}

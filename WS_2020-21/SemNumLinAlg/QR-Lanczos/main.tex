\documentclass{article}

\input{../../../Fundament-LaTeX/packages.tex}
\input{../../../Fundament-LaTeX/macros.tex}
\input{../../../Fundament-LaTeX/listings.tex}

\parskip 5pt
\parindent 0pt

\addbibresource{references.bib}

\setlength{\headsep}{0.25in}

\lstset{language=Python,
frame=tb,
numbers = left,
numbersep = 8pt,
captionpos=b,
tabsize=2,
basicstyle=\footnotesize,
xleftmargin = 0.2\textwidth,
xrightmargin = 0.2\textwidth}

\theoremstyle{plain}

% numbered theorems
\newtheorem{theorem}    {Satz}   [section]
\newtheorem{lemma}      [theorem]{Lemma}
\newtheorem{corollary}  [theorem]{Korollar}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}     [theorem]{Bemerkung}
\newtheorem{definition} [theorem]{Definition}
\newtheorem{example}    [theorem]{Beispiel}
\newtheorem{heuristics} [theorem]{Heuristik}
\renewcommand{\proofname}{Beweis}

\begin{document}

\begin{titlepage}
	%\vspace*{-2cm}
	\begin{center}
		\includegraphics[width=0.45\textwidth]{TULogo-eps-converted-to.pdf}
		\vskip 1cm%
		{\LARGE S~\Large E~M~I~N~A~R~A~R~B~E~I~T}
		\vskip 8mm
		{\huge\bfseries Eigenwertberechnung mithilfe des Lanczos-Verfahrens}
		\vskip 1cm
		\large
		ausgef\"uhrt am
		\vskip 0.75cm
		{\Large Institut f\"ur\\[1ex] Analysis und Scientific Computing}\\[1ex]
		{\Large TU Wien}
		\vskip0.75cm
		unter der Anleitung von
		\vskip0.75cm
		{\Large\bfseries Lothar Nannen}\\[1ex]
		\vskip 0.5cm
		durch
		\vskip 0.5cm
		{\Large\bfseries Göth Christian \quad Moik Matthias \quad Sallinger Christian}\\[1ex]
	\end{center}

	\vfill

	\small
	Wien, am {\today}
	\vspace*{-15mm}
\end{titlepage}

\newpage
\tableofcontents
\newpage

\pagestyle{headings}

\section{Einleitung}

Das folgende Projekt beschäftigt sich mit der numerischen Berechnung von Eigenwerten mithilfe des Lanczos-Verfahrens. Die Arbeit orientiert sich dabei in großen Teilen an \cite{EWPs} und \cite{Num}. Zur Motivation betrachten wir das Eigenwertproblem des negativen Laplace-Operators mit homogenen Neumann-Randbedingungen auf einem zweidimensionalen Gebiet $\Omega$.

\subsection{Problemstellung}

Wir wiederholen zunächst die Definition eines Eigenwertproblems.

\begin{definition}
	Sei $\Omega$ eine offene und beschränkte Menge. Das Paar $(\lambda, u) \in \C \times H^2(\Omega) \setminus \{0\}$\footnote{$H^k(\Omega)$ bezeichnet hier den Sobolevraum $W^{k,2}(\Omega)$} heißt Eigenpaar, wenn es eine Lösung des Eigenwertproblems
	\begin{align}
	    \begin{cases}
	    -\Delta u = \lambda u & \text{in } \Omega, \\
	    \frac{\partial u}{\partial \nu} = 0 & \text{auf}~ \partial\Omega,
	    \end{cases}
	    \label{neumann}
	\end{align}
	ist, wobei $\nu$ der äußere Normalenvektor an $\partial\Omega$ sei.
\end{definition}

Um die aus dem numerischen Verfahren gewonnen Eigenwerte mit den tatsächlichen Eigenwerten vergleichen zu können, wählen wir als Gebiet das Rechteck $\Omega := (0,a) \times (0,b)$ mit $a, b \in \R^+$, auf dem sich diese analytisch berechnen lassen.

\subsection{Analytische Lösung für ein Rechteck}

Die Eigenwerte und Eigenfunktionen des oben genannten Problems werden zunächst mit einem Separationsansatz hergeleitet.
Sei dazu $u(x,y) = v(x)w(y)$, die Differentialgleichung ergibt dann
\begin{align*}
	- \Delta u(x,y) = -v''(x) w(y) - v(x) w''(y) = \lambda v(x)w(y)
\end{align*}
Dividiert man formal durch $v(x)w(y)$, erhält man
\begin{align*}
	&-\frac{v''(x)}{v(x)} - \frac{w''(y)}{w(y)} = \lambda \\
	 \Leftrightarrow\quad &-\frac{v''(x)}{v(x)} = \lambda + \frac{w''(y)}{w(y)}.
\end{align*}
Da die linke Seite nur von $x$ abhängt und die rechte nur von $y$, müssen beide konstant sein. Es gibt also ein $\kappa_v^2 \in \R \setminus \{0\}$, sodass
\begin{align*}
	-\frac{v''(x)}{v(x)} = \kappa_v^2, \quad \Forall x \in (0,a).
\end{align*}
Dies liefert uns die gewöhnliche Differentialgleichung für $v$
\begin{align*}
	v''(x) = -\kappa_v^2 v(x)
\end{align*}
mit der Lösung
\begin{align*}
	v(x) = c_1 \sin(\kappa_v x) + c_2 \cos(\kappa_v x).
\end{align*}

Analog erhält man $w(y) = c_3 \sin(\kappa_w y) + c_4 \cos(\kappa_w y)$, wobei $\lambda = \kappa_v^2 + \kappa_w^2$ gelten muss.

Nun lassen wir noch die Neumann-Randbedingungen einfließen. Mit $\nabla u(x,y) = \Big(v'(x)w(y), v(x)w'(y)\Big)^\top$ erhalten wir für den linken Rand
\begin{align*}
	\frac{\partial u}{\partial \nu}(0,y) = - v'(0)w(y) = \kappa_v(- c_1 \cos(\kappa_v \cdot 0) + c_2 \underbrace{\sin(\kappa_v \cdot 0)}_{=0})w(y) \stackrel{!}{=} 0
\end{align*}
Es muss also $c_1 = 0$ gelten. Analog schließt man bei Betrachtung des unteren Randes auch $c_3 = 0$.

Für den rechten Rand gilt nun
\begin{align*}
	&\frac{\partial u}{\partial \nu}(a,y) = v'(a)w(y) = \kappa_v c_2 \sin(\kappa_v \cdot a)w(y) \stackrel{!}{=} 0 \\
	\Rightarrow \quad &\kappa_v = \frac{n \pi}{a}, \quad n \in \N.
\end{align*}

Wiederum analog schließt man bei Betrachtung des oberen Randes $\kappa_w = \frac{m \pi}{b}, m \in \N$. Wir wählen $c_2 = c_4 := 1$ und halten fest:

\begin{theorem}\label{satz_1_2}
	Eigenfunktionen für das Eigenwertproblem \eqref{neumann} auf dem Gebiet $\Omega := (0,a) \times (0,b)$ sind gegeben durch
	\begin{align*}
		u_{n,m}(x,y) = \cos(\frac{n \pi}{a}x)\cos(\frac{m \pi}{b}y), \quad n,m \in \N
	\end{align*}
	und die zugehörigen Eigenwerte durch
	\begin{align*}
		\lambda_{n,m} = \pi^2(\frac{n^2}{a^2} + \frac{m^2}{b^2}), \quad n,m \in \N.
	\end{align*}
\end{theorem}

\begin{proof}
	Nachrechnen.
\end{proof}

\begin{remark}
	Man rechnet leicht nach, dass die Eigenfunktionen aus Satz \ref{satz_1_2} paarweise orthogonal sind. Wählt man einen geeigneten Funktionenraum, lässt sich sogar zeigen, dass es sich bei den Eigenfunktionen um ein vollständiges Orthogonalsystem handelt.
\end{remark}

\subsection{Numerischer Lösungsansatz}

Das Eigenwertproblem \eqref{neumann} kann durch Multiplizieren mit $v \in H^1(\Omega)$ und Integrieren auf schwache Form gebracht werden (beim Anwenden des Satzes von Gauß verschwindet der Randterm aufgrund der homogenen Randbedingungen). Gesucht sind also Lösungen $(\lambda, u) \in \C \times H^1(\Omega) \setminus \{0\}$, sodass

\begin{align*}
	\Int[\Omega]{\nabla u \nabla v}{x} = \lambda \Int[\Omega]{uv}{x}, \quad \Forall v \in H^1(\Omega).
\end{align*}

Dieses Problem kann mittels der Finite-Elemente-Methode auf ein endlichdimensionales verallgemeinertes Eigenwertproblem der Form

\begin{align} \label{discretization}
	A x_h = \lambda_h B x_h
\end{align}

gebracht werden, mit $A, B \in \R^{N \times N}$ und symmetrisch.

\begin{lemma}\label{lemma_1_3}
	Mit $\rho_h \in \R$, welches kein Eigenwert ist, und $\lambda_h = \frac{1}{\mu_h} + \rho_h$ ist das verallgemeinerte Problem \eqref{discretization} äquivalent  zu
	\begin{align} \label{equivdiscret}
		(A - \rho_h B)^{-1} B x_h = \mu_h x_h
	\end{align}
\end{lemma}

\begin{proof}
	Der Einfachheit halber lassen wir den Index $h$ weg. Die Umformungen
	\begin{align*}
		&A x = \lambda B x = (\frac{1}{\mu} + \rho) B x \\
		\Leftrightarrow \quad &(A - \rho B)x = \frac{1}{\mu} B x \\
		\Leftrightarrow \quad &\mu x = (A - \rho B)^{-1}B x
	\end{align*}
	zeigen die Äquivalenz
\end{proof}

\begin{remark}\label{remark_1_4}
	Die Matrix $(A - \rho_h B)^{-1} B$ im Eigenwertproblem \eqref{equivdiscret} ist zwar Produkt zweier symmetrischer Matrizen, aber i. A. nicht mehr symmetrisch. Dies wird im Laufe des Projektes noch wichtig sein, da das Lanczos-Verfahren nur für symmetrische Matrizen funktioniert.
\end{remark}

Die Diskretisierung durch die Finite-Elemente-Methode liefert uns Matrizen $A$ und $B$ mit sehr großer Dimension $N$. Daher ist es sinnvoll, ein Verfahren zu entwickeln, das uns für sehr große Matrizen eine gute Approximation zumindest an extremale Eigenwerte liefert. Das Lanczos-Verfahren wird dies unter Zuhilfenahme des QR-Verfahrens bewerkstelligen.

\section{Das QR-Verfahren}\label{section_QR}

\subsection{Idee und Algorithmus}

Das QR-Verfahren ist eine Methode zur Eigenwertbestimmung einer Matrix $A \in \C^{n \times n}$ mit Eigenwerten $\lambda_1, ..., \lambda_n$. Dabei wird mithilfe der QR-Zerlegung iterativ eine Folge $(A^{(t)})_{t \in \N}$ definiert. Diese ist durch

\begin{equation}
	A^{(0)} \coloneqq A, \quad A^{(t+1)} \coloneqq R^{(t)}Q^{(t)}
\end{equation}

gegeben, wobei $A^{(t)} = Q^{(t)} R^{(t)}$ die QR-Zerlegung von $A^{(t)}$ ist.
Für alle $t\in \N$ hat $A^{(t)}$ die gleichen Eigenwerte wie $A$ und die Folge $(A^{(t)})_{t \in \N}$ konvergiert gegen eine obere Dreiecksmatrix. Das heißt, die Diagonaleinträge von $A^{(t)}$ konvergieren gegen die Eigenwerte von $A$.

\floatname{algorithm}{Algorithmus}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}

	\caption{Basisalgorithmus QR-Verfahren}
	\label{QR-Verfahren}
	\algorithmicrequire{ $A \in \C^{n\times n}$}
	\begin{algorithmic}[1]
		\While{Abbruchbedingung nicht erfüllt}
		\State Berechne die QR-Zerlegung $A = QR$
		\State $A \coloneqq RQ$
		\EndWhile
	\end{algorithmic}
	\algorithmicensure{ $A$ wird überschrieben mit einer Matrix, die eine Approximation einer oberen Dreiecksmatrix ist und die gleichen Eigenwerte wie die ursprüngliche Matrix $A$ besitzt}
\end{algorithm}


Die Konvergenz dieses Algoritmus kann unter gewissen Voraussetzungen gezeigt werden. Wir zitieren hier nur den Satz und verweisen für den Beweis auf \cite{Num}, Satz 9.15.
%\cite{Nannen-Skript}, Satz 9.15.

\begin{theorem}[Konvergenz des QR-Verfahrens]
	\label{Konv_QR}
	Sei $A \in \C^{n\times n}$ eine diagonalisierbare Matrix mit Eigenwerten $\lambda_1,\dots,\lambda_n \in \C$ und $|\lambda_1| > |\lambda_2| > \dots > |\lambda_n| > 0$. Weiters sei $V \in \C^{n \times n}$ die Matrix, die die Eigenvektoren $v_1,\dots v_n$ zu den jeweiligen Eigenwerten als Spalten enthält. Zudem existiere eine untere normierte Dreiecksmatrix $L$ und eine obere Dreiecksmatrix $U$, sodass $V^{-1} = LU$.\\
	Dann konvergieren die Hauptdiagonaleinträge der Matrizen $A^{(t)}$, die in Algorithmus \ref{QR-Verfahren} definiert sind, gegen die Eigenwerte von $A$.
\end{theorem}


\subsection{Beschleunigung der Konvergenz}

Wir betrachten die Matrizen $M^{(t)} \coloneqq \Lambda^t L \Lambda^{-t},\, t \in \N$, die im Beweis von Satz \ref{Konv_QR} vorkommen. Diese haben die Form

\begin{equation*}
	M^{(t)} =
	\left( \begin{array}{cccc}
		1 & 0 & \dots & 0 \\
		m_{21} \left(\frac{\lambda_2}{\lambda_1}\right)^t & 1 &  &\vdots \\
		\vdots & \ddots & \ddots & 0 \\
		m_{n1} \left(\frac{\lambda_n}{\lambda_1}\right)^t & \dots & m_{n(n-1)} \left(\frac{\lambda_n}{\lambda_{n-1}}\right)^t & 1 \\
	\end{array}\right).
\end{equation*}

\vspace{4pt}
Da die Eigenwerte der Größe nach sortiert sind, erkennt man, dass diese Matrixfolge gegen die Einheitsmatrix konvergiert. Dies zieht schlussendlich die Konvergenz von $(A^{(t)})_{t\in \N}$ gegen eine obere Dreiecksmatrix nach sich.
Die Einträge konvergieren schneller gegen $0$, wenn die Brüche kleiner sind. Das versuchen wir zu erreichen, indem wir einen shift durchführen.\\
Für einen Eigenwert $\lambda$ einer Matrix $A \in \C^{n\times n}$ gilt

\begin{equation*}
	\ker(A - \lambda \id) \neq \{0\} \Leftrightarrow \ker ((A - \rho \id) - (\lambda -\rho) \id) \neq \{0\} \Leftrightarrow (\lambda-\rho) \text{ ist Eigenwert von }  (A - \rho \id).
\end{equation*}

Die Wahl des shifts als $a^{(t)}_{nn}$ macht also Sinn, da dieser Eintrag für $t \rightarrow \infty$ gegen $\lambda_n$ konvergiert. Dieser shift wird auch Rayleigh-Quotienten-Shift genannt. Nach der QR-Zerlegung von $A^{(t)}$ setzt man $A^{(t+1)} \coloneqq R^{(t)}Q^{(t)} + \rho \id$, um wieder eine Matrix zu erhalten, die die gleichen Eigenwerte wie $A^{(t)}$ und somit auch wie $A$ besitzt.
Je nachdem, wie genau $\lambda_n$ approximiert werden soll, wählt man eine Toleranz, die von den Einträgen $a^{(t)}_{ni}, \, i = 1,\dots, n-1$ unterschritten werden muss, bevor analoge Schritte für den zweitkleinsten Eigenwert durchgeführt werden. Dessen Näherung ist der Eintrag $a^{(t)}_{(n-1)(n-1)}$.

\floatname{algorithm}{Algorithmus}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}

	\caption{QR-Verfahren mit Rayleigh-Quotienten-Shift}
	\label{QR-Verfahren_shifts_rayleigh}
	\algorithmicrequire{ $A \in \C^{n\times n}$}
	\begin{algorithmic}[1]
		\For{$i = n,\dots, 2$}
			\While{$|a_{i,i-1}| > tol$}
				\State $\rho = a_{i,i}$
				\State Berechne die QR-Zerlegung $A - \rho \id = QR$
				\State $A = RQ + \rho \id$
			\EndWhile
		\EndFor
	\end{algorithmic}
	\algorithmicensure{ $A$ wird überschrieben mit einer Matrix, die eine Approximation einer oberen Dreiecksmatrix ist und die gleichen Eigenwerte wie die ursprüngliche Matrix $A$ besitzt}
\end{algorithm}

Eine weitere Möglichkeit, den shift zu wählen, ist, sich die Eigenwerte $\rho_1, \rho_2$ der $2 \times 2$ Matrix rechts unten von $A^{(t)}$ zu berechnen und anschließend den Eigenwert, der betragsmäßig näher an $a^{(t)}_{nn}$ liegt, zu wählen. Die Konvergenzgeschwindigkeit wird dadurch nochmals verbessert (vgl. \cite{Num}, S. 114). Dieser shift wird auch Wilkinson-Shift genannt.


\begin{algorithm}[H]
	\label{QR-Verfahren_shifts}
	\caption{QR-Verfahren mit Wilkinson-Shift}
	\algorithmicrequire{ $A \in \C^{n\times n}$}
	\begin{algorithmic}[1]
		\For{$i = n,\dots, 2$}
		\While{$a_{i(i-1)} > tol (a_{(i-1)(i-1)} + a_{ii})$}
		\State Berechne die Eigenwerte $\rho_1$, $\rho_2$ von $\left(\begin{array}{rr}
			a_{(i-1)(i-1)}&a_{(i-1)i}\\
			a_{i(i-1)}&a_{ii}\\
		\end{array}\right)$
		\If{$|\rho_1 - a_{ii}| < |\rho_2 - a_{ii}|$}
		\State $\rho = \rho_1$
		\Else
		\State $\rho = \rho_2$
		\EndIf
		\State Berechne die QR-Zerlegung $A - \rho \id = QR$
		\State $A = RQ + \rho \id$
		\EndWhile
		\EndFor
	\end{algorithmic}
	\algorithmicensure{ $A$ wird überschrieben mit einer Matrix, die eine Approximation einer oberen Dreiecksmatrix ist und die gleichen Eigenwerte wie die ursprüngliche Matrix $A$ besitzt}
\end{algorithm}


Eine weitere Beschleunigung erreichen wir bei der QR-Zerlegung, wenn die Matrix $A$ Hessenbergform hat. Auf diesen Fall werden wir im nächsten Abschnitt eingehen.

\subsection{Die QR-Zerlegung für Hessenberg-Matrizen}

Den Hauptaufwand beim QR-Verfahren bildet die QR-Zerlegung die in jedem Iterationsschritt durchgeführt werden muss. Der Aufawnd einer QR-Zerlegung mithilfe von Householdermatrizen wächst kubisch mit der Dimension der Matrix (vgl. \cite{Num}, S.78). Nun gibt es für sogenannte Hessenberg-Matrizen einen wesentlich effizienteren Algorithmus, bei dem statt Householdermatrizen andere unitäre Matrizen verwendet werden.

\begin{definition}[Hessenberg-Matrix]
	Eine Matrix $A = (a_{ij})_{i,j = 1}^n \in \C^{n \times n}$ hat (obere) Hessenbergform, falls $a_{ij} = 0$ für alle $j = 1,\dots,n$ und $i = j+2, \dots, n$. Eine Hessenberg-Matrix hat also fast obere Dreiecksform, nur in der unteren Nebendiagonale sind noch zusätzliche nicht-null Einträge.
\end{definition}

\begin{definition}[Givens-Rotation]
	Eine Givens-Rotation $G \in \C^{n\times n}$ ist eine Matrix der Form

	\begin{align*}
	G(j,k,c,s)  = \left(\begin{array}{ccc|ccccc|ccc}
	1 &  &  & & & & & & &\\
	& \ddots & & & & & & & & &\\
	& & 1 & & & & & & & \\
	\hline
	& & &  \overline{c} & & & & \overline{s} & & &\\
	& & & & 1 & & & & & &\\
	& & & & & \ddots & & & & &\\
	& & & & &  &1 & & & &\\
	& & & -s & & & & c & & &\\
	\hline
	& & & & & & & & 1 & &\\
	& & & & & & & & & \ddots &\\
	& & & & & & & & & & 1\\
	\end{array}\right)
	\end{align*}
	mit $|c|^2 + |s|^2 = 1$. Der innere Block reicht dabei von der $j$-ten bis zur $k$-ten Zeile sowie Spalte.
\end{definition}


Wir wollen nun für die QR-Zerlegung geeignete Givens-Rotationen verwenden. Zunächst sehen wir, dass Givens-Rotationen unitär sind, da

\begin{align*}
\left(\begin{array}{cc}
\overline{c}& \overline{s} \\
-s & c\\
\end{array}\right)\left(\begin{array}{cc}
c & -\overline{s} \\
s & \overline{c}
\end{array}\right)
=
\left(\begin{array}{cc}
|c|^2 + |s|^2 & 0 \\
0 & |c|^2 + |s|^2 \\
\end{array}\right).
\end{align*}


Wir werden es mit Givens-Rotationen der Form

$$
G(j,j+1,c,s) =
    \left(\begin{array}{ccc|cc|ccc}
    1 & & & & & & & \\
    & \ddots & & & & & & \\
    & & 1 & & & & & \\
    \hline
    & & & \overline{c} & \overline{s} & & & \\
    & & & -s & c & & &\\
    \hline
    & & & & & 1 & & \\
    & & & & & & \ddots & \\
    & & & & & & & 1\\
    \end{array}\right)
$$

zu tun haben.

Wie man unmittelbar sieht, hat die Multiplikation einer Givens-Rotation der obigen Form von links an eine Matrix $A\in \C^{n\times n}$ nur in der $j$-ten sowie $j+1$-ten Zeile von $A$ eine Auswirkung. Dort werden für $k = 1,\dots, n$ die Einträge $a_{j,k}$ sowie $a_{j+1, k}$ ersetzt durch $a_{j,k}\overline{c} + a_{j+1, k}\overline{s}$ sowie $-a_{j,k}s + a_{j+1, k}c$. Dies können wir nun dazu verwenden, um bei einer Hessenberg-Matrix die Einträge unter der Diagonale zu eliminieren und dadurch eine obere Dreiecksmatrix erhalten.

Dazu müssen wir die Gleichung $-a_{j,j}s + a_{j+1, j}c = 0$ unter der Nebenbedingung $|c|^2 + |s|^2 = 1$ lösen. Dieses Problem hat zwei Lösungen, die sich, wie man sich leicht überzeugen kann, nur durch das Vorzeichen unterscheiden. Eine Lösung ist gegeben durch

\begin{align*}
\left(\begin{array}{c}
    c \\
    s \\
\end{array}\right)
=
\textbf{rot}(a_{j,j},a_{j+1,j})
:=
\frac{1}{\sqrt{|a_{j,j}|^2 + |a_{j+1,j}|^2}}
\left(\begin{array}{c}
a_{j,j} \\
a_{j+1,j} \\
\end{array}\right).
\end{align*}

Zur Vermeidung von over- bzw. underflow verwendet man in der Praxis jedoch die äquivalenten Formeln

\begin{align*}
	c &= \frac{a_{j,j} / |a_{j,j}|}{\sqrt{1+|\tau|^2}},~
	s = \frac{\tau}{\sqrt{1 +|\tau|^2}},~
	\tau = \frac{a_{j+1,j}}{|a_{j,j}|}, \quad
	\text{für}~ |a_{j,j}| \geq |a_{j+1,j}|, \\
	c &= \frac{\tau}{\sqrt{1 +|\tau|^2}},~
	s = \frac{a_{j+1,j} / |a_{j+1,j}|}{\sqrt{1+|\tau|^2}},~
	\tau = \frac{a_{j,j}}{|a_{j+1,j}|}, \quad
	\text{für}~ |a_{j,j}| < |a_{j+1,j}|.
\end{align*}

Eine Hessenberg-Matrix $A \in \C^{n\times n}$ können wir dann durch Anwendung von $n-1$ Givens-Rotationen auf die Form einer oberen Dreiecksmatrix bringen. Dies können wir iterativ durchführen, dazu definieren wir $A^{(0)} := A$ und für $j = 1,\dots, n-1$

\begin{align*}
	A^{(j)} :=
	G(j, j+1, c_j, s_j)A^{(j-1)} \quad
	\text{mit}~
	(c_j, s_j)^\top  = \textbf{rot}(a_{j,j},a_{j+1,j}).
\end{align*}

Die spezielle Struktur der Hessenberg-Matrix können wir nun verwenden, um Rechenschritte zu sparen: in der $j$-ten Zeile sind wegen unserer Wahl von $c$ und $s$ die ersten $j-1$ Spalteneinträge bereits $0$ (erst recht in der $j+1$-ten Zeile). Daher müssen wir die Anwendung der Givens-Rotation erst ab dem $j$-ten Spalteneintrag realisieren. Wir skizzieren die Vorgehensweise für $n=4$, wobei $+$ für beliebige Einträge der Matrix und $*$ für die im jeweiligen Schritt durch Anwendung der Givens-Rotation veränderten Einträge steht.

\begin{align*}
\setlength\arraycolsep{1.5pt}
	A
	=
	\begin{pmatrix}
		+ & + & + & + \\
		+ & + & + & + \\
		& + & + & + \\
		& & +& + \\
	\end{pmatrix}
	\stackrel{j=1}{\longrightarrow}
	\begin{pmatrix}
		* & * & * & * \\
		 & * & * & * \\
		& + & + & + \\
		& & + &+ \\
	\end{pmatrix}
	\stackrel{j=2}{\longrightarrow}
	\begin{pmatrix}
		+ & + & + & + \\
		 & * & * & * \\
		&  & * & * \\
		& & +& + \\
	\end{pmatrix}
	\stackrel{j=3}{\longrightarrow}
	\begin{pmatrix}
		+ & + & + & + \\
		 & + & + & + \\
		&  & * & * \\
		& & & * \\
	\end{pmatrix}
	= R
\end{align*}

Um schließlich noch die Matrix $Q$ aus der QR-Zerlegung zu erhalten, bemerken wir zunächst, dass die Multiplikation zweier unitärer Matrizen wieder eine unitäre Matrix liefert. Durch unseren Algorithmus erhalten wir

\begin{align*}
R := A^{(n-1)} = G(n-1,n,c_{n-1},s_{n-1})\cdots G(1,2, c_1, s_1) A.
\end{align*}

Die Multiplikation der Givens-Rotationen gibt wieder eine unitäre Matrix, definieren wir also

\begin{align}
Q^* := G(n-1,n,c_{n-1},s_{n-1})\cdots G(1,2, c_1, s_1),
\label{Q_givens}
\end{align}

erhalten wir $Q$ durch Adjungieren des Produktes der Givens-Rotationen, da dann

$$
QR = A.
$$

Bei Multiplikation der $l+1$-ten Givens-Rotation mit dem Produkt der $l$ vorhergegangenen können wir wieder verwenden, dass nur in zwei Zeilen eine Änderung passiert. Da das Produkt der Givens-Rotationen jedoch keine Hessenberg-Matrix ist, müssen wir die Rechnung in allen Spalten durchführen.


\begin{algorithm}
	\label{QR-Zerlegung_Hessenberg}
	\caption{QR-Zerlegung für Hessenberg-Matrizen}
	\algorithmicrequire{ $A \in \C^{n\times n}$}
	\begin{algorithmic}[1]
		\State $Q = \id \in \C^{n\times n}$
		\For{$i = 1,\dots,n-1$}
		\State $\left(\begin{array}{c}
    c_i \\
    s_i \\
\end{array}\right)
=
\frac{1}{\sqrt{|a_{i,i}|^2 + |a_{i+1,i}|^2}}
\left(\begin{array}{c}
a_{i,i} \\
a_{i+1,i} \\
\end{array}\right)$
		\For{$j = i,\dots,n$}
		\State $\left(\begin{array}{c}
		a_{i,j} \\
		a_{i+1,j}\\
		\end{array}\right)
		=
		\left(\begin{array}{cc}
		\overline{c_i} & \overline{s_i} \\
		-s_i & c_i \\
		\end{array}
		\right)
		\left(\begin{array}{c}
		a_{i,j} \\
		a_{i+1,j}\\
		\end{array}\right)$
		\EndFor
		\For{$j= 1,\dots,n$}
		\State $\left(\begin{array}{c}
		q_{i,j} \\
		q_{i+1,j}\\
		\end{array}\right)
		=
		\left(\begin{array}{cc}
		\overline{c_i} & \overline{s_i} \\
		-s_i & c_i \\
		\end{array}
		\right)
		\left(\begin{array}{c}
		q_{i,j} \\
		q_{i+1,j}\\
		\end{array}\right)$
		\EndFor
		\EndFor
	\end{algorithmic}
	\algorithmicensure{ $A$ wird mit oberer Dreiecksmatrix ($=R$) überschrieben, $Q$ ist das Produkt der Givens-Rotationen und $Q^*$ ist die gewünschte unitäre Matrix}
\end{algorithm}

Wie man hier schnell erkennt, hat eine QR-Zerlegung einer Hessenberg-Matrix mithilfe von Givens-Rotationen nun Aufwand $\Landau(n^2)$, ist also um eine ganze $n-$Potenz weniger aufwändig als die Zerlegung mit Householdermatrizen.

Das QR-Verfahren für Hessenberg-Matrizen kann nun noch weiter vereinfacht werden, wenn wir die Matrix $Q$ nicht explizit aufstellen. Man kann die obige QR-Zerlegung noch dahingehend abändern, dass nicht die unitäre Matrix $Q$ und die Dreiecksmatrix $R$ zurückgegeben werden, sondern direkt das Produkt $RQ$. Dazu verwenden die Gleichheit in \eqref{Q_givens}, wonach wir

\begin{align*}
	Q = G^*(1,2,c_1,s_1) \cdots G^*(n-1,n,c_{n-1},s_{n-1})
\end{align*}

erhalten. Nun hat die Multiplikation der Givens-Rotation $G^*(j,j+1, c_j, s_j)$ von rechts an eine Matrix $A \in \C^{n\times n}$ nur in der $j-$ten sowie $j+1-$ten Spalte eine Auswirkung. Hierbei werden nun für $k = 1,\dots,n$ die Einträge $a_{k,j}$ sowie $a_{k,j+1}$ ersetzt durch $a_{k,j}c + a_{k,j+1}s$ sowie $-a_{k,j}\overline{s} + a_{k,j+1}\overline{c}$.
So können wir mit nochmaliger Anwendung von $n-1$ Inversen der Givens-Rotationen schließlich das Produkt $RQ$ berechnen, dabei wird die Matrix $R$ mit der Matrix $RQ$ überschrieben. Da, um die Matrix $R$ zu erhalten, ja bereits unsere ursprüngliche Matrix $A$ überschrieben wurde, muss nur für die $2 \cdot (n-1)$ Werte von $c$ und $s$ neuer Speicher allokiert werden. Wir skizzieren die Vorgehensweise wiederum für den Fall $n=4$, wobei wieder genau die Einträge in denen Änderungen geschehen mit $*$ gekennzeichnet sind.

\begin{align*}
	\setlength\arraycolsep{1.5pt}
	R
	=
	\begin{pmatrix}
		+ & + & + & + \\
		 & + & + & + \\
		&  & + & + \\
		& & & + \\
	\end{pmatrix}
	\stackrel{j=1}{\longrightarrow}
	\begin{pmatrix}
		* & * & + & + \\
		* & * & + & + \\
		&  & + & + \\
		& & & + \\
	\end{pmatrix}
	\stackrel{j=2}{\longrightarrow}
	\begin{pmatrix}
		+ & * & * & + \\
		+ & * & * & + \\
		& * &*  & + \\
		& & & + \\
	\end{pmatrix}
	\stackrel{j=3}{\longrightarrow}
	\begin{pmatrix}
		+ & + & * & * \\
		+ & + & * & * \\
		& + &*  & * \\
		& & *& * \\
	\end{pmatrix}
\end{align*}

Auch hier können wir die Struktur der Matrix verwenden. Wir müssen die Anwendung der Inversen der Givens-Rotationen nur bis zum $j+1$-ten Zeileneintrag realisieren. Wie man schnell einsieht, ist nun auch $RQ$ wieder von oberer Hessenbergform. Im Folgenden ist der Algorithmus in Pseudocode formuliert.

\begin{algorithm}
	\label{RQ_Hessenberg}
	\caption{Berechnung von RQ für Hessenberg-Matrizen}
	\algorithmicrequire{ $A \in \C^{n\times n}$}
	\begin{algorithmic}[1]
		\For{$i = 1,\dots,n-1$}
		\State $\left(\begin{array}{c}
    c_i \\
    s_i \\
\end{array}\right)
=
\frac{1}{\sqrt{|a_{i,i}|^2 + |a_{i+1,i}|^2}}
\left(\begin{array}{c}
a_{i,i} \\
a_{i+1,i} \\
\end{array}\right)$
		\For{$j = i,\dots,n$}
		\State $\left(\begin{array}{c}
		a_{i,j} \\
		a_{i+1,j}\\
		\end{array}\right)
		=
		\left(\begin{array}{cc}
		\overline{c_i} & \overline{s_i} \\
		-s_i & c_i \\
		\end{array}
		\right)
		\left(\begin{array}{c}
		a_{i,j} \\
		a_{i+1,j}\\
		\end{array}\right)$
		\EndFor
		\EndFor
	\For{$j = 1,\dots, n-1$}
		\For{$i = 1,\dots, j+1$}
			\State $\left(
			a_{i,j},
			a_{i,j+1}
			\right)
			=
			\left(
			a_{i,j},
			a_{i,j+1}
			\right)
			\left(\begin{array}{cc}
			c_j & -\overline{s_j} \\
			s_j & \overline{c_j} \\
			\end{array}
			\right)$
			\EndFor
			\EndFor
	\end{algorithmic}
	\algorithmicensure{ $A$ wird mit dem Produkt $RQ$ überschrieben, wobei $Q$ und $R$ die Matrizen aus einer QR-Zerlegung von $A$ sind}
\end{algorithm}

\newpage
\subsection{Implementierung und Vergleich der verschiedenen Methoden}

Die Implementierung der Verfahren wurde in Python durchgeführt. Wir werden bei den Codeausschnitten nur auf etwaige Unterschiede oder Ergänzungen zum Pseudocode eingehen, da die Algoritmen grundsätzlich schon dort erklärt wurden.\newline


\lstset{caption={Implementation des QR-Verfahrens ohne shifts}}

\begin{lstlisting}
def QR_simple(A, tol = 1e-7):
	n = A.shape[1]
	count = 0
	for i in range(n-1,0,-1):
		while abs(A[i,i-1]) > tol:
				Q,R = np.linalg.qr(A)
				A = R@Q
				count +=1
		A[i,:i-n] = 0
return A, sorted(np.diag(A)), count
\end{lstlisting}


Hier werden in Codezeile $5$ die Zeilen der Matrix von unten nach oben durchgegangen. Sobald das (modulo Konstanten) größte Element, das laut Theorie in der unteren Nebendiagonale zu finden ist, nahe bei $0$ ist, wird (in Codezeile 9) die gesamte Matrixzeile bis zum Diagonalelement auf 0 gesetzt. Dies ist nötig, um sich aufschaukelnde Fehler zu vermeiden.
Es wird zudem für den späteren Vergleich eine Programmvariable eingeführt, die die Anzahl an Schleifendurchläufen im Verfahren zählt. Die QR-Zerlegung wird zunächst mit dem numpy-linalg Paket durchgeführt.

Die Implementation der anderen Verfahren unterscheidet sich vom Pseudocode nur in den schon dargelegten Punkten. Die anderen Codeausschnitte sind daher im Anhang zu finden.

Kommen wir nun zum Vergleich der QR-Verfahren. Zur Testung werden zufällige Matrizen, deren Eigenwerte bekannt sind, erstellt. Dazu wird zuerst ein zufälliges Array mit $n$ Elementen generiert (unsere Eigenwerte), sowie eine Diagonalmatrix mit ebendiesen Werten auf der Diagonale. Schließlich führen wir eine Ähnlichkeitstransformation mit einer Zufallsmatrix durch.

\begin{figure}[H]
  \centering
  \subfloat[][Ohne shifts]{\includegraphics[width=0.45\linewidth]{Plots/schleifendurchläufe_wo_shifts}}%
  \qquad
  \subfloat[][Mit shifts]{\includegraphics[width=0.45\linewidth]{Plots/schleifendurchläufe_shifts}}%
  \caption{Anzahl der Schleifendurchläufe bei Matrixdimension $n$}%
  \label{Schleifendurchläufe}
\end{figure}

Wir sehen in Abbildung \ref{Schleifendurchläufe} die Anzahl an Schritten bei den Verfahren für verschiedene Matrixdimensionen $n$. Hier kann man beobachten, dass das Verfahren ohne shifts selbst für kleine Matrizen schon sehr viele Schritte benötigt.  Man sieht, dass die Anzahl an Schleifendurchläufen mit dem Wilkinson-Shift doch erkennbar niedriger ist als die Anzahl beim Verfahren mit Rayleigh-Quotienten-Shift.

In Abbildung \ref{Error_QR} wird der Fehler in der Approximation semilogarithmisch dargestellt. Dabei wird das Eigenwertarray sowie das Array mit den Approximationen an die Eigenwerte sortiert, die Differenz der Arrays gebildet und davon die Euklidnorm gemessen. Im Hinblick auf die Laufzeit wird bei dem Verfahren ohne shifts die Toleranz etwas höher gewählt, was sich auch im Fehler widerspiegelt. Die Fehler in den beiden Verfahren mit den shifts unterscheiden sich nur minimal. Daher ist wohl das QR-Verfahren mit Wilkinson-Shift vorzuziehen.


\begin{figure}[H]
  \centering
  \subfloat[][Ohne shifts]{\includegraphics[width=0.45\linewidth]{Plots/error_wo_shifts}}%
  \qquad
  \subfloat[][Mit shifts]{\includegraphics[width=0.45\linewidth]{Plots/error_shifts}}%
  \caption{Fehler in den Eigenwerten, gemessen in $||\cdot||_2$ bei Matrixdimension $n$}%
  \label{Error_QR}
\end{figure}

Um noch die Laufzeit zwischen dem QR-Verfahren mit Wilkinson-Shift, welches die vorimplementierte QR-Zerlegung aus dem \textit{numpy.linalg} Paket verwendet, und dem selbstimplementierten Verfahren speziell für Hessenberg-Matrizen zu vergleichen, verwenden wir das \textit{time} Paket und messen so die Zeit, die das Verfahren bei zufällig generierten Hessenberg-Matrizen mit immer höher werdender Dimension benötigt.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.6\linewidth]{Plots/Laufzeitvergleich}
	\caption{Vergleich der Laufzeit des vorimplementierten QR-Verfahrens mit QR-Verfahren speziell für Hessenberg-Matrizen}
	\label{Laufzeitvergleich}
\end{figure}

In Abbildung \ref{Laufzeitvergleich} werden dabei beide Verfahren auf zufällige Matrizen der Dimension $n$ angewendet und die Laufzeit gemessen. Dem QR-Verfahren mit der vorimplementierten QR-Zerlegung werden dabei volle Matrizen übergeben, während das QR-Verfahren für Hessenberg-Matrizen natürlich auf eine Hessenberg-Matrix angewendet wird. Dabei zeigt sich, dass das Verfahren mit der vorimplementierten QR-Zerlegung selbst bei vollbesetzten Matrizen noch um ein Vielfaches schneller ist als das selbstimplementierte Verfahren. In der \textit{numpy} Dokumentation liest man, dass die verwendete vorimplementierte QR-Zerlegung Interface für eine \texttt{LAPACK}-Routine ist. Das \texttt{LAPACK} (Linear Algebra Package) ist Fortran basiert und Fortran ist für Berechnungen aus der numerischen Linearen Algebra um ein Vielfaches schneller als Python-Code. Das erklärt wohl auch, warum das Verfahren mit der vorimplementierten QR-Zerlegung so viel besser abschneidet. Eine Dokumentation von \texttt{LAPACK} ist in \cite{LAPACK} zu finden.

\section{Das Lanczos-Verfahren}
Wie in der Einleitung erwähnt, liefert uns das dort beschriebene Eigenwertproblem Matrizen mit sehr großer Dimension. Außerdem ist man in der Praxis häufig nur an wenigen extremalen Eigenwerten interessiert. Das QR-Verfahren ist für solche Probleme also nicht geeignet. Im Folgenden werden wir deshalb das Lanczos-Verfahren kennenlernen.

\subsection{Idee}

Wir zeigen zunächst ein theoretisches Resultat, welches eine Darstellung der Eigenwerte für hermitesche Matrizen liefert. Man sieht leicht, dass es sich hierbei um eine Verallgemeinerung des Satzes von Rayleigh handelt.

\begin{lemma} \label{Rayleigh}
	Sei $A \in \C^{n \times n}$ hermitesch bezüglich des Skalarproduktes $(\cdot, \cdot)$, $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n$ die Eigenwerte von $A$ (gemäß Vielfachheit gezählt) und $u_1, \dots, u_n$ die zugehörigen normierten Eigenvektoren. Dann gilt
	\begin{align}\label{lambdamax}
		\lambda_1 = \max_{v \in \C^n \setminus \{0\}} \frac{(Av,v)}{(v,v)}, \quad \lambda_k = \max_{\substack{v \in \C^n \setminus \{0\}\\
    (u_j,v) = 0, j= 1,\dots, k-1}} \frac{(Av,v)}{(v,v)}, \quad k= 2, \dots, n
	\end{align}
	und
	\begin{align}\label{lambdamaxmin}
		\lambda_k = \max_{\substack{S \subseteq \C^n\\
    \dim S = k}} \min_{v \in S \setminus \{0\}} \frac{(Av,v)}{(v,v)}, \quad k=1, \dots n.
	\end{align}

	Für $v \neq 0$ nennt man $\frac{(Av,v)}{(v,v)}$ den Rayleigh-Quotienten von $v$.
\end{lemma}

\begin{proof}
	Sei $v \in \C^n \setminus \{0\}$ beliebig. Da $u_1, \dots, u_n$ eine Orthonormalbasis des $\C^n$ bilden, gilt $ v = \sum_{j=1}^{n} (v,u_j) u_j$ und $\|v\|^2 = \sum_{j=1}^{n} |(u_j,v)|^2$. Damit folgt
	\begin{align*}
		(Av,v) = \sum_{j=1}^{n} (v,u_j) (\lambda_j u_j,v) = \sum_{j=1}^{n} \lambda_j |(u_j,v)|^2 \leq \lambda_1 \sum_{j=1}^{n} |(u_j,v)|^2 = \lambda_1 (v,v).
	\end{align*}
	Dividieren durch $(v,v)$ liefert $\lambda_1 \geq \frac{(Av,v)}{(v,v)}$ für alle $v \in \C^n \setminus \{0\}$ und für $v = u_1$ gilt sogar Gleichheit.

	Für ein anderes $k = 2,\dots,n$ können wir analog ein beliebiges $v \in \C^n \setminus \{0\}$ wählen, nun aber mit der Bedingung $(u_j,v) = 0, j=1,\dots,k$ betrachten. In der Summe $\sum_{j=1}^{n} \lambda_j |(u_j,v)|^2$ fallen somit die ersten $k-1$ Summanden weg und wir können die verbleibenden Eigenwerte mit $\lambda_k$ nach oben abschätzen. Gleichheit gilt für den jeweiligen Eigenvektor $u_k$.

	Für die Gleichheit in \eqref{lambdamaxmin} gilt ähnlich zu oben für $v \in \tilde{S} := \textbf{span}\{u_1, \dots, u_k\}$, dass
	\begin{align*}
		(Av,v) = \sum_{j=1}^{k} \lambda_j |(v,u_j)|^2 \geq \lambda_k (v,v)
	\end{align*}

	und daraus
	\begin{align*}
		\lambda_k \leq \min_{v \in \tilde{S} \setminus \{0\}} \frac{(Av,v)}{(v,v)}.
	\end{align*}
	Für $v = u_k$ gilt wiederum Gleichheit.

	Betrachten wir nun einen anderen Unterraum $S \neq \tilde{S}$ mit $\dim S = k$. Dann gibt es ein $v_0 \in S \setminus \{0\}$ mit $(v_0,u_j) = 0$ für $j=1,\dots,k$. Für dieses $v_0$ gilt $(Av_0,v_0) \leq \lambda_k (v_0,v_0)$. Das heißt, für beliebige solche $S$ ist
	\begin{align*}
		\min_{v \in S \setminus \{0\}} \frac{(Av,v)}{(v,v)} \leq \lambda_k,
	\end{align*}
	womit die Gleichheit gezeigt ist.
\end{proof}

Wir wollen das Verfahren der Vektoriteration zur Bestimmung des größten Eigenwertes einer hermiteschen Matrix wiederholen. Diese erzeugt ausgehend von einem Startvektor $x^{(0)} \in \C^{n}$ eine Folge von Iterierten $x^{(m)} := \frac{Ax^{(m-1)}}{\|Ax^{(m-1)}\|}$ für $m \in \N$. Definiert man die Approximationen $\lambda^{(m)}$ an den größten Eigenwert $\lambda_1$ durch den Rayleigh-Quotienten von $x^{(m-1)}$, so konvergiert $\lambda^{(m)}$ quadratisch gegen $\lambda_1$ (vgl.\ \cite{Num}, Satz 9.9).

\begin{definition}[Krylov-Raum]
	Sei $v_0 \in \C^{n}$ und $A \in \C^{n \times n}$. Dann bezeichnet
	\begin{align*}
		\mathcal{K}_m(A,v_0) := \textbf{span}\{v_0, Av_0, \dots, A^{m-1}v_0\}, \quad m \in \N
	\end{align*}
	den Krylov-Raum von $A$ und $v_0$. Es bezeichne $\mathcal{P}_m \in \C^{n \times n}$ die orthogonale Projektion auf $\mathcal{K}_m$, also die eindeutige hermitesche Matrix mit $\mathcal{P}_m^2 = \mathcal{P}_m$ und $\mathcal{R}(\mathcal{P}_m) = \mathcal{K}_m$.
\end{definition}

Beim Lanczos-Verfahren wird nun als Näherung an den größten Eigenwert das Maximum des Rayleigh-Quotienten über dem Krylov-Raum
\begin{align*}
	\mathcal{K}_m(A,x^{(0)}) = \textbf{span}\{x^{(0)}, x^{(1)}, \dots , x^{(m-1)} \}
\end{align*}
verwendet, also

\begin{align*}
	\lambda_1^{(m)} := \max_{x \in \mathcal{K}_m \setminus \{0\}} \frac{(Ax,x)}{(x,x)}.
\end{align*}

Da $x^{(m-1)} \in \mathcal{K}_m$, gilt
\begin{align*}
	\lambda^{(m)} \leq \lambda_1^{(m)} \leq \lambda_1
\end{align*}

und $\lambda_1^{(m)}$ ist somit eine mindestens so gute Approximation von $\lambda_1$ wie $\lambda^{(m)}$.

Betrachten wir die Abbildung

\begin{align*}
	\mathcal{A}_m: \mathcal{K}_m &\to \mathcal{K}_m \\
	x &\mapsto \mathcal{P}_m Ax,
\end{align*}

so sehen wir, dass diese selbstadjungiert ist. Es gilt nämlich für $x,y \in \mathcal{K}_m$

\begin{align*}
	(\mathcal{A}_mx, y) = (\mathcal{P}_m A \mathcal{P}_m x, y) = (x, \mathcal{P}_m A \mathcal{P}_m y) = (x, \mathcal{A}_m y),
\end{align*}

da $\mathcal{P}_m$ und $A$ hermitesch sind. Also besitzt $ \mathcal{A}_m$ nur reelle Eigenwerte $\lambda_1(\mathcal{A}_m) \geq \lambda_2(\mathcal{A}_m) \geq \dots \geq \lambda_m(\mathcal{A}_m)$. Für den größten Eigenwert gilt

\begin{align*}
	\lambda_1(\mathcal{A}_m) = \max_{x \in \mathcal{K}_m \setminus \{0\}} \frac{(\mathcal{A}_m x, x)}{(x,x)} = \max_{x \in \mathcal{K}_m \setminus \{0\}} \frac{(Ax, \mathcal{P}_m x)}{(x,x)} = \max_{x \in \mathcal{K}_m \setminus \{0\}} \frac{(A x, x)}{(x,x)} = \lambda_1^{(m)}.
\end{align*}

Die erste Gleichheit kann man sich mit gleichen Rechnungen wie im Beweis von Lemma \ref{Rayleigh} überlegen. Der größte Eigenwert $\lambda_1(\mathcal{A}_m)$ von $\mathcal{A}_m$ ist also nach unseren bisherigen Überlegungen eine gute Approximation an den größten Eigenwert $\lambda_1$ von $A$. Es liegt nahe, für alle $j \leq m$ auch $\lambda_j(\mathcal{A}_m)$ als Approximation von $\lambda_j$ zu betrachten. Dass dies eine gute Wahl ist, werden wir im  Abschnitt \ref{konvergenz} zeigen.

\subsection{Herleitung des Verfahrens}

Es ist also nun Ziel, die Eigenwerte der Abbildung $\mathcal{A}_m$ zu berechnen. Hierfür benötigen wir eine Repräsentation von $\mathcal{A}_m$ durch eine Matrix $B_m$ bezüglich einer Orthonormalbasis $\{v_0, \dots, v_{m-1}\}$ des Krylov-Raums $\mathcal{K}_m(A,v_0)$. Für die Einträge von $B_m$ gilt
\begin{align*}
	(B_m)_{ij} &= (v_i, \mathcal{A}_m v_j) = v_i^*Av_j,
\end{align*}
woraus wir die Matrixgleichung
\begin{align}\label{mateq}
	B_m &= V_m^*AV_m \in \C^{m \times m}
\end{align}
erhalten.

Der Vorteil dieser Darstellung ist, dass die Eigenwerte von $B_m$ jenen von $\mathcal{A}_m$ entsprechen. Wenn wir $m$ nicht zu groß gewählt haben, können wir diese mit dem aus Kapitel \ref{section_QR} bekannten QR-Verfahren mit wenig Aufwand berechnen.

Eine Möglichkeit zur Bestimmung einer Orthonormalbasis liefert das CG-Verfahren, auf welches an dieser Stelle nicht genauer eingegangen werden soll. Aus diesem ergibt sich der Ansatz, $B_m$ als eine Tridiagonalmatrix (also auch Hessenberg-Matrix) der Form

\begin{align}\label{trimatrix}
\begin{pmatrix}
	\gamma_0 & \delta_0 & 0 & \hdots & 0 \\
	\delta_0 & \gamma_1 & \delta_1 &  & \vdots \\
	0 & \delta_1 & \ddots & \ddots & 0 \\
	\vdots &  & \ddots & \ddots & \delta_{m-2} \\
	0 & \hdots & 0 & \delta_{m-2} & \gamma_{m-1}
\end{pmatrix}
\end{align}

aufzustellen. Die Gleichung \eqref{mateq} schreiben wir um als $AV_m = V_m B_m$, was spaltenweise zu folgenden Gleichungen führt:
\begin{align*}
	A v_0 &= \gamma_0 v_0 + \delta_0 v_1 \\
	A v_j &= \delta_{j-1} v_{j-1} + \gamma_j v_j + \delta_j v_{j+1}, \quad j=1, \dots, m-2.
\end{align*}

Da es sich bei $\{v_0, \dots, v_{m-1}\}$ um ein Orthogonalsystem handelt, liefert Multiplizieren der $j$-ten Gleichung von links mit $v_j^*$ die Bedingung $\gamma_j = v_j^*Av_j$ für $j = 0, \dots, m-2$. Somit können wir die $j$-te Gleichung rekursiv nach $v_{j+1}$ auflösen:

\begin{align*}
	v_{j+1} = \begin{cases}
		\frac{\overbrace{(A-\gamma_0)v_0}^{=:w_0}}{\delta_0} &,j = 0 \\
		\frac{\overbrace{(A-\gamma_j)v_j - \delta_{j-1} v_{j-1}}^{=:w_j}}{\delta_j} &,j \geq 1
	\end{cases}
\end{align*}

Hierbei setzen wir $\delta_j := \|w_j\|$ falls $\|w_j\| \neq 0$. Andernfalls bricht die Iteration ab. Daraus ergibt sich nun der Algorithmus für das Lanczos-Verfahren.

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}
	\label{Lanczos-Verfahren}
	\caption{Lanczos-Verfahren}
	\algorithmicrequire{ $A \in \C^{n\times n}$ hermitesche Matrix, zufälliger normierter Startvektor $v_0 \in \C^n$, maximale Krylov-Raum-Dimension $m$}
	\begin{algorithmic}[1]
		\State $\gamma_0 = v_0^*Av_0$
		\State $w_0 = (A-\gamma_0 I)v_0$
	  \State $\delta_0 = \|w_0\|_2$
		\While{$\delta_j \neq 0$ und $j < m$}
		\State $v_{j+1} = w_j/\delta_j$
		\State $j = j+1$
		\State $\gamma_j = v_j^*Av_j$
		\State $w_j = (A-\gamma_j I)v_j - \delta_{j-1}v_{j-1}$
		\State $\delta_j = \|w_j\|_2$
		\EndWhile
		\State Berechne mittels QR-Verfahren Eigenwerte von $T_j \in \C^{j \times j}$ aus \eqref{trimatrix} (Hessenberg-Matrix)
	\end{algorithmic}
	\algorithmicensure{Approximation an die Eigenwerte von $\mathcal{A}_j$}
\end{algorithm}


\subsection{Konvergenz der Eigenwerte von hermiteschen Matrizen}\label{konvergenz}

In diesem Abschnitt beweisen wir, dass die durch das Lanczos-Verfahren erhaltenen Eigenwerte $\lambda_1^{(m)},\dots,\lambda_m^{(m)}$ gegen Eigenwerte $\lambda_1,\dots,\lambda_m$ der hermiteschen Matrix $A \in \C^{n \times n}$ konvergieren. Die Argumentation ist dabei in großen Teilen angelehnt an \cite{EWPs}, Abschnitt 4.1.

Wir werden ein Lemma benötigen, welches Elemente des Krylov-Raums in Zusammenhang mit Polynomen charakterisiert.

\begin{lemma}
	\label{v0_Polynom_Darstellung}
	Sei $\Pi_m$ der Raum der Polynome in einer Veränderlichen mit maximalem Grad $m$. Dann ist $v \in \mathcal{K}_m(A,v_0) \subseteq \C^n$ genau dann, wenn ein Polynom $p \in \Pi_{m-1}$ existiert mit $v = p(A)v_0$. \newline
	Ist $A$ diagonalisierbar mit Eigenwerten $\lambda_1, \dots, \lambda_n$ und zugehörigen Eigenvektoren $u_1, \dots, u_n$, dann existiert eine eindeutige Darstellung $v_0 = \sum_{j=1}^{n} \alpha_j u_j$ und es gilt
	\begin{align*}
		v \in \mathcal{K}_m \Leftrightarrow \Exists p \in \Pi_{m-1}: v = \sum_{j=1}^{n} p(\lambda_j)\alpha_j u_j.
	\end{align*}
\end{lemma}

\begin{proof}
	Wir zeigen zunächst die erste Aussage. Nach Definition des Krylov-Raums gilt
	\begin{align}\label{Krylovproof}
		v \in \mathcal{K}_m \Leftrightarrow& \Exists a_0, \dots a_{m-1} \in \C: v = \sum_{i=0}^{m-1} a_i A^i v_0 = (\sum_{i=0}^{m-1} a_i A^i)v_0\\
		\Leftrightarrow& \Exists p: x \mapsto \sum_{i=0}^{m-1} a_i x^i \in \Pi_{m-1}: v = p(A)v_0.
	\end{align}

	Da die Eigenvektoren eine Basis des $\C^n$ bilden, folgt die eindeutige Darstellung von $v_0 = \sum_{j=1}^{n} \alpha_j u_j$. Setzen wir diese in \eqref{Krylovproof} ein und verwenden die Tatsache, dass $A^i u_j = \lambda_j^i u_j$ für $j=1,\dots, n $ und $i = 0, \dots, m-1$, so erhalten wir auch die zweite Aussage.
\end{proof}


\begin{definition}
	Für $m \in \N$ sind die Chebyshev-Polynome $T_m \in \Pi_m$ definiert durch
	\begin{align}\label{chebyshev}
		T_m(x) := \frac{1}{2}\Big((x+ \sqrt{x^2 - 1})^m + (x- \sqrt{x^2 - 1})^m\Big), \quad x \in \R.
	\end{align}
\end{definition}

\begin{remark}
	Es gibt weitere Definitionen der Chebyshev-Polynome. Die bekannteste ist
	\begin{align}\label{chebyalt}
		T_m(x) := \cos(m \arccos x), \quad x \in [-1,1],
	\end{align}

	welche aus der Identität $T_m(\cos \varphi) = \cos (m \varphi)$ für $\varphi \in \R$ folgt.

	Aus dieser Darstellung folgt auch mittels Additionstheorem die 2-Term-Rekursion
	\begin{align*}
		T_0(x) = 1, \quad T_1(x) = x, \quad T_{m}(x) = 2x T_{m-1}(x) - T_{m-2}(x), \quad m \geq 2,
	\end{align*}

	welche zeigt, dass es sich bei $T_m$ tatsächlich um ein Polynom vom Grad $m$ handelt.
\end{remark}

\begin{lemma}
	\label{lem:polminmax}
	Sei $[a,b]$ ein nicht-leeres Intervall in $\R$ und sei $c \geq b$. Dann gilt mit $\gamma := 1 + 2 \frac{c-b}{b-a} \geq 1$
	\begin{align}\label{polminmax}
		\min_{\substack{p \in \Pi_m \\
		p(c) = 1}} \max_{x \in [a,b]} |p(x)| \leq \frac{1}{|T_m(\gamma)|} \leq 2 (\gamma + \sqrt{\gamma^2 -1})^{-m}.
	\end{align}
\end{lemma}

\begin{proof}
	Wir verwenden die affin lineare Abbildung $\Phi: [a,b] \to [-1,1]$ mit
	\begin{align*}
		\Phi(x) = 1 + 2 \frac{x-b}{b-a}, \quad x \in [a,b].
	\end{align*}
	Wir definieren
	\begin{align*}
		\hat{p} := \frac{T_m \circ \Phi}{|T_m(\Phi(c))|}
	\end{align*}
	und können die Darstellung von $T_m$ aus \eqref{chebyalt} verwenden (da $\Phi$ nach $[-1,1]$ abbildet), aus der
	\begin{align*}
		\max_{x \in [a,b]}|\hat{p}(x)| \leq \frac{1}{|T_m(\gamma)|}
	\end{align*}
	folgt und daraus auch die erste Ungleichung.

	Die zweite Ungleichung folgt aus der Definition der Chebyshev-Polynome \eqref{chebyshev} und der Tatsache, dass $\gamma \geq 1$ und deshalb $|T_m(\gamma)| = T_m(\gamma) \geq \frac{1}{2} (\gamma+ \sqrt{\gamma^2-1})^m$ gilt.
\end{proof}

Mit den bis hier getroffenen Vorbereitungen können wir nun die Konvergenz des Projektionsverfahrens nachweisen.

\begin{theorem}[Konvergenz der Eigenwerte hermitescher Matrizen]
	Sei $A \in \C^{n \times n}$ eine hermitesche Matrix mit paarweise verschiedenen Eigenwerten $\lambda_1 > \lambda_2 > \dots > \lambda_n$. Bezeichne mit $u_1,\dots,u_n$ eine Orthonormalbasis aus Eigenvektoren zu den jeweiligen Eigenwerten. Für $1 \le m < n$ werden die Eigenwerte der linearen Abbildung $\mathcal{A}_m: \mathcal{K}_m(A,v_0)\rightarrow \mathcal{K}_m(A,v_0)$, die durch $v \mapsto \mathcal{P}_mAv$ gegeben ist,  mit $\lambda_1^{(m)} \ge \lambda_2^{(m)} \ge \dots \ge \lambda_m^{(m)}$ bezeichnet. Dabei ist $v_0$ ein beliebiger Startvektor, der nicht orthogonal zu den ersten $m-1$ Eigenvektoren von $A$ ist. Dann gilt

	\begin{equation}
		\label{konvergenz Eigenwerte}
		0 \le \lambda_i - \lambda_i^{(m)} \le (\lambda_i -\lambda_n) (\tan\theta_i)^2 \kappa_i^{(m)} \left(\frac{1}{T_{m-i}(\gamma_i)}\right)^2, \quad i=1,\dots,m-1,
	\end{equation}

	wobei
	\begin{equation*}
		\tan \theta_i \coloneqq \frac{\norm{(\id - \mathcal{P}_{u_i})v_0}}{\norm{ \mathcal{P}_{u_i}v_0}}, \quad \gamma_i \coloneqq 1+2 \frac{\lambda_i-\lambda_{i+1}}{\lambda_{i+1} -\lambda_n}
	\end{equation*}

	und
	\begin{equation*}
		\kappa_1^{(m)} \coloneqq 1, \quad \kappa_i^{(m)} \coloneqq \left(\prod_{k=1}^{i-1} \frac{\max_{j=i+1,\dots,n} (\lambda_k^{(m)} - \lambda_j)}{\lambda_k^{(m)} - \lambda_i}\right)^2, \quad i = 2,\dots,m.
	\end{equation*}

	Dabei wird mit $\mathcal{P}_{u_i}$ die Projektion auf den eindimensionalen Unterraum, der von $u_i$ aufgespannt wird, bezeichnet. Das $\theta_i$ kann somit als Winkel zwischen $v_0$ und $u_i$  verstanden werden.
	\begin{proof}

	Aufgrund der Übersichtlichkeit bezeichnen wir im folgenden Beweis mit $\mathcal{K}_m$ den Krylov-Raum $\mathcal{K}_m(A,v_0)$.\\
	Seien $v_1, \dots, v_m$ Orthonormalbasisvektoren von $\mathcal{K}_m$ und $V \in \C^{n\times m}$ die Matrix, die diese Vektoren als Spalten besitzt. Dann ist

	\begin{equation*}
		B_m \coloneqq V^* A V \in \C^{m\times m}
	\end{equation*}

	die Matrixdarstellung der Abbildung $\mathcal{A}_m$ bezüglich der Orthonormalbasis $\{v_1,\dots,v_m\}$.

	Diese Matrix ist offensichtlich hermitesch. Somit können die Eigenwerte $\lambda_1^{(m)}, \dots, \lambda_m^{(m)}$ mithilfe von \eqref{lambdamaxmin} berechnet werden.

	Es gilt also

	\begin{equation}
		\label{Eigenwert_Ungl}
		\begin{aligned}
			\lambda_k^{(m)} &= \max_{\substack{S \subseteq \C^m\\ \dim S = k}} \min_{v\in S\setminus\{0\}} \frac{(B_m v,v)}{(v,v)} \\
			&= \max_{\substack{S \subseteq \C^m\\ \dim S = k}} \min_{v\in S\setminus\{0\}} \frac{(AVv,Vv)}{(Vv,Vv)}\\
			&= \max_{\substack{T \subseteq \mathcal{K}_m \\ \dim T = k}} \min_{w\in T\setminus\{0\}} \frac{(Aw,w)}{(w,w)}\\
			 &\le \max_{\substack{T \subseteq \C^n\\ \dim T = k}} \min_{w\in T\setminus\{0\}} \frac{(Aw,w)}{(w,w)} = \lambda_k.
		\end{aligned}
	\end{equation}

	Dabei gilt die dritte Gleichheit, da $V$ als Abbildung von $\C^m$ nach $\mathcal{K}_m$ bijektiv ist. Also kann jeder $k$-dimensionale Unterraum $T \subseteq \mathcal{K}_m$ als Bild von einem $k$-dimensionalen Unterraum $S \subseteq \C^m$ unter der Abbildung $V$ gesehen werden.
	Somit ist die erste Ungleichung in \eqref{konvergenz Eigenwerte} erfüllt.\\

	Wir beweisen nun die zweite Ungleichung zuerst für den Fall $i = 1$.
	Sei $v_0 = \sum_{j=1}^{n}\alpha_j u_j$. Wir bemerken, dass $\alpha_1, \dots, \alpha_{m-1}$ nicht verschwinden, da $v_0$ nicht orthogonal zu diesen Eigenvektoren gewählt wurde. Dies rechtfertigt im Folgenden Abschätzungen, bei denen ohne diese Bedingung durch 0 geteilt werden würde.\\

	Mit analogen Umformungen wie in \eqref{Eigenwert_Ungl}, dem Satz von Pythagoras und Lemma \ref{v0_Polynom_Darstellung} gilt somit
	\begin{equation}
		\label{Ungl i=1}
		\begin{aligned}
			\lambda_1 - \lambda_1^{(m)} &= \lambda_1 - \max_{v \in \mathcal{K}_m \setminus\{0\}} \frac{(Av,v)}{(v,v)} = \min_{v \in \mathcal{K}_m\setminus\{0\}} \lambda_1 - \frac{(Av,v)}{(v,v)} = \min_{v \in \mathcal{K}_m\setminus\{0\}} \frac{\lambda_1(v,v) - (Av,v)}{(v,v)} \\ &= \min_{p\in \Pi_{m-1}\setminus \{0\}} \frac{\lambda_1 \norm{\sum_{j=1}^{n}p(\lambda_j)\alpha_j u_j}^2 - (\sum_{j=1}^{n}p(\lambda_j)\alpha_j A u_j,\sum_{j=1}^{n}p(\lambda_j)\alpha_j u_j)}{\norm{\sum_{j=1}^{n}p(\lambda_j)\alpha_j u_j}^2} \\
			&= \min_{p\in \Pi_{m-1}\setminus \{0\}} \frac{\lambda_1 \sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2 - \sum_{j=1}^{n} \lambda_j |p(\lambda_j)\alpha_j|^2 }{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			&\le \min_{\substack{p\in \Pi_{m-1} \\ p(\lambda_1) =1}} \frac{\sum_{j=2}^{n} (\lambda_1 - \lambda_j)|p(\lambda_j)\alpha_j|^2}{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			&\le (\lambda_1 - \lambda_n) \frac{\sum_{j=2}^{n} |\alpha_j|^2}{|\alpha_1|^2} \min_{\substack{p\in \Pi_{m-1} \\ p(\lambda_1) =1}} \max_{j=2,\dots,n} \left(\frac{|p(\lambda_j)|}{|p(\lambda_1)|}\right)^2.
		\end{aligned}
	\end{equation}

	Wir betrachten nun den Ausdruck $\tan \theta_i$. Für diesen gilt
		\begin{align}\label{tang}
			(\tan\theta_1)^2 = \frac{\norm{(\id - \mathcal{P}_{u_1})v_0}^2}{\norm{ \mathcal{P}_{u_1}v_0}^2} = \frac{\norm{v_0 - \alpha_1 u_1}^2}{\norm{\alpha_1 u_1}^2} = \frac{\sum_{j=2}^{n} |\alpha_j|^2}{|\alpha_1|^2}.
		\end{align}
	Die rechte Seite von \eqref{tang} finden wir auch in obiger Abschätzung und können sie also durch $(\tan\theta_1)^2$ ersetzen.
	Mithilfe der Ungleichung \eqref{polminmax} mit $a = \lambda_n, b= \lambda_2$ und $c= \lambda_1$ erhalten wir außerdem

		\begin{equation*}
			\min_{\substack{p\in \Pi_{m-1} \\ p(\lambda_1) = 1}} \max_{j=2,\dots,n} 	\left(\frac{|p(\lambda_j)|}{|p(\lambda_1)|}\right)^2 \le \min_{\substack{p\in \Pi_{m-1} \\ p(\lambda_1) = 1}} \max_{\lambda \in [\lambda_n,\lambda_2]} |p(\lambda)|^2 \le \left(\frac{1}{T_{m-1}(\gamma_1)}\right)^2.
		\end{equation*}
	Daraus folgt die gewünschte Ungleichung und \eqref{konvergenz Eigenwerte} ist für $i = 1$ erfüllt.\\

	Betrachten wir nun den Fall $i>1$.
	Bezeichne mit $\tilde{u}_j^{(m)} \in \C^m, \, j = 1,\dots, m$ eine Orthonormalbasis aus Eigenvektoren zu den Eigenwerten $\lambda_j^{(m)} , \, j = 1,\dots, m$ der Abbildung $B_m$. Mit \eqref{lambdamax} erhalten wir dann

	\begin{equation}
		\begin{aligned}
			\label{Darstellung EW}
			\lambda_i^{(m)} &= \max_{\substack{v\in \C^m\setminus \{0\} \\ (\tilde{u}_j^{(m)},v) = 0, j = 1,\dots,i-1}} \frac{(B_m v,v)}{(v,v)} \\
			&= \max_{\substack{v\in \C^m\setminus \{0\} \\ (\tilde{u}_j^{(m)},v) = 0, j = 1,\dots,i-1}} \frac{(AVv,Vv)}{(Vv,Vv)}\\
			&= \max_{\substack{w\in \mathcal{K}_m\setminus \{0\} \\ (V\tilde{u}_j^{(m)},w) = 0 j = 1,\dots,i-1}} \frac{(Aw,w)}{(w,w)}
		\end{aligned}
	\end{equation}

	Da die $\tilde{u}_j^{(m)}, j = 1,\dots,m$ eine Orthonormalbasis von $\C^m$ sind, sind somit die $u_j^{(m)} \coloneqq \nicefrac{V\tilde{u}_j^{(m)}}{\norm{V\tilde{u}_j^{(m)}}}$ eine Orthonormalbasis von $\mathcal{K}_m$. Sei nun $w\in \mathcal{K}_m$ gegeben durch $w = \sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k u_k^{(m)}$. Diese Darstellung erhalten wir, da $\C^m$ als ein Krylov-Raum $\mathcal{K}_m (B_m, \tilde{v_0})$ gesehen werden kann, wobei $V \tilde{v_0} = v_0$ sein soll. Der Raum $\mathcal{K}_m (B_m, \tilde{v_0})$ hat Dimension $m$, da

	\begin{equation*}
		\begin{aligned}
			\mathcal{K}_m (B_m, \tilde{v_0}) &= \mathcal{K}_m (V^*AV, \tilde{v_0})
			&= \textbf{span}\{V^*v_0, V^*Av_0, \dots, V^*A^{m-1}v_0\} = V^* \mathcal{K}_m
		\end{aligned}
	\end{equation*}

	gilt und $\mathcal{K}_m$ nach Vorraussetzung $m$-dimensional sein muss. Durch $V^*$ geht keine Dimension verloren, also gilt $\C^m = \mathcal{K}_m (B_m, \tilde{v_0})$

	Weil es für jedes $w\in \mathcal{K}_m$ ein $\tilde{w} \in \C^m$ gibt, sodass $w = V \tilde{w}$, folgt mit Lemma \ref{v0_Polynom_Darstellung} die Darstellung

	\begin{equation*}
		w = V \left(\sum_{k=1}^{m} p(\lambda_k^{(m)}) \tilde{\beta_k} \tilde{u}_j^{(m)}\right) = \sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k u_k^{(m)},
	\end{equation*}

	wobei $p \in \Pi_{m-1}$ ist.
	Dann gilt für alle $j < i$
	\begin{equation*}
		0 \stackrel{!}{=} (w,u_j^{(m)}) = (\sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k u_k^{(m)},u_j^{(m)}) = \sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k (u_k^{(m)},u_j^{(m)}) = p(\lambda_j^{(m)}) \beta_j.
	\end{equation*}

	Es gibt also zwei Möglichkeiten, diese Gleichheit zu erfüllen. Entweder ist $p(\lambda_j^{(m)}) = 0$ oder $\beta_j = 0$. Wir betrachten zuerst den Fall, dass  $\beta_j \neq 0$ für alle $j = 1,\dots, i-1$ gilt.

	Da dann die $\lambda_j^{(m)}, \, j = 1,\dots,i-1$ Nullstellen von $p$ sind, können wir

	\begin{equation*}
		p(\lambda) = \prod_{j = 1}^{i-1} \frac{\lambda_j^{(m)} -\lambda}{\lambda_j^{(m)} - \lambda_i} q(x)
	\end{equation*}
	schreiben, wobei $q \in \Pi_{m-i}$.\\

	Mit der Darstellung \eqref{Darstellung EW} von $\lambda_i^{(m)}$ gilt nach analogen Umformungen wie in \eqref{Ungl i=1}

	\begin{equation*}
		\begin{aligned}
			\lambda_i - \lambda_i^{(m)} &= \min_{\substack{p\in \Pi_{m-1}\setminus \{0\} \\ p(\lambda_j^{(m)}) = 0, j = 1,\dots,i-1}} \frac{\sum_{j=1}^{n} (\lambda_i - \lambda_j)|p(\lambda_j)\alpha_j|^2}{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			&= \min_{\substack{p\in \Pi_{m-1}\setminus \{0\} \\ p(\lambda_j^{(m)}) = 0, j = 1,\dots,i-1}} \frac{\sum_{\substack{j=1, j\neq i}}^{n} (\lambda_i - \lambda_j)|p(\lambda_j)\alpha_j|^2}{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			& \le (\lambda_i - \lambda_n) \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2} \min_{\substack{p\in \Pi_{m-1}, p(\lambda_i) \neq 0 \\ p(\lambda_j^{(m)}) = 0, j = 1,\dots,i-1}} \max_{j=i+1,\dots,n} \left(\frac{|p(\lambda_j)|}{|p(\lambda_i)|}\right)^2 \\
			&\le (\lambda_i - \lambda_n) \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2} \min_{\substack{q\in \Pi_{m-i} \\ q(\lambda_i) = 1}} \max_{j=i+1,\dots,n} \left(\left|\prod_{k = 1}^{i-1} \frac{\lambda_k^{(m)} -\lambda_j}{\lambda_k^{(m)} - \lambda_i}\right| \left|\frac{q(\lambda_j)}{q(\lambda_i)}\right|\right)^2 \\
			&\le (\lambda_i - \lambda_n) \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2} \left|\prod_{k = 1}^{i-1} \frac{\max_{j=i+1,\dots,n} (\lambda_k^{(m)} -\lambda_j)}{\lambda_k^{(m)} - \lambda_i}\right|^2 \min_{\substack{q\in \Pi_{m-i} \\ q(\lambda_i) = 1}} \max_{j=i+1,\dots,n} \left(\left|\frac{q(\lambda_j)}{q(\lambda_i)}\right|\right)^2. \\
		\end{aligned}
	\end{equation*}

	Wieder ist
	\begin{equation*}
		(\tan\theta_i)^2 = \frac{\norm{(\id - \mathcal{P}_{u_i})v_0}^2}{\norm{ \mathcal{P}_{u_i}v_0}^2} = \frac{\norm{v_0 - \alpha_i u_i}^2}{\norm{\alpha_i u_i}^2} = \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2}
	\end{equation*}

	und  mit $a = \lambda_n, b= \lambda_{i+1}$, $c = \lambda_i$ in Lemma \ref{lem:polminmax}, erhalten wir die Behauptung.\\

	Wenn nun $\beta_j$ existieren, die gleich 0 sind, kann man die Bedinungung $p(\lambda_j^{(m)}) = 0$ für die Polynome, über die minimiert wird, weglassen.
	Da somit mehr Polynome betrachtet werden, wird das Minimum sicherlich nicht größer. Das heißt, wir können für diesen Fall die gleiche Abschätzung verwenden.
	Also gilt \eqref{konvergenz Eigenwerte} für alle $i = 1, \dots, m-1$.
	\end{proof}
\end{theorem}

Die mit $m$ exponentielle Konvergenz der Eigenwerte ist nun ersichtlich, da mit \eqref{polminmax}
\begin{equation*}
	\left(\frac{1}{T_{m-i}(\gamma_i)}\right)^2 \le 4 \left(\gamma_i + \sqrt{\gamma_i^2 -1}\right)^{-2(m-i)}
\end{equation*}
gilt. Die Konvergenzgeschwindigkeit nimmt in der Theorie in Richtung der kleineren Eigenwerte ab, da der Grad des Chebyshev-Polynoms mit wachsendem $i$ kleiner wird. Wir bemerken noch, dass bei der Approximation von $\lambda_i$ bei Eigenwerten $\lambda_i \approx \lambda_j, \, j < i$, das Produkt $\kappa_i^{(m)}$ groß werden kann, da der Nenner wegen $\lambda^{(m)}_j - \lambda_i$ klein wird. Dies hat eine schlechtere Approximation zur Folge. Die anderen vorkommenden Terme wie $\tan \theta_i$ oder auch der Zähler vom Produkt in $\kappa_i^{(m)}$ haben für die Größenordnung der Abschätzung eher wenig Bedeutung. Wie sich die Konvergenz in der Praxis verhält, wird im Abschnitt \ref{ergebnisse} beschrieben.


\subsection{Implementierung und Arnoldi-Verfahren}

Die Implementation des Lanczos-Verfahrens entspricht dem Pseudocode und ist im Anhang zu finden. Hier werden wir besprechen, wo das Lanczos-Verfahren an seine Grenzen stößt. Da wir das Lanczos-Verfahren nur auf einfache Eigenwertprobleme und nicht auf verallgemeinerte Eigenwertprobleme der Form \eqref{discretization} anwenden können, müssen wir so ein verallgemeinertes Eigenwertproblem zuerst nach Lemma \ref{lemma_1_3} zu einem äquivalenten einfachen Eigenwertproblem umformulieren. Hier kommt Bemerkung \ref{remark_1_4} zu tragen, das Lanczos-Verfahren können wir nicht ohne weiteres anwenden.

Um diese Problematik zu umgehen, werden wir das sogenannte Arnoldi-Verfahren verwenden. Auch in diesem Verfahren bekommen wir eine Orthonormalbasis des Krylov-Raums und eine Matrix, welche die Abbildung $\mathcal{A}_m$ bezüglich dieser Basis beschreibt. Dabei ist $\mathcal{A}_m$ nicht mehr selbstadjungiert. In dem Arnoldi-Verfahren wird nun jeder neue Basisvektor mit dem (modifizierten) Gram-Schmidtschen Orthogonalisierungsverfahren gewonnen. Dabei wird ein neuer Basisvektor zu allen bisherigen Basisvektoren orthogonalisiert. Dies ist ein wesentlicher Unterschied zum Lanczos-Verfahren, wo nur gegen den vorherigen Basisivektor explizit orthogonalisiert werden muss.
Es macht Sinn, auch beim Lanczos-Verfahren eine volle Reorthogonalisierung durchzuführen. Diese ist zwar etwas aufwändiger, aber wenn nur gegen den vorherigen Basisvektor orthogonalisiert wird, kann es zu Auslöschung und somit zu Verlust der Orthogonalität kommen. Es gibt auch diverse Strategien, bei denen keine volle, sondern nur eine teilweise Reorthogonalisierung durchgeführt wird. Dazu gibt es eine Vielzahl an Literatur, wir verweisen hier auf \cite{reorthogonalization}.

Die Matrix, die man schließlich aus dem Arnoldi-Verfahren erhält, hat dann nicht Tridiagonalform, sondern ist eine obere Hessenberg-Matrix. Die Eigenwerte dieser können wir wieder mit dem QR-Verfahren für Hessenberg-Matrizen approximieren.
Das Lanczos-Verfahren ist so gesehen eigenlich ein Spezialfall des Arnoldi-Verfahrens - es ist das Arnoldi-Verfahren angewandt auf hermitesche Matrizen.

Das Arnoldi-Verfahren funktioniert nun nicht nur für hermitesche Matrizen, sondern auch für allgemeine. Für einen Konvergenzbeweis des Arnoldi-Verfahrens verweisen wir auf \cite{Saad_book}. Im Folgenden ist das Verfahren als Pseudocode formuliert.

\floatname{algorithm}{Algorithmus}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}[H]

	\caption{Arnoldi-Verfahren}
	\label{Arnoldi-Verfahren}
	\algorithmicrequire{ $A \in \C^{n\times n}$, zufälliger normierter Startvektor $v_0 \in \C^n$}
	\begin{algorithmic}[1]
		\For{$j = 1,\dots,k$}
			\State $w = A v_{j-1}$
			\For{$l = 1, \dots, j$}
				\State $h_{l,j} = (w, v_{l-1})$
				\State $w = w - h_{l,j}v_{l-1}$
			\EndFor
			\State $h_{j+1,j} = \norm[2]{w}$
			\If{$h_{j+1,j} = 0$}
			\State Ein invarianter Unterraum wurde gefunden
			\Else
				\State $v_j = \frac{w}{h_{j+1,j}}$
			\EndIf
		\EndFor
	\end{algorithmic}
	\algorithmicensure{Die Vektoren $v_0,\dots,v_{k-1}$ bilden eine Orthonormalbasis des Krylov-Raums, die Matrix $H$ ist eine Hessenberg-Matrix, die die Projektion von $A$ bezüglich dieser Basis darstellt}
\end{algorithm}


\subsection{Ergebnisse}\label{ergebnisse}

Zunächst wird das Lanczos-Verfahren mit zufällig generierten hermiteschen Matrizen mit bekannten Eigenwerten getestet. Dazu wird wieder ein zufälliges Array mit Eigenwerten generiert und auf die Diagonalmatrix mit diesen Eigenwerten eine Ähnlichkeitstransformation, hier mit zufälligen unitären Matrizen, angewendet. Dass dabei eine hermitesche Matrix entsteht, kann man schnell nachrechnen: sei dazu $A \in \R^{n\times n}$ eine Diagonalmatrix und $B \in \C^{n\times n}$ eine unitäre Matrix. Dann gilt also für die aus der Ähnlichkeitstransformation entstehende Matrix

\begin{align*}
	(B^* A B)^*
	=
	B^* A^* B^{**}
	=
		B^* A B.
\end{align*}

Erste Testungen mit zufällig generierten uniform verteilten Eigenwerten zeigen sehr mäßige Konvergenz. Lässt man dabei das gesamte Array mit den approximierten Eigenwerten ausgeben, zeigt sich eine gewisse Symmetrie, mit dem Fehler in den größten Eigenwerten sinkt auch der in den kleinsten Eigenwerten mit der selben Rate. In Abbildung \ref{error_uniform} ist der absolute Fehler in den 2 größten Eigenwerten für steigende Krylov-Raumdimension dargestellt. Dabei wurde das Lanczos-Verfahren auf eine Matrix der Dimension $1000$ mit Eigenwerten uniform verteilt im Intervall $(-100,400)$ angewendet. Selbst bei Krylov-Raumdimension 100 lässt sich noch keine besonders gute Approximation an nur zwei Eigenwerte erkennen. Für uniform verteilte Eigenwerte ist das Lanczos-Verfahren also nicht gut geeignet. Sobald jedoch Eigenwerte existieren, die sich weiter vom Spektrum entfernen, erhalten wir sehr schnelle Konvergenz gegen diese.

\begin{figure}[H]\label{error_uniform}
\begin{center}

	\includegraphics[width = 0.6 \linewidth]{Plots/error_uniform}
	\caption{Fehler im größten sowie zweitgrößten Eigenwert bei Verwendung des Lanczos-Verfahrens}
	\end{center}
\end{figure}

Diese Eigenschaft und andererseits die Symmetrie der Konvergenz kann man sehr gut in \ref{eigv_lanczos} erkennen. Dort wird das Lanczos-Verfahren auf eine $500 \times 500$ Matrix, mit Eigenwerten gleichverteilt im Intervall $(0,2)$ und $4$ extremalen Eigenwerten bei $-1$, $-0.5$, $2.5$ und $3$, angewendet. Bei $x = 12$ sieht man das gesamte Spektrum der Eigenwerte, wobei ein Eigenwert durch ein Kreuz mit dem entsprechenden Wert an der $y-$Achse dargestellt wird. Für $x = 2,...,11$ sind die Approximationen an die Eigenwerte bei entsprechender Krylov-Raumdimension dargestellt. Der größte sowie kleinste Eigenwert werden hier schon bei Krylov-Raumdimension $6$ gut approximiert. Bei Krylov-Raumdimension $10$ beginnt die rapide Konvergenz der Approximation an den zweitgrößten sowie zweitkleinsten Eigenwert.


\begin{figure}[H]\label{eigv_lanczos}
	\begin{center}
		\includegraphics[width = 0.7 \linewidth]{Plots/eigv_kyrlov_approx}
		\caption{Die Approximation an die Eigenwerte beim Lanczos-Verfahren}
	\end{center}
\end{figure}


Verwendet man nun also zur Testung keine uniform verteilten Eigenwerte, sondern lognormal verteilte, erhält man nun wesentlich bessere Konvergenzgeschwindigkeit. In Abbildung \ref{lanczos_lognomal} wird das Lanczos-Verfahren wieder auf eine Matrix der Dimension $1000$ angewendet, diesmal jedoch mit lognormal verteilen Eigenwerten ($\mu = 1, \sigma = 1$). Hier erkennt man die erwartete exponentielle Konvergenz, da der Fehler ja semilogarithmisch dargestellt wird.

\begin{figure}[H]
\begin{center}
	\includegraphics[width = 0.7 \linewidth]{Plots/lanczos_lognormal}\label{lanczos_lognomal}
	\caption{Fehler in den drei größten Eigenwerten bei Verwendung des Lanczos-Verfahrens mit lognormal-Verteilen Eigenwerten}
	\end{center}
\end{figure}


Die Testungen zeigen also, dass sich das Lanczos-Verfahren vorallem für Eigenwertprobleme eignet, bei denen sich einige Eigenwerte stark vom Spektrum abheben. Beim Arnoldi-Verfahren erkennt man bei Testung mit nun voll besetzten Matrizen ein ähnliches Verhalten. Auch hier stellt man wesentlich bessere Konvergenzgeschwindigkeit bei lognormal verteilen Eigenwerten fest, wie man in Abbildung \ref{Arnoldi_error} erkennen kann.

\begin{figure}[H]
  \centering
  \subfloat[][uniform verteilte Eigenwerten]{\includegraphics[width=0.45\linewidth]{Plots/error_arnoldi_uniform}}%
  \qquad
  \subfloat[][lognormal verteilte Eigenwerte]{\includegraphics[width=0.45\linewidth]{Plots/error_arnoldi_lognormal}}%
  \caption{Fehler in der Approximation bei Verwendung des Arnoldi-Verfahrens}%
  \label{Arnoldi_error}
\end{figure}

\begin{figure}\label{error_eigv_laplace}
	\centering
	\includegraphics[width = 0.9\linewidth]{Plots/error_eigv_laplace}
	\caption{Fehler in den ersten 40\% der Eigenwerte bei Anwendung des Arnoldi-Verfahrens auf das diskretisierte Problem}
\end{figure}

Wir wollen nun das Arnoldi-Verfahren verwenden, um unser Problem aus der Einleitung, die Eigenwertberechnung für $-\Delta$ auf einem Rechteck $\Omega$, zu lösen. Dabei verwenden wir zur Diskretisierung des Problems die Finite-Elemente-Methode. Hier kommen das Netgen sowie NG-Solve Paket zum Einsatz. Mit diesen können wir das Rechteck $\Omega := (0,2) \times (0,1)$ triangulieren und einen Finite-Elemente-Raum darauf erstellen. Aus NG-Solve bekommen wir dann unsere Matrizen, die genaue Ausführung dazu befindet sich im Anhang. Zum Vergleich verwenden wir die in Satz \ref{satz_1_2} berechneten Eigenwerte. Als shift in \eqref{equivdiscret} wird $\rho = 3$ gewählt. In diesem Schritt ist zusätzlicher Aufwand nötig, da in \eqref{equivdiscret} eine Inverse berechnet werden muss.

 In Abbildung \ref{error_eigv_laplace} sieht man den Fehler für verschiedene Krylov-Raumdimensionen. Dabei ist auf der $x-$Achse die Nummer des betrachteten Eigenwerts (aufsteigend sortiert) und auf der $y-$Achse je der Fehler der Approximation aus dem Arnoldi-Verfahren zu dem analytisch berechneten Eigenwert.
 Man erkennt eine gute Approximation in den ersten $40\%$ der berechneten Eigenwerte, wobei die Anzahl an berechneten Eigenwerten genau mit der Krylov-Raumdimension übereinstimmt. Die Schranke der $40\%$ wurde dabei aus skalierungstechnischen Gründen gewählt, man erhält sogar eine ziemlich gute Approximation an die ersten (also kleinsten) $45\%$ Eigenwerte, bei den größeren steigt der Fehler jedoch schnell und stark an. Im Bezug zu den vorhergegangenen Testungen scheinen die Eigenwerte der geshifteten Matrix aus dem verallgemeinerten Problem eine günstige Verteilung der Eigenwerte zu haben, bei dem sich einige extremale ausprägen.


\newpage
\pagestyle{plain}
\section*{Code}

\subsection*{QR-Verfahren ohne shifts}
\lstset{caption={Implementation des QR-Verfahren ohne shifts}}

\begin{lstlisting}
def QR_simple(A, tol = 1e-7):
	n = A.shape[1]
	count = 0
	for i in range(n-1,0,-1):
		while abs(A[i,i-1]) > tol:
				Q,R = np.linalg.qr(A)
				A = R@Q
				count +=1
		A[i,:i-n] = 0
return A, sorted(np.diag(A)), count
\end{lstlisting}



\subsection*{QR-Verfahren mit Rayleigh-Quotienten-Shift}
\lstset{caption={Implementation des QR-Verfahrens mit Rayleigh-Quotienten-Shift}}
\begin{lstlisting}
def QR_shift(A,tol=1e-10):
	n = A.shape[1]
	count = 0
	for i in range(n-1,0,-1):
			while abs(A[i,i-1]) > tol:
					rho = A[i,i]
					Q,R = np.linalg.qr(A-rho*np.identity(n))
					A = R@Q + rho*np.identity(n)
					count +=1
			A[i,:i-n] = 0
	return A, sorted(np.diag(A)), count
\end{lstlisting}


\subsection*{QR-Verfahren mit Wilkinson-Shift}

\lstset{caption={Implementation des QR-Verfahrens mit Wilkinson-Shift}}

\begin{lstlisting}
def QR_shift2(A,tol=1e-14):
    n = A.shape[1]
    count = 0
    for i in range(n-1,0,-1):
        while abs(A[i,i-1]) > tol*(abs(A[i-1,i-1])+abs(A[i,i])):
            w = np.linalg.eigvals(A[i-1:i+1,i-1:i+1])
            if abs(w[0] - A[i,i]) < abs(w[1] - A[i,i]):
                rho = w[0]
            else:
                rho = w[1]
            Q,R = np.linalg.qr(A-rho*np.identity(n))
            A = R@Q + rho*np.identity(n)
            count += 1
        A[i,:i-n] = 0
    return A, sorted(np.diag(A)), count
\end{lstlisting}

\newpage
\subsection*{QR-Zerlegung einer Hessenberg-Matrix mit Givens-Rotationen}

\lstset{caption={Implementation der QR-Zerlegung für Hessenberg-Matrizen mit Givens-Rotationen}}

\begin{lstlisting}
def QR_decomp_hesse(A):
    n = A.shape[0]
    Q = np.eye(n,n, dtype = complex)

    for i in range(0,n-1):
        if abs(A[i,i]) >= abs(A[i+1,i]):
            t = A[i+1,i]/abs(A[i,i])
            root = (1+abs(t)**2)**(1/2)
            c = A[i,i]/(abs(A[i,i])*root)
            s = t/root
        else:
            t = A[i,i]/abs(A[i+1,i])
            root = (1+abs(t)**2)**(1/2)
            s = A[i+1,i]/(abs(A[i+1,i])*root)
            c = t/root

        for j in range(n):
            if j < i:
                temp_2 = Q[i,j]
                Q[i,j] = c.conj()*temp_2 + s.conj()*Q[i+1,j]
                Q[i+1,j] = -s*temp_2 + c*Q[i+1,j]
            else:
                temp_1 = A[i,j]
                A[i,j] = c.conj()*temp_1 + s.conj()*A[i+1,j]
                A[i+1,j] = -s*temp_1 + c*A[i+1,j]
                temp_2 = Q[i,j]
                Q[i,j] = c.conj()*temp_2 + s.conj()*Q[i+1,j]
                Q[i+1,j] = -s*temp_2 + c*Q[i+1,j]

    return Q.T.conj(), A
\end{lstlisting}

\newpage
\subsection*{Berechnung von RQ für eine Hessenberg-Matrix mit Givens-Rotationen}

\lstset{caption={Implementation eines Schrittes des QR-Verfahrens für Hessenberg-Matrizen}}
\begin{lstlisting}
def Hessenberg_QR_RQ(A):
	n = A.shape[0]
	c_arr = np.zeros(n, dtype = complex)
	s_arr = np.zeros(n, dtype = complex)
	for i in range(0,n-1):
			if abs(A[i,i]) >= abs(A[i+1,i]):
					t = A[i+1,i]/abs(A[i,i])
					root = (1+abs(t)**2)**(1/2)
					c_arr[i] = A[i,i]/(abs(A[i,i])*root)
					s_arr[i] = t/root
			else:
					t = A[i,i]/abs(A[i+1,i])
					root = (1+abs(t)**2)**(1/2)
					s_arr[i] = A[i+1,i]/(abs(A[i+1,i])*root)
					c_arr[i] = t/root

			for j in range(i,n):
							temp_1 = A[i,j]
							A[i,j] = c_arr[i].conj()*temp_1 + s_arr[i].conj()*A[i+1,j]
							A[i+1,j] = -s_arr[i]*temp_1 + c_arr[i]*A[i+1,j]

	for i in range(0,n-1):
			for j in range(0,i+2):
					temp_1 = A[j,i]
					A[j,i] = c_arr[i]*temp_1 + s_arr[i]*A[j,i+1]
					A[j,i+1] = -s_arr[i].conj()*temp_1 + c_arr[i].conj()*A[j,i+1]

	return A
\end{lstlisting}


\subsection*{QR-Verfahren mit Wilkinson-Shift für Hessenberg-Matrix}

\lstset{caption={Implementation des QR-Verfahrens mit Wilkinson-Shift für Hessenberg-Matrizen}}

\begin{lstlisting}
def QR_hesse(A,tol=1e-14):
	n = A.shape[1]
	count = 0
	for i in range(n-1,0,-1):
			while abs(A[i,i-1]) > tol*(abs(A[i-1,i-1])+abs(A[i,i])):
					w = np.linalg.eigvals(A[i-1:i+1,i-1:i+1])
					if abs(w[0] - A[i,i]) < abs(w[1] - A[i,i]):
							rho = w[0]
					else:
							rho = w[1]
					A = Hessenberg_QR_RQ(A-rho*np.identity(n))
					A += rho*np.identity(n)
					count += 1
			A[i,:i-n] = 0
	return A, sorted(np.diag(A)), count
\end{lstlisting}

\newpage
\subsection*{Lanczos-Verfahren}

\lstset{caption={Implementation des Lanczos-Verfahrens}}

\begin{lstlisting}
def lanczos(A,m = 0):
    n = A.shape[1]
    if m == 0:
        m = n

    v0 = np.random.rand(n)
    v = [v0/np.linalg.norm(v0)]
    gam = [v[0].T.conj()@A@v[0]]
    w = (A - gam[0]*np.identity(n))@v[0]
    delta = [np.linalg.norm(w)]
    j = 0
    while delta[j] > 1e-10 and j < m-1:
        v.append(w/delta[j])
        j +=1
        gam.append(v[j].T.conj()@A@v[j])
        w = (A - gam[j]*np.identity(n))@v[j] - delta[j-1]*v[j-1]
        delta.append(np.linalg.norm(w))
    T = np.diag(delta[:-1], -1) + np.diag(gam) + np.diag(delta[:-1], 1)
    return QR_hesse(T)
\end{lstlisting}

\subsection*{Arnoldi-Verfahren}

\lstset{caption ={Implementation des Arnoldi-Verfahrens}}

\begin{lstlisting}
def arnoldi(A, dim, k = 0):
	n = dim
	if k == 0:
			k = n

	v0 = np.random.rand(n)
	v = [v0/np.linalg.norm(v0)]
	h = np.zeros((k,k))

	for j in range(k):
			w = A(v[j])
			for l in range(j+1):
					h[l][j] = v[l].T.conj()@w
					w = w - h[l][j]*v[l]
			if j < k-1:
					h[j+1][j] = np.sqrt(w.T.conj()@w)
					if abs(h[j+1][j]) < 1e-14:
							return QR_hesse(h[:j+1,:j+1])
					else:
							v.append(w/h[j+1][j])
	return QR_hesse(h)
\end{lstlisting}
\newpage
\subsection*{Code zur Diskretisierung mit NG-Solve}

\lstset{caption={Code mit Parametern die bei der Testung des Arnoldi-Verfahrens verwendet werden}}

\begin{lstlisting}
a = 2
b = 1
maxh = 0.05
order = 4

geo = SplineGeometry()
geo.AddRectangle((0,0), (a,b), bcs=["b","r","t","l"])

mesh = Mesh(geo.GenerateMesh(maxh=maxh))
Draw(mesh)

fes = H1(mesh, complex=True, order=order)

eigenvec = GridFunction(fes,multidim=nr_eigs)

u = fes.TrialFunction()
v = fes.TestFunction()

a = BilinearForm (fes)
a += SymbolicBFI (grad(u)*grad(v))

b = BilinearForm (fes)
b += SymbolicBFI (u*v)

a.Assemble()
b.Assemble()

A=a.mat
B=b.mat

shift = 3

shifted = a.mat.CreateMatrix()
shifted.AsVector().data = a.mat.AsVector() - (shift**2)*b.mat.AsVector()
invshifted = shifted.Inverse(freedofs=fes.FreeDofs())

tmp1 = eigenvec.vec.CreateVector()
tmp2 = eigenvec.vec.CreateVector()

def matvec(v):
    tmp1.FV().NumPy()[:] = v
    tmp2.data = b.mat * tmp1
    tmp1.data = invshifted * tmp2
    return tmp1.FV().NumPy()

A = scipy.sparse.linalg.LinearOperator( (a.mat.height,a.mat.width), matvec)
\end{lstlisting}
\newpage

\printbibliography

\end{document}

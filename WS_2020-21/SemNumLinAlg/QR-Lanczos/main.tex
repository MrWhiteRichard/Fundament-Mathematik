\documentclass{article}

\input{../../../Fundament-LaTeX/packages.tex}
\input{../../../Fundament-LaTeX/macros.tex}
\input{../../../Fundament-LaTeX/environments.tex}

\parindent 0pt

\begin{document}
\title
{
	Eigenwertberechnung mithilfe des Lanczos-Verfahren
}
\author
{
	Göth Christian
	\and
	Moik Matthias
	\and
	Sallinger Christian
}
\date{\today}
\maketitle

\newpage
\tableofcontents
\newpage


\section{Einleitung}

Das folgende Projekt beschäftigt sich mit der numerischen Berechnung von Eigenwerten mithilfe des Lanczos-Verfahrens. Zur Motivation dieser Berechnung betrachten wir das Eigenwertproblem des negativen Laplace-Operators mit homogenene Neumann-Randbedingungen auf einem zweidimensionalen Gebiet $\Omega$.

\subsection{Problemstellung}

Wir wiederholen zunächst die Definition eines Eigenwertproblems.

\begin{definition}
	Das Paar $(\lambda, u) \in \C \times H^2(\Omega) \setminus \{0\}$ heisst ein Eigenpaar, wenn es eine Lösung des Eigenwertproblems
	\begin{align}
	    \begin{cases}
	    -\Delta u = \lambda u & \text{in } \Omega, \\
	    \frac{\partial u}{\partial \nu} = 0 & \text{auf}~ \partial\Omega,
	    \end{cases}
	    \label{neumann}
	\end{align}
	ist, wobei $\nu$ der äußere Normalenvektor an $\partial\Omega$ sei.
\end{definition}

Um die aus dem numerischen Verfahren gewonnen Eigenwerte mit den tatsächlichen Eigenwerten vergleichen zu können, wählen wir als Gebiet das Rechteck $\Omega := (0,a) \times (0,b)$, auf dem sich diese analytisch berechnen lassen.

\subsection{Analytische Lösung für ein Rechteck}

Die Eigenwerte und Eigenfunktionen des oben genannten Problems werden zunächst mit einem Separationsansatz berechnet.

\begin{theorem}
	Die Eigenfunktionen für das Eigenwertproblem \ref{neumann} auf dem Gebiet $\Omega := (0,a) \times (0,b)$ sind gegeben durch
	\begin{align*}
		u_{n,m}(x,y) = \cos(\frac{n \pi}{a}x)\cos(\frac{m \pi}{b}y), \quad n,m \in \N
	\end{align*}
	und die zugehörigen Eigenwerte durch
	\begin{align*}
		\lambda_{n,m} = \pi^2(\frac{n^2}{a^2} + \frac{m^2}{b^2}), \quad n,m \in \N.
	\end{align*}
\end{theorem}

\begin{proof}
	Wir machen einen Separationsansatz. Sei $u(x,y) = v(x)w(y)$, die Differentialgleichung ergibt dann
	\begin{align*}
		- \Delta u(x,y) = -v''(x) w(y) - v(x) w''(y) = \lambda v(x)w(y)
	\end{align*}
	Multiplizieren mit $\frac{1}{v(x)w(y)}$ liefert
	\begin{align*}
		-\frac{v''(x)}{v(x)} - \frac{w''(y)}{w(y)} &= \lambda \\
		\Leftrightarrow \quad -\frac{v''(x)}{v(x)} &= \lambda + \frac{w''(y)}{w(y)}
	\end{align*}
	Da die linke Seite nur von $x$ abhängt und die rechte nur von $y$, müssen beide konstant sein. Es gibt also ein $\kappa_{x}^2$, sodass
	\begin{align*}
		-\frac{v''(x)}{v(x)} = \kappa_{x}^2, \quad \Forall x \in (0,a) \\
		\Rightarrow v''(x) = -\kappa_{x}^2 v(x) \\
		\Rightarrow v(x) = c_1 \sin(\kappa_{x} x) + c_2 \cos(\kappa_{x} x)
	\end{align*}

	Analog erhält man $w(y) = c_3 \sin(\kappa_{y} y) + c_4 \cos(\kappa_{y} y)$, wobei $\lambda = \kappa_{x}^2 + \kappa_{y}^2$ gelten muss.

	Nun lassen wir noch die Neumann-Randbedingungen einfließen. Mit $\nabla u(x,y) = (v'(x)w(y), v(x)w'(y))$ erhalten wir für den linken Rand
	\begin{align*}
		\frac{\partial u}{\partial \nu}(0,y) = - v'(0)w(y) = \kappa_{x}(- c_1 \cos(\kappa_{x} \cdot 0) + c_2 \underbrace{\sin(\kappa_{x} \cdot 0)}_{=0})w(y) \stackrel{!}{=} 0
	\end{align*}
	Es muss also $c_1 = 0$ gelten. Analog schließt man auch $c_3 = 0$.

	Für den rechten Rand gilt nun
	\begin{align*}
		\frac{\partial u}{\partial \nu}(a,y) = v'(a)w(y) = \kappa_{x} c_2 \sin(\kappa_{x} \cdot a)w(y) \stackrel{!}{=} 0 \\
		\Rightarrow \kappa_x = \frac{n \pi}{a}, \quad n \in \N
	\end{align*}

	Wiederum analog schließt man $\kappa_y = \frac{m \pi}{b}, m \in \N$. Durch die Wahl $c_2 = c_4 := 1$ erhalten wir also die Eigenfunktionen $u_{n,m}(x,y) = \cos(\frac{n \pi}{a}x)\cos(\frac{m \pi}{b}y)$ mit Eigenwerten $\lambda_{n,m} = \frac{n^2 \pi^2}{a^2} + \frac{m^2 \pi^2}{b^2}$.
\end{proof}

\subsection{Numerischer Lösungsansatz}

Das Eigenwertproblem \ref{neumann} kann durch Multiplizieren mit $v \in H^1(\Omega)$ und integrieren auf schwache Form gebracht werden (beim Anwenden des Satzes von Gauß verschwindet der Randterm aufgrund der homogenen Randbedingungen). Gesucht sind also Lösungen $\lambda, u) \in \C \times H^1(\Omega) \setminus \{0\}$ von

\begin{align*}
	\Int[\Omega]{\nabla u \nabla v}{x} = \lambda \Int[\Omega]{uv}{x}, \quad \Forall v \in H^1(\Omega)
\end{align*}

Dieses Problem kann mittels Finite-Elemente-Methoden auf ein endlichdimensionales verallgemeinertes Eigenwertproblem der Form

\begin{align} \label{discretization}
	A x_h = \lambda_h B x_h
\end{align}

gebracht werden, mit $A, B \in \R^{N \times N}$ und symmetrisch.

\begin{lemma}
	Mit $\rho_h \in \R$, welches kein Eigenwert ist, und $\lambda_h = \frac{1}{\mu_h} + \rho_h$ ist das verallgemeinerte Problem \ref{discretization} äquivalent  zu
	\begin{align} \label{equivdiscret}
		(A - \rho_h B)^{-1} B x_h = \mu_h x_h
	\end{align}
\end{lemma}

\begin{proof}
	\begin{align*}
		A x = \lambda B x =& A x = (\frac{1}{\mu} + \rho) B x \\
		\Leftrightarrow (A - \rho B)x =& \frac{1}{\mu} B x \\
		\Leftrightarrow \mu x =& (A - \rho B)^{-1}B x
	\end{align*}
\end{proof}

\begin{remark}
	Die Matrix $(A - \rho_h B)^{-1} B$ im Eigenwertproblem \ref{equivdiscret} ist zwar Produkt zweier symmetrischer Matrizen, aber i. A. nicht mehr symmetrisch. Dies wird im Laufe des Projektes noch wichtig sein, da das Lanczos-Verfahren nur für symmetrische Matrizen funktioniert.
\end{remark}

Da uns die Diskretisierung durch Finite-Elemente-Methoden Matrizen $A$ und $B$ mit sehr großer Dimension $N$ liefert, ist es also nun sinnvoll, ein Verfahren zu entwickeln, dass uns für sehr große Matrizen eine gute Approximation zumindest an die kleinsten Eigenwerte liefert. Das Lanczos-Verfahren wird dies unter Zuhilfenahme des QR-Verfahrens bewerkstelligen.

\section{Das QR-Verfahren}

\subsection{Idee und Algorithmus}

Das QR-Verfahren ist eine Methode zur Eigenwertbestimmung einer Matrix $A \in \K^{n \times n}$ mit Eigenwerten $\lambda_1, ..., \lambda_n$. Dabei wird iterativ eine Folge $(A^{(t)})_{t \in \N}$ definiert, wobei $A^{(t)} = R^{(t-1)}Q^{(t-1)}$ mit den Matrizen $Q^{(t-1)}$ und $R1{(t-1)}$ aus der QR-Zerlegung $A^{(t-1)} = Q^{(t-1)}R^{(t-1)}$ berechnet wird. Diese Folge von Matrizen konvergiert gegen eine obere Dreiecksmatrix, die die gleichen Eigenwerte wie $A$ besitzt.

\floatname{algorithm}{Algorithmus}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}
	\caption{QR-Verfahren}
	\label{QR-Verfahren}
	\algorithmicrequire{ $A \in \K^{n\times n}$}
	\begin{algorithmic}[1]			
		\While{Abbruchbedingung nicht erfüllt}
		\State Berechne QR-Zerlegung von $A$
		\State $A \coloneqq RQ$
		\EndWhile
	\end{algorithmic}
	\algorithmicensure{ $A$ als Approximation einer oberen Dreiecksmatrix mit gleichen Eigenwerten wie $A$ anfangs hatte}
\end{algorithm}


Wir führen einen Satz an, der unter gewissen Vorraussetzungen die Konvergenz dieses Algorithmus beweist. Für den Beweis verweisen wir auf \cite{Nannen-Skript}, Satz 9.15.

\begin{theorem}[Konvergenz des QR-Verfahrens]
	\label{Konv_QR}
	Sei $A \in \K^{n\times n}$ eine Matrix mit Eigenwerten $\lambda_1,\dots,\lambda_n \in \K$ und $|\lambda_1| > |\lambda_2| > \dots > |\lambda_n| > 0$. Dann konvergieren die Hauptdiagonaleinträge der Matrizen $A^{(t)}$, die in Algorithmus \ref{QR-Verfahren} definiert sind, gegen die Eigenwerte von $A$.
\end{theorem}


\subsection{Beschleunigung der Konvergenz}

Wir betrachten die Matrizenfolge $\left(M^{(t)}\right)_{t \in \N}$, die im Beweis von Satz \ref{Konv_QR} vorkommt. Diese Matrizen haben die Form

\begin{equation*}
	M^{(t)} \coloneqq
	\left( \begin{array}{rrrr}
		1 & 0 & \dots & 0 \\
		m_{21} \left(\frac{\lambda_2}{\lambda_1}\right)^t & 1 & \ddots \vdots \\
		\vdots & \ddots & \ddots & 0 \\
		m_{n1} \left(\frac{\lambda_n}{\lambda_1}\right)^t & \dots & m_{n(n-1)} \left(\frac{\lambda_n}{\lambda_{n-1}}\right)^t & 1 \\
	\end{array}\right).
\end{equation*}

Da die Eigenwerte der Größe nach sortiert sind, erkennt man, dass diese Matrix gegen die Einheitsmatrix konvergiert. Dies zieht schlussendlich die Konvergenz von $(A^{(t)})_{t\in \N}$ gegen eine obere Dreiecksmatrix nach sich.
Die Einträge konvergieren schneller gegen $0$, wenn der Bruch kleiner ist. Wir versuchen das zu erreichen, in dem wir einen Shift durchführen. Für einen Eigenwert $\lambda$ einer Matrix $A$ gilt

\begin{equation*}
	(A - \lambda \id) = 0 \Leftrightarrow ((A - \rho \id) - (\lambda -\rho) \id) = 0 \Leftrightarrow (\lambda-\rho) \text{ ist Eigenwert von }  (A - \rho \id).
\end{equation*}

Die Wahl des Shifts als $a^{(t)}_{nn}$ macht also sinn, da dieser Eintrag für $t \rightarrow \infty$ gegen $\lambda_n$ konvergiert. Dieser Shift wird auch Rayleigh-Quotienten-Shift genannt. Nach der QR-Zerlegung von $A^{(t)}$ setzt man $A^{(t+1)} \coloneqq R^{(t)}Q^{(t)} + \rho \id$, um wieder eine Matrix zu erhalten, die die gleichen Eigenwerte wie $A^{(t)}$ und somit auch wie $A$ besitzt.
Je nach dem, wie genau $\lambda_n$ approximiert werden soll, wählt man eine Toleranz, die von den Einträgen $a^{(t)}_{ni}, \, i = 1,\dots, n-1$ unterschritten werden muss, bevor analoge Schritte für den zweitkleinsten Eigenwert durchgefürht werden. Dessen Näherung steht in $a^{(t)}_{(n-1)(n-1)}$.

Eine weitere Möglichkeit, den Shift zu wählen, ist sich die Eigenwerte $\rho_1, \rho_2$ der $2 \times 2$ Matrix rechts unten von $A^{(t)}$ zu berechnen und anschließend den Eigenwert, der betragsmäßig näher an $a^{(t)}_{nn}$ liegt, zu wählen. Die Konvergenzgeschwindigkeit wird dadurch nochmals verbessert. Vergleiche dazu \cite{Nannen-Skript}. Dieser Shift wird Wilkinson-Shift genannt. 

In unserer Implementierung des QR-Verfahrens verwenden wir die letzte angeführte Methode.

\begin{algorithm}
	\label{QR-Verfahren_shifts}
	\caption{Verbessertes QR-Verfahren}
	\algorithmicrequire{ $A \in \K^{n\times n}$}
	\begin{algorithmic}[1]
		\For{$i = n,\dots, 2$}
		\While{Abbruchbedingung nicht erfüllt}
		\State Berechne die Eigenwerte $\rho_1$, $\rho_2$ von $\left(\begin{array}{rr}
			a&b\\
			c&d\\
		\end{array}\right)$
		\If{$|\rho_1 - a_{ii}| < |\rho_2 - a_{ii}|$}
		\State $\rho = \rho_1$
		\Else
		\State $\rho = \rho_2$
		\EndIf
		\State Berechne QR-Zerlegung von $A - \rho \id$
		\State $A = RQ + \rho \id$
		\EndWhile
		\EndFor
	\end{algorithmic}
	\algorithmicensure{ $A$ als Approximation einer oberen Dreiecksmatrix mit gleichen Eigenwerten wie $A$ anfangs hatte}
\end{algorithm}

Eine weitere Beschleunigung erreichen wir bei der QR-Zerlegung, wenn die Matrix $A$ Hessenbergform hat.

\subsection{Die QR-Zerlegung für Hessenbergmatrizen}

\subsection{Implementierung und Vergleich der Verschiedenen Methoden}


\section{Das Lanczos-Verfahren}

\subsection{Idee}

\subsection{Konvergenz der Eigenwerte von hermiteschen Matrizen}

In diesem Abschnitt beweisen wir die zentrale Aussage, dass die durch das Lanczos-Verfahren erhaltenen Eigenwerte $\lambda_1^{(m)},\dots,\lambda_m^{(m)}$ gegen Eigenwerte $\lambda_1,\dots,\lambda_m$ der hermiteschen Matrix $A \in \K^{n \times n}$ konvergieren.

%vorbereitungs lemmata


\begin{theorem}[Konvergenz der Eigenwerte hermitscher Matrizen]
	Sei $A \in \K^{n \times n}$ eine hermitesche Matrix mit paarweise verschiedenen Eigenwerten $\lambda_1 > \lambda_2 > \dots > \lambda_n$ und der zugehörigen Orthonormalbasis aus Eigenvektoren $u_1,\dots,u_n$. Für $1 \le m < n$ werden die Eigenwerte der linearen Abbildung $\mathcal{A}_m: \mathcal{K}_m(A,v_0)\rightarrow \mathcal{K}_m(A,v_0)$, die durch $v \rightarrowtail \mathcal{P}_mAv$ gegeben ist,  mit $\lambda_1^{(m)} \ge \lambda_2^{(m)} \ge \dots \ge \lambda_m^{(m)}$ bezeichnet. Dabei ist $v_0$ ein beliebiger Startvektor, der nicht orthogonal zu den ersten $m-1$ Eigenvektoren von $A$ ist. Dann gilt

	\begin{equation}
		\label{konvergenz Eigenwerte}
		0 \le \lambda_i - \lambda_i^{(m)} \le (\lambda_i -\lambda_n) (\tan\theta_i)^2 \kappa_i^{(m)} \left(\frac{1}{T_{m-i}(\gamma_i)}\right)^2, \quad i=1,\dots,m-1
	\end{equation}

	wobei $\theta_i$ der Winkel zwischen $u_i$ und $v_0$, also
	
	\begin{equation*}
		\tan \theta_i \coloneqq \frac{\norm{(\id - \mathcal{P}_{u_i})v_0}}{\norm{ \mathcal{P}_{u_i}v_0}},
	\end{equation*}
	wobei $\mathcal{P}_{u_i}$ die Projektion auf $u_i$ ist,
	\begin{equation*}
		\gamma_i \coloneqq 1+2 \frac{\lambda_i-\lambda_{i+1}}{\lambda_{i+1} -\lambda_n}
	\end{equation*}
und
	\begin{equation*}
		\kappa_1^{(m)} \coloneqq 1, \quad \kappa_i^{(m)} \coloneqq \left(\prod_{j=1}^{i-1} \frac{\lambda_j^{(m)} - \lambda_n}{\lambda_j^{(m)} - \lambda_i}\right)^2, \quad i = 2,\dots,m.
	\end{equation*}

	\begin{proof}

	Seien $v_1, \dots, v_m$ Orthonormalbasisvektoren von $\mathcal{K}_m$ und $V^{(m)} \in \K^{n\times m}$ die Matrix, die diese Vektoren als Spalten besitzt. Dann ist

	\begin{equation*}
		A^{(m)} \coloneqq V^* A V \in \K^{m\times m}
	\end{equation*}

	die Matrixdarstellung der Abbildung $\mathcal{A}_m$ bezüglich der Orthonormalbasis $\{v_1,\dots,v_m\}$.

	Diese Matrix ist offensichtlich hermitesch. Somit können die Eigenwerte $\lambda_1^{(m)}, \dots, \lambda_m^{(m)}$ mithilfe von (...) berechnet werden.

	Es gilt also

	\begin{equation}
		\label{Eigenwert_Ungl}
		\begin{aligned}
			\lambda_k^{(m)} &= \max_{\substack{S \subseteq \C^m\\ \dim S = k}} \min_{v\in S\setminus\{0\}} \frac{(A^{(m)}v,v)}{(v,v)} \\
			&= \max_{\substack{S \subseteq \C^m\\ \dim S = k}} \min_{v\in S\setminus\{0\}} \frac{(AVv,Vv)}{(Vv,Vv)}\\
			&= \max_{\substack{S \subseteq \mathcal{K}_m\\ \dim S = k}} \min_{w\in S\setminus\{0\}} \frac{(Aw,w)}{(w,w)}\\
			 &\le \max_{\substack{S \subseteq \C^n\\ \dim S = k}} \min_{w\in S\setminus\{0\}} \frac{(Aw,w)}{(w,w)} = \lambda_k
		\end{aligned}
	\end{equation}

	Somit ist die erste Ungleichung erfüllt.\\

	Wir beweisen nun die zweite Ungleichung zuerst für den Fall $i = 1$.
	Sei $v_0 = \sum_{j=1}^{n}\alpha_j u_j$. Wir bemerken, dass $\alpha_1, \dots, \alpha_{m-1}$ nicht verschwinden, da $v_0$ nicht orthogonal zu diesen Eigenvektoren gewählt wurde.\\

	Mit analogen Umformungen wie in \ref{Eigenwert_Ungl}, dem Satz von Pythagoras und Lemma (...) gilt somit
	\begin{equation}
		\label{Ungl i=1}
		\begin{aligned}
			\lambda_1 - \lambda_1^{(m)} &= \lambda_1 - \max_{v \in \mathcal{K}_m \setminus\{0\}} \frac{(Av,v)}{(v,v)} = \min_{v \in \mathcal{K}_m\setminus\{0\}} \lambda_1 - \frac{(Av,v)}{(v,v)} = \min_{v \in \mathcal{K}_m\setminus\{0\}} \frac{\lambda_1(v,v) - (Av,v)}{(v,v)} \\ &= \min_{p\in \Pi_{m-1}\setminus \{0\}} \frac{\lambda_1 \norm{\sum_{j=1}^{n}p(\lambda_j)\alpha_j u_j}^2 - (\sum_{j=1}^{n}p(\lambda_j)\alpha_j A u_j,\sum_{j=1}^{n}p(\lambda_j)\alpha_j u_j)}{\norm{\sum_{j=1}^{n}p(\lambda_j)\alpha_j u_j}^2} \\
			&= \min_{p\in \Pi_{m-1}\setminus \{0\}} \frac{\lambda_1 \sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2 - \sum_{j=1}^{n} \lambda_j |p(\lambda_j)\alpha_j|^2 }{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			&=  \min_{p\in \Pi_{m-1}\setminus \{0\}} \frac{\sum_{j=2}^{n} (\lambda_1 - \lambda_j)|p(\lambda_j)\alpha_j|^2}{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			&\le (\lambda_1 - \lambda_n) \frac{\sum_{j=2}^{n} |\alpha_j|^2}{|\alpha_1|^2} \min_{p\in \Pi_{m-1}\setminus \{0\}} \max_{j=2,\dots,n} \left(\frac{|p(\lambda_j)|}{|p(\lambda_1)|}\right)^2.
		\end{aligned}
	\end{equation}

	Wir bezeichnen mit $\mathcal{P}_{u_i}, \, i = 1,\dots, m-1$ die Projektion auf den Vektor $u_i$. Damit gilt
		\begin{equation*}
			(\tan\theta_1)^2 = \frac{\norm{(\id - \mathcal{P}_{u_1})v_0}^2}{\norm{ \mathcal{P}_{u_1}v_0}^2} = \frac{\norm{v_0 - \alpha_1 u_1}^2}{\norm{\alpha_1 u_1}^2} = \frac{\sum_{j=2}^{n} |\alpha_j|^2}{|\alpha_1|^2}
		\end{equation*}
	und mithilfe von Lemma (...) mit $a = \lambda_n, b= \lambda_2$ und $c= \lambda_1$

		\begin{equation*}
			\min_{p\in \Pi_{m-1}\setminus \{0\}} \max_{j=2,\dots,n} 		\left(\frac{|p(\lambda_j)|}{|p(\lambda_1)|}\right)^2 \le \min_{p\in \Pi_{m-1}\setminus \{0\}} \max_{\lambda \in [a,b]} \left(\frac{|p(\lambda)|}{|p(\lambda_1)|}\right)^2 \le \left(\frac{1}{T_{m-1}(\gamma_1)}\right)^2.
		\end{equation*}
	Daraus folgt die gewünschte Ungleichung und \ref{konvergenz Eigenwerte} ist für $i = 1$ erfüllt.

	Betrachten wir nun den Fall $i>1$.
	Bezeichne mit $u_j^{(m)}, \, j = 1,\dots, m$ die Eigenvektoren zu den Eigenwerten $\lambda_j^{(m)} , \, j = 1,\dots, m$. Mit (...) erhalten wir dann

	\begin{equation*}
		\lambda_i^{(m)} = \max_{\substack{v\in \C^n\setminus \{0\} \\ (u_j^{(m)},v) = 0, j = 1,\dots,i-1}} \frac{(A^{(m)}v,v)}{(v,v)} = \max_{\substack{v\in \C^n\setminus \{0\} \\ (u_j^{(m)},v) = 0, j = 1,\dots,i-1}} \frac{(AVv,Vv)}{(Vv,Vv)} = \max_{\substack{w\in \mathcal{K}_m\setminus \{0\} \\ (Vu_j^{(m)},w) = 0, j = 1,\dots,i-1}} \frac{(Aw,w)}{(w,w)}
	\end{equation*}


	Definiere  $\tilde{u}_j^{(m)}$ als den normierten Vektor $Vu_j$. Da $A^{(m)}$ hermitesch ist, gilt für alle $i,j = 1,\dots, m$

	\begin{equation*}
		\lambda^{(m)}_i (u_i^{(m)}, u_j^{(m)}) = (A^{(m)}u_i^{(m)}, u_j^{(m)}) = (u_i^{(m)}, A^{(m)}u_j^{(m)}) = \lambda^{(m)}_j (u_i^{(m)}, u_j^{(m)}).
	\end{equation*}

	Also sind Eigenvektoren zu verschiedenen Eigenwerten orthogonal. Wegen $ 0 = (u_i, u_j) = (Vu_i, Vu_j)$ sind auch die $\tilde{u}_j^{(m)}$ orthogonal und somit eine Basis von $\mathcal{K}_m$. Sei nun $w\in \mathcal{K}_m$ gegeben durch $w = \sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k \tilde{u}_k^{(m)}$. Dann gilt für alle $j < i$
	\begin{equation*}
		0 \stackrel{!}{=} (w,\tilde{u}_j^{(m)}) = (\sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k \tilde{u}_k^{(m)},\tilde{u}_j^{(m)}) = \sum_{k=1}^{m} p(\lambda_k^{(m)}) \beta_k (\tilde{u}_k^{(m)},\tilde{u}_j^{(m)}) = p(\lambda_j^{(m)}) \beta_j.
	\end{equation*}

	Es gibt also zwei Möglichkeiten, diese Gleichheit zu erfüllen. Entweder ist $p(\lambda_j^{(m)}) = 0$ oder $\beta_j = 0$. Wir betrachten zuerst den Fall, dass  $\beta_j \neq 0$ für alle $j = 1,\dots, i-1$ gilt.

	Da dann die $\lambda_j^{(m)}, \, j = 1,\dots,i-1$ Nullstellen von $p$ sind, können wir

	\begin{equation*}
		p(\lambda) = \prod_{j = 1}^{i-1} \frac{\lambda_j^{(m)} -\lambda}{\lambda_j^{(m)} - \lambda_i} q(x)
	\end{equation*}
	schreiben, wobei $q \in \Pi_{m-i}$.\\

	Nach analogen Umformungen wie in \ref{Ungl i=1} gilt

	\begin{equation*}
		\begin{aligned}
			\lambda_i - \lambda_i^{(m)} &= \min_{\substack{p\in \Pi_{m-1}\setminus \{0\} \\ p(\lambda_j^{(m)}) = 0, j = 1,\dots,i-1}} \frac{\sum_{j=1}^{n} (\lambda_i - \lambda_j)|p(\lambda_j)\alpha_j|^2}{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			&= \min_{\substack{p\in \Pi_{m-1}\setminus \{0\} \\ p(\lambda_j^{(m)}) = 0, j = 1,\dots,i-1}} \frac{\sum_{\substack{j=1, j\neq i}}^{n} (\lambda_i - \lambda_j)|p(\lambda_j)\alpha_j|^2}{\sum_{j=1}^{n} |p(\lambda_j)\alpha_j|^2} \\
			& \le (\lambda_i - \lambda_n) \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2} \min_{\substack{p\in \Pi_{m-1}\setminus \{0\} \\ p(\lambda_j^{(m)}) = 0, j = 1,\dots,i-1}} \max_{j=i+1,\dots,n} \left(\frac{|p(\lambda_j)|}{|p(\lambda_i)|}\right)^2 \\
			&= (\lambda_i - \lambda_n) \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2} \min_{q\in \Pi_{m-i}\setminus \{0\}} \max_{j=i+1,\dots,n} \left(\left|\prod_{k = 1}^{i-1} \frac{\lambda_k^{(m)} -\lambda_j}{\lambda_k^{(m)} - \lambda_i}\right| \left|\frac{q(\lambda_j)}{q(\lambda_i)}\right|\right)^2 \\
			&\le (\lambda_i - \lambda_n) \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2} \left|\prod_{k = 1}^{i-1} \frac{\lambda_k^{(m)} -\lambda_n}{\lambda_k^{(m)} - \lambda_i}\right|^2 \min_{q\in \Pi_{m-i}\setminus \{0\}} \max_{j=i+1,\dots,n} \left(\left|\frac{q(\lambda_j)}{q(\lambda_i)}\right|\right)^2 \\
		\end{aligned}
	\end{equation*}

	Wiederum ist
	\begin{equation*}
		(\tan\theta_i)^2 = \frac{\norm{(\id - \mathcal{P}_{u_i})v_0}^2}{\norm{ \mathcal{P}_{u_i}v_0}^2} = \frac{\norm{v_0 - \alpha_i u_i}^2}{\norm{\alpha_i u_i}^2} = \frac{\sum_{\substack{j=1, j\neq i}}^{n} |\alpha_j|^2}{|\alpha_i|^2}
	\end{equation*}

	und  mit $a = \lambda_n, b= \lambda_{i+1}$, $c = \lambda_i$ folg Lemma (...) an, erhalten wir die Behauptung.\\

	Wenn nun $\beta_j$ existieren, die gleich 0 sind, kann man die Bediungung $p(\lambda_j^{(m)}) = 0$ weglassen und das damit berechnete Minimum wird dadurch nicht größer, also kann die gleiche Abschätzung verwendet werden.
	Also gilt \ref{konvergenz Eigenwerte} für alle $i = 1, \dots, m-1$.
	\end{proof}
\end{theorem}

Die mit $m$ exponentielle Konvergenz der Eigenwerte ist nun ersichtlich, da mit (...)
\begin{equation*}
	\left(\frac{1}{T_{m-i}(\gamma_i)}\right)^2 \le \frac{1}{4} \left(\gamma_i + \sqrt{\gamma_i^2 -1}\right)^{-2(m-i)}
\end{equation*}
gilt. Die Konvergenzgeschwindigkeit nimmt in Richtung der kleineren Eigenwerte ab. Wir bemerken noch, dass bei der Approximation von $\lambda_i$ bei Eigenwerten $\lambda_i \approx \lambda_j, \, j < i$, das Produkt $\kappa^{(m)}$ groß werden kann, da der Nenner wegen $\lambda^{(m)}_j - \lambda_i$ klein wird. Dies hat eine schlechtere Approximation zur folge.


\subsection{Implementierung}

\subsection{Ergebnisse}

\newpage

%Literaturverzeichnis

\end{document}

\section{Der Satz von Keldysh}

\begin{theorem}[Keldysh, nicht-linear] \label{keldysh_nicht_linear}

    Sei $\Lambda \subset \C$ ein beschränktes Gebiet und $A \in H(\Lambda, \C^{N \times N})$ holomorph.
    Es existiere ein $\lambda \in \Lambda$, sodass $A(\lambda) \in \GL_N(\C)$, d.h. invertierbar ist.

    Weiter sei $\lambda_1 \in \Lambda$ ein halb-einfacher Eigenwert, d.h. es existiere eine $L_1$-dimensionale Orthonormalbasis aus (Rechts-)Eigenvektoren $v_{1, 1}, \dots, v_{1, L_1} \in \ker A(\lambda_1)$.
    Für diese gelte $\Forall l = 1, \dots, L_1:$
    
    \begin{align*}
        A^\prime(\lambda_1) v_{1, l}
        \not \in
        \ran A(\lambda_1).
    \end{align*}

    Dann existiert eine Basis $w_{1, 1}, \dots, w_{1, L_1}$ von $\ker(A^\ast(\lambda_1))$, sodass $\Forall l, k = 1, \dots, L_1:$

    \begin{align}
        w_{1, l}^\ast A^\prime(\lambda_1) v_{1, k} = \delta_{l, k}.
    \end{align}

    Weiterhin existiert eine Umgebung $U_1$ von $\lambda_1$ und $R_1 \in H(U_1)$ holomorph, sodass $\Forall \lambda \in U_1 \setminus \Bbraces{\lambda_1}:$

    \begin{align}
        A(\lambda)^{-1}
        =
        \frac{1}{\lambda - \lambda_1} P_1^+
        +
        R_1(\lambda),
        \quad
        P_1^+
        :=
        \sum_{l=1}^{L_1}
            v_{1, l} w_{1, l}^\ast.
    \end{align}

\end{theorem}

\begin{remark}
    
    Sei $(\lambda, v)$ ein Eigenpaar der Matrix $A \in \C^{N \times N}$.
    Wir nennen $v$ einen \textit{Rechts-Eigenvektor} von $A$ zum Eigenwert $\lambda$.
    Dieser besitzt den bekannten \textit{Rechts-Eigenraum} $\Ker(A - I_N \lambda)$.

    $A^\ast$ ist tatsächlich die Adjungierte von $A$ im Sinne der Funktionalanalysis, weil $\Forall x, y \in \C^N:$

    \begin{align*}
        (A x, y)_2
        =
        y^\ast A x
        =
        (A^\ast y)^\ast x
        =
        (x, A^\ast y)_2.
    \end{align*}

    Nun ist $\overline \lambda$ Eigenwert von $A^\ast$ mit derselben algebraischen Vielfachheit wie $\lambda$, weil

    \begin{align*}
        \chi_{A^\ast}(\lambda)
        =
        \det(A^\ast - I_N \lambda)
        =
        \overline{\det(A - I_N \lambda)^\top}
        =
        \overline{\chi_A(\lambda)}.
    \end{align*}

    $v$ ist dann auch ein sogenannter \textit{Links-Eigenvektor} von $A^\ast$ zum Eigenwert $\overline \lambda$, weil

    \begin{align*}
        v^\ast A^\ast
        =
        (A v)^\ast
        =
        (\lambda v)^\ast
        =
        \overline \lambda v^\ast.
    \end{align*}

    Sämtliche Links-Eigenvektoren bilden (gemeinsam mit der $0$) den \textit{Links-Eigenraum} von $\lambda$ bzgl. $A$.
    Dieser ist in der Tat ein Unterraum.

\end{remark}

\begin{theorem}[Keldysh, linear] \label{keldysh_linear}
    
    Sei $\lambda_1$ ein halb-einfacher Eigenwert einer Matrix $A \in \C^{N \times N}$, d.h. geometrische Vielfachheit $L_1^\mathrm{geo}$ und algebraische Vielfachheit $L_1^\mathrm{alg}$ stimmen überein.

    \begin{align*}
        L_1^\mathrm{geo} = \Def(A - I_N \lambda),
        \quad
        L_1^\mathrm{alg} = \mu_1 = \max \Bbraces{\mu \in \N: (\lambda - \lambda_1)^\mu \mid \chi_A(\lambda)},
        \quad
        L_1 := L_1^\mathrm{geo} + L_1^\mathrm{alg}
    \end{align*}

    Dann gelten folgende $3$ Analoga zu Satz \ref{keldysh_nicht_linear}.

    \begin{enumerate}[label = (\roman*)]

        \item Es gibt eine Orthonormalbasis $V_1 = (v_{1, 1}, \dots, v_{1, L_1})$ von $\Ker(A - I_N \lambda_1)$.

        \item Es gibt eine Basis $W_1 = (w_{1, 1}, \dots, w_{1, L_1})$ von $\Ker(A^\ast - I_N \overline \lambda_1)$, sodass
        
        \begin{align*}
            \Forall l, k = 1, \dots, L_1:
            (v_{1, k}, w_{1, l})_2 = -\delta_{l, k}.
        \end{align*}

        \item Es existiert eine Umgebung $U_1$ von $\lambda_1$ und $R_1 \in H(U_1)$ holomorph, sodass $\Forall \lambda \in U_1 \setminus \Bbraces{\lambda_1}:$

        \begin{align*}
            (A - I_N \lambda)^{-1}
            =
            \frac{1}{\lambda - \lambda_1} P_1^+
            +
            R_1(\lambda),
            \quad
            P_1^+
            :=
            \sum_{l=1}^{L_1}
                v_{1, l} w_{1, l}^\ast.
        \end{align*}

    \end{enumerate}

\end{theorem}

\begin{remark}
    
    $P_1^- := -P_1^+$ nennt man die \textit{Spektrale Projektion} auf den Eigenraum von $\lambda_1$.
    Offensichtlich bildet $P_1^-$ ganz $\C^N$ auf den Rechts-Eigenraum von $\lambda_1$ ab, sie ist aber auch idempotent, weil $\Forall x \in \Ker(A - I_N \lambda_1):$

    \begin{align*}
        P_1^- x
        =
        -\sum_{l=1}^{L_1}
            v_{1, l}
            w_{1, l}^\ast
            \sum_{k=1}^{L_1}
                (x, v_{1, k})_2
                v_{1, k}
        =
        -\sum_{l=1}^{L_1}
            v_{1, l}
            \sum_{k=1}^{L_1}
                \underbrace{(v_{1, k}, w_{1, l})_2}_{-\delta_{l, k}}
                (x, v_{1, k})_2
        =
        \sum_{l=1}^{L_1}
            (x, v_{1, l})_2
            v_{1, l}
        =
        x.
    \end{align*}

    Also ist $P_1^-$ tatsächlich eine Projektion.

\end{remark}

\begin{proof}[Beweis (als Korollar)]

    Wir überprüfen also die Voraussetzungen von Satz \ref{keldysh_nicht_linear}.

    \begin{enumerate}[label = \arabic*.]

        \item Unsere (lineare) Matrix-Funktion $\lambda \mapsto A - I_N \lambda$ ist offensichtlich holomorph.
        
        \item Wenn $\lambda$ kein Eigenwert von $A$ ist, dann ist $\Ker(A - I_N \lambda) = \Bbraces{0}$, also $A - I_N \lambda \in \GL_N(\C)$ d.h. invertierbar.
        
        \item Seien $\lambda_2, \dots, \lambda_k$ die restlichen (paarweise verschiedenen) Eigenwert von $A$.
        Seien $L_n^\mathrm{geo}$ und $L_n^\mathrm{alg}$ die geometrische bzw. algebraische Vielfachheit von $\lambda_n$ für $n = 2, \dots, k$.
        Betrachte die JNF von $A$.
    
        \begin{align*}
            T J T^{-1} & = A, \\
            J & = \diag (J_1, \dots, J_k),
            \quad
            T \in \GL_N(\C) \\
            J_n
            & =
            \diag
            \underbrace
            {
                \pbraces
                {
                    \begin{pmatrix}
                        \lambda_n & 1      &        &           \\
                                  & \ddots & \ddots &           \\
                                  &        & \ddots & 1         \\
                                  &        &        & \lambda_n \\
                    \end{pmatrix},
                    \dots,
                    \begin{pmatrix}
                        \lambda_n & 1      &        &           \\
                                  & \ddots & \ddots &           \\
                                  &        & \ddots & 1         \\
                                  &        &        & \lambda_n \\
                    \end{pmatrix}
                }
            }_{
                \displaystyle
                L_n^\mathrm{geo} \text{-viele}
            }
            \in
            \C^{
                L_n^\mathrm{geo}
                \times
                L_n^\mathrm{alg}
            },
            \quad
            n = 1, \dots, k
        \end{align*}
    
        Weil $\lambda_1$ halb-einfach ist, muss $J_1 = I_{L_1} \lambda_1$.
        Seien $\hat v_1, \dots, \hat v_N$ die linear unabhängig Spalten der \\ Transformations-Matrix $T$.
    
        \begin{align*}
            \implies
            (A \hat v_1, \dots, A \hat v_{L_1}, \ast)
            =
            A T
            \stackrel
            {
                \text{JNF}
            }{=}
            T J
            =
            (\hat v_1, \dots, \hat v_{L_1}, \ast)
            \underbrace
            {
                \begin{pmatrix}
                    J_1 & 0 \\
                    0   & \ast
                \end{pmatrix}
            }_J
            =
            (\lambda_1 \hat v_1, \dots, \lambda_1 \hat v_{L_1}, \ast)
        \end{align*}
    
        Wir können die linear unabhängig $\hat v_1, \dots, v_{L_1}$ also orthonormalisieren (Gram-Schmidt) und erhalten die Orthonormalbasis $V_1 := (v_{1, 1}, \dots, v_{1, L_1})$.

        \item Die Größe der Jordan-Kästchen entspricht der Länge der Jordan-Ketten (Hauptvektor-Ketten).
        Alle Jordan-Ketten sind also $1$-gliedrig, bestehen also nur aus \Quote{echten} Eigenwerten.
        Es gibt also keine Hauptvektoren $2$-ter Stufe.
    
        Sei $y \in \Ker(A - I_N \lambda_1) \cap \ran(A - I_N \lambda_1)$, dann gibt es ein $x \in \C^N$ mit $y = (A - I_N \lambda_1) x$ und $(A - I_N \lambda_1) y = 0$.
        Angenommen, $y \neq 0$, dann wäre $x$ ein Hauptvektor $2$-ter Stufe, weil

        \begin{align*}
            & \implies
            (A - I_N \lambda_1) x = y \neq 0,
            \quad
            (A - I_N \lambda_1)^2 x = (A - I_N \lambda_1) y = 0 \\
            & \implies
            x \in \Ker(A - I_N \lambda_1)^2 \setminus \Ker(A - I_N \lambda_1) = \emptyset.
        \end{align*}

        Widerspruch!
    
        \begin{align*}
            \implies
            \Ker(A - I_N \lambda_1) \cap \ran(A - I_N \lambda_1) = \Bbraces{0}
        \end{align*}
    
        Die Ableitung unserer Matrixfunktion berechnet man komponentenweise.
        Weil $0 \neq v_{1, 1}, \dots, v_{1, L_1} \in \Ker(A - I_N \lambda)$, folgt damit die letzte Voraussetzung.
        $\Forall l = 1, \dots, L_1:$

        \begin{align*}
            \derivative{\lambda} (A - I_N \lambda) \Big |_{\lambda = \lambda_1} v_{1, l}
            =
            -I_N v_{1, l}
            =
            -v_{1, l}
            \not \in
            \ran(A - I_N \lambda_1)
        \end{align*}

    \end{enumerate}

    Wir können also Satz \ref{keldysh_nicht_linear} anwenden.
    
    \begin{enumerate}[label = (\roman*)]

        \item Die Orthonormalbasis $V_1 = (v_{1, 1}, \dots, v_{1, L_1})$ von $\Ker(A - I_N \lambda_1)$ haben wir bereits konstruiert.
        
        \item Der Satz \ref{keldysh_nicht_linear} gibt uns eine Basis $w_{1, 1}, \dots, w_{1, L_1}$ von $\Ker(A - I_N \lambda_1)^\ast = \Ker(A^\ast - I_N \overline \lambda_1)$, sodass $\Forall l, k = 1, \dots, L_1:$
        
        \begin{align*}
            (v_{1, k}, w_{1, l})_2
            =
            w_{1, l}^\ast v_{1, k}
            =
            -w_{1, l}^\ast (-I_N) v_{1, k}
            =
            -w_{1, l}^\ast \derivative{\lambda} (A - I_N \lambda) \Big |_{\lambda = \lambda_1} v_{1, k}
            =
            -\delta_{l, k}.
        \end{align*}

        \item Diese Tatsache kann $1 : 1$ aus Satz \ref{keldysh_nicht_linear} übernommen werden.

    \end{enumerate}
    
\end{proof}

\begin{proof}[Beweis (zu Fuß)]

    \phantom{}

    \begin{enumerate}[label = (\roman*)]

        \item Siehe vorheriger Beweis.

        \item Betrachte abermals die JNF.
        Es gibt $\hat W_1 = (\hat w_{1, 1}, \dots, \hat w_{1, L_1})$ linear unabhängig, sodass
        
        \begin{align*}
            \begin{pmatrix}
                \lambda_1 \hat w_{1,   1}^\ast \\
                \vdots                         \\
                \lambda_1 \hat w_{1, L_1}^\ast \\
                \ast
            \end{pmatrix}
            =
            \underbrace
            {
                \begin{pmatrix}
                    J_1 & 0 \\
                    0   & \ast
                \end{pmatrix}
            }_J
            \begin{pmatrix}
                \hat w_{1,   1}^\ast \\
                \vdots               \\
                \hat w_{1, L_1}^\ast \\
                \ast
            \end{pmatrix}
            =
            J T^{-1}
            \stackrel
            {
                \text{JNF}
            }{=}
            T^{-1} A
            =
            \begin{pmatrix}
                \hat w_{1,   1}^\ast \\
                \vdots               \\
                \hat w_{1, L_1}^\ast \\
                \ast
            \end{pmatrix}
            A
            =
            \begin{pmatrix}
                \hat w_{1,   1}^\ast A \\
                \vdots                 \\
                \hat w_{1, L_1}^\ast A \\
                \ast
            \end{pmatrix}.
        \end{align*}

        $\hat W_1$ sind also Links-Eigenvektoren zum Eigenwert $\lambda_1$, d.h. Rechts-Eigenvektoren von $A^\ast$ zum Eigenwert $\overline \lambda_1$.
        Weil die algebraische Vielfachheit von $\overline \lambda_1$ bzgl. $A^\ast$ ja $L_1$ ist, bilden diese eine Basis von $\Ker(A^\ast - I_N \overline \lambda_1)$.
        Wir setzen an mit

        \begin{align*}
            w_{1, l}
            \stackrel{!}{=}
            \sum_{i=1}^{L_1}
                \alpha_{1, l, i} \hat w_{1, i},
            \quad
            l = 1, \dots, L_1.
        \end{align*}

        Wir formulieren unseren Wunsch an $W_1$ etwas um.

        \begin{align*}
            \iff
            \Forall l, k = 1, \dots, L_1:
                -\delta_{l, k}
                \stackrel{!}{=}
                (v_{1, k}, w_{1, l})_2
                =
                \pbraces
                {
                    v_{1, k},
                    \sum_{i=1}^{L_1}
                        \alpha_{1, l, i} \hat w_{1, i}
                }_2
                =
                \sum_{i=1}^{L_1}
                    \overline \alpha_{1, l, i} (v_{1, k}, \hat w_{1, i})_2
        \end{align*}

        \begin{align*}
            \iff
            \Forall l = 1, \dots, L_1:
                -e_l
                \stackrel{!}{=}
                \overline{M \alpha_{1, l}}
            \iff
                e_l
                =
                \overline e_l
                \stackrel{!}{=}
                -M \alpha_{1, l}
        \end{align*}

        \begin{align*}
            e_l
            :=
            \begin{pmatrix}
                \delta_{l 1} \\ \vdots \\ \delta_{l L_1}
            \end{pmatrix},
            \quad
            \alpha_{1, l}
            :=
            \begin{pmatrix}
                \alpha_{1, l, 1} \\ \vdots \\ \alpha_{1, l, L_1}
            \end{pmatrix}
        \end{align*}

        \begin{multline*}
            M
            :=
            \overline
            {
                \begin{pmatrix}
                    (v_{1,   1}, \hat w_{1, 1})_2 & \cdots & (v_{1,   1}, \hat w_{1, L_1})_2 \\
                    \vdots                        & \ddots & \vdots                          \\
                    (v_{1, L_1}, \hat w_{1, 1})_2 & \cdots & (v_{1, L_1}, \hat w_{1, L_1})_2 \\
                \end{pmatrix}
            }
            =
            \begin{pmatrix}
                (\hat w_{1, 1}, v_{1,   1})_2 & \cdots & (\hat w_{1, L_1}, v_{1,   1})_2 \\
                \vdots                        & \ddots & \vdots                          \\
                (\hat w_{1, 1}, v_{1, L_1})_2 & \cdots & (\hat w_{1, L_1}, v_{1, L_1})_2 \\
            \end{pmatrix} \\
            =
            \begin{pmatrix}
                v_{1,   1}^\ast \hat w_{1, 1} & \cdots & v_{1,   1}^\ast \hat w_{1, L_1} \\
                \vdots                        & \ddots & \vdots                          \\
                v_{1, L_1}^\ast \hat w_{1, 1} & \cdots & v_{1, L_1}^\ast \hat w_{1, L_1} \\
            \end{pmatrix}
            =
            \begin{pmatrix}
                v_{1, 1}^\ast \\ \vdots \\ v_{1, L_1}^\ast
            \end{pmatrix}
            \hat W_1
            =
            V_1^\ast \hat W_1
            \in
            \GL_{L_1}(\C),
        \end{multline*}

        Man beachte dabei, dass $M \in \GL_{L_1}(\C)$, weil $\hat W_1 \in \GL_{L_1}(\C)$ und

        \begin{align*}
            V_1 \in \GL_{L_1}(\C)
            \implies
            \det V_1 \neq 0
            \implies
            0 \neq \overline{\det V_1^\top} = \det V_1^\ast
            \implies
            V_1^\ast \in \GL_{L_1}(\C).
        \end{align*}

        $W_1 = (w_{1, 1}, \dots, w_{1, L_1})$ ist linear unabhängig, also eine Basis, weil

        \begin{align*}
            & \implies
            \alpha_1
            :=
            (\alpha_{1, 1}, \dots, \alpha_{1, L_1})
            =
            -M^{-1} (e_1, \dots, e_{L_1})
            =
            -M^{-1} I_{L_1}
            =
            -M^{-1}
            \in
            \GL_{L_1}(\C) \\
            & \implies
            W_1
            =
            \hat W_1 \alpha_1
            \in
            \GL_{L_1}(\C).
        \end{align*}

        \item Weil ja $\dim \C^\N < \infty$, genügt es, nachzurechnen, dass die rechte Seite eine Linksinverse ist.
        
        \begin{align*}
            (A - I_N \lambda)^{-1}
            & \stackrel{!}{=}
            \frac{1}{\lambda - \lambda_1} P_1^+
            +
            R_1(\lambda) \\
            \iff
            I_N
            & =
            (A - I_N \lambda)
            \pbraces
            {
                \frac{1}{\lambda - \lambda_1} P_1^+
                +
                R_1(\lambda)
            } \\
            & =
            (A - I_N \lambda)
            \pbraces
            {
                \frac{1}{\lambda - \lambda_1}
                \sum_{l=1}^{L_1}
                    v_{1, l} w_{1, l}^\ast
                +
                R_1(\lambda)
            } \\
            & =
            -\frac{1}{\lambda_1 - \lambda}
            \sum_{l=1}^{L_1}
                (
                    A v_{1, l}
                    -
                    \lambda v_{1, l}
                )
                w_{1, l}^\ast
            +
            (A - I_N \lambda) R_1(\lambda) \\
            & =
            -\frac{1}{\lambda_1 - \lambda}
            \sum_{l=1}^{L_1}
                (\lambda_1 - \lambda) v_{1, l} w_{1, l}^\ast
            +
            (A - I_N \lambda)
            R_1(\lambda) \\
            & =
            -\sum_{l=1}^{L_1}
                v_{1, l} w_{1, l}^\ast
            +
            (A - I_N \lambda)
            R_1(\lambda) \\
            & =
            P_1^-
            +
            (A - I_N \lambda)
            R_1(\lambda) \\
            \stackrel{!}{\iff}
            I_N - P_1^-
            & =
            (A - I_N \lambda) R(\lambda)
        \end{align*}

        Das motiviert folgende Definition des Residuums.

        \begin{align*}
            R(\lambda)
            :=
            (A - I_N \lambda) |_{E_\mathrm{C}}^{-1} (I_N - P_1^-)
        \end{align*}

        Sei dabei $E := \Ker(A - I_N \lambda)$, d.h. der Eigenraum von $\lambda_1$ und $E_\mathrm{C}$ dessen Komplementärraum, d.h. $E \oplus E_\mathrm{C} = \C^N$.
        $P_1^-$ ist eine Projektion auf $E$ und $I_N - P_1^-$ eine auf $E_\mathrm{C}$.

        Nur Eigenvektoren $x$ von $\lambda_1$ bzgl. $A$, und $0$ erfüllen

        \begin{align*}
            A x = \lambda_1 x
            \iff
            (A - I_N \lambda_1) x = 0.
        \end{align*}

        Die lineare Abbildung $(A - I_N \lambda_1) |_{E_\mathrm{C}}$ hat jedoch keine Eigenvektoren.

        \begin{align*}
            \implies
            \Ker(A - I_N \lambda_1) |_{E_\mathrm{C}} = \Bbraces{0}
        \end{align*}



    \end{enumerate}

\end{proof}

Das folgende Korollar werden wir als Ausgangspunkt für die Konstruktion des zentralen Algorithmus' verwenden.

\begin{corollary} \label{keldysh_multi}

    Sei $\Lambda \subset \C$ ein beschränktes Gebiet und $A \in H(\Lambda, \C^{N \times N})$ holomorph.
    Es existiere ein $\lambda \in \Lambda$, sodass $A(\lambda) \in \GL_N(\C)$, d.h. invertierbar ist.
    Mögen $\lambda_1, \dots, \lambda_k$ die Voraussetzungen von Satz \ref{keldysh_nicht_linear} erfüllen.

    Dann existiert ein $R \in H(\Lambda, \C^{N \times N})$, sodass $\Forall \lambda \in \Lambda \setminus \Bbraces{\lambda_1, \dots, \lambda_k}:$

    \begin{align*}
        A^{-1}(\lambda)
        =
        \sum_{n=1}^k
            \frac{1}{\lambda - \lambda_n} P_n^+
        +
        R(\lambda).
    \end{align*}

\end{corollary}

\begin{proof}

    Wir definieren das Residuum zunächst auf $\Lambda \setminus \Bbraces{\lambda_1, \dots, \lambda_k}$ durch umstellen.

    Sei $m = 1, \dots, n$.
    Laut \ref{keldysh_nicht_linear}, existiert eine Umgebung $U_m$ von $\lambda_m$ und $R_m \in H(U_m)$ holomorph, sodass $\Forall \lambda \in U_m \setminus \Bbraces{\lambda_m}:$

    \begin{align*}
        A(\lambda)^{-1}
        =
        \frac{1}{\lambda - \lambda_m} P_m^+
        +
        R_m(\lambda).
    \end{align*}

    $R$ können wir nun auf $\lambda_m$ holomorph fortsetzen.

    \begin{align*}
        R(\lambda)
        :=
        A^{-1}(\lambda)
        -
        \sum_{n=1}^k
            \frac{1}{\lambda - \lambda_n} P_n^+
        =
        \frac{1}{\lambda - \lambda_m} P_m^+
        +
        R_m(\lambda)
        -
        \sum_{n=1}^k
            \frac{1}{\lambda - \lambda_n} P_n^+
        =
        R_m(\lambda)
        -
        \sum_{\substack{n = 1 \\ n \neq m}}^k
            \frac{1}{\lambda - \lambda_n} P_n^+
    \end{align*}

    Wenn wir das für alle $m = 1, \dots, n$ machen, haben wir $R$ auf ganz $\Lambda$ holomorph erweitert.

\end{proof}
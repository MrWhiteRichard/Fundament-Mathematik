\begin{proof}[Beweis (zu Fuß)]

    \phantom{}

    \begin{enumerate}[label = (\roman*)]

        \item Siehe vorheriger Beweis.

        \item Betrachte abermals die JNF.
        Es gibt $\hat W_1 = (\hat w_{1, 1}, \dots, \hat w_{1, L_1})$ linear unabhängig, sodass
        
        \begin{align*}
            \begin{pmatrix}
                \lambda_1 \hat w_{1,   1}^\ast \\
                \vdots                         \\
                \lambda_1 \hat w_{1, L_1}^\ast \\
                \ast
            \end{pmatrix}
            =
            \underbrace
            {
                \begin{pmatrix}
                    J_1 & 0 \\
                    0   & \ast
                \end{pmatrix}
            }_J
            \begin{pmatrix}
                \hat w_{1,   1}^\ast \\
                \vdots               \\
                \hat w_{1, L_1}^\ast \\
                \ast
            \end{pmatrix}
            =
            J T^{-1}
            \stackrel
            {
                \text{JNF}
            }{=}
            T^{-1} A
            =
            \begin{pmatrix}
                \hat w_{1,   1}^\ast \\
                \vdots               \\
                \hat w_{1, L_1}^\ast \\
                \ast
            \end{pmatrix}
            A
            =
            \begin{pmatrix}
                \hat w_{1,   1}^\ast A \\
                \vdots                 \\
                \hat w_{1, L_1}^\ast A \\
                \ast
            \end{pmatrix}.
        \end{align*}

        $\hat W_1$ sind also Links-Eigenvektoren von $A$ zum Eigenwert $\lambda_1$, d.h. Rechts-Eigenvektoren von $A^\ast$ zum Eigenwert $\overline \lambda_1$.
        Weil die algebraische Vielfachheit von $\overline \lambda_1$ bzgl. $A^\ast$ ja $L_1$ ist, bilden diese eine Basis von $\ker (A^\ast - I_N \overline \lambda_1)$.
        Wir setzen an mit

        \begin{align*}
            w_{1, l}
            \stackrel{!}{=}
            \sum_{i=1}^{L_1}
                \alpha_{1, l, i} \hat w_{1, i},
            \quad
            l = 1, \dots, L_1.
        \end{align*}

        Wir formulieren unseren Wunsch an $W_1$ etwas um.

        \begin{align*}
            \iff
            \Forall l, k = 1, \dots, L_1:
                -\delta_{l, k}
                \stackrel{!}{=}
                (v_{1, k}, w_{1, l})_2
                =
                \pbraces
                {
                    v_{1, k},
                    \sum_{i=1}^{L_1}
                        \alpha_{1, l, i} \hat w_{1, i}
                }_2
                =
                \sum_{i=1}^{L_1}
                    \overline \alpha_{1, l, i} (v_{1, k}, \hat w_{1, i})_2
        \end{align*}

        \begin{align*}
            \iff
            \Forall l = 1, \dots, L_1:
                -e_l
                \stackrel{!}{=}
                \overline{M \alpha_{1, l}}
            \iff
                e_l
                =
                \overline e_l
                \stackrel{!}{=}
                -M \alpha_{1, l}
        \end{align*}

        \begin{align*}
            e_l
            :=
            \begin{pmatrix}
                \delta_{l 1} \\ \vdots \\ \delta_{l L_1}
            \end{pmatrix},
            \quad
            \alpha_{1, l}
            :=
            \begin{pmatrix}
                \alpha_{1, l, 1} \\ \vdots \\ \alpha_{1, l, L_1}
            \end{pmatrix}
        \end{align*}

        \begin{multline*}
            M
            :=
            \overline
            {
                \begin{pmatrix}
                    (v_{1,   1}, \hat w_{1, 1})_2 & \cdots & (v_{1,   1}, \hat w_{1, L_1})_2 \\
                    \vdots                        & \ddots & \vdots                          \\
                    (v_{1, L_1}, \hat w_{1, 1})_2 & \cdots & (v_{1, L_1}, \hat w_{1, L_1})_2 \\
                \end{pmatrix}
            }
            =
            \begin{pmatrix}
                (\hat w_{1, 1}, v_{1,   1})_2 & \cdots & (\hat w_{1, L_1}, v_{1,   1})_2 \\
                \vdots                        & \ddots & \vdots                          \\
                (\hat w_{1, 1}, v_{1, L_1})_2 & \cdots & (\hat w_{1, L_1}, v_{1, L_1})_2 \\
            \end{pmatrix} \\
            =
            \begin{pmatrix}
                v_{1,   1}^\ast \hat w_{1, 1} & \cdots & v_{1,   1}^\ast \hat w_{1, L_1} \\
                \vdots                        & \ddots & \vdots                          \\
                v_{1, L_1}^\ast \hat w_{1, 1} & \cdots & v_{1, L_1}^\ast \hat w_{1, L_1} \\
            \end{pmatrix}
            =
            \begin{pmatrix}
                v_{1, 1}^\ast \\ \vdots \\ v_{1, L_1}^\ast
            \end{pmatrix}
            \hat W_1
            =
            V_1^\ast \hat W_1
            \in
            \GL_{L_1}(\C),
        \end{multline*}

        Man beachte dabei, dass $M \in \GL_{L_1}(\C)$, weil $\hat W_1 \in \GL_{L_1}(\C)$ und

        \begin{align*}
            V_1 \in \GL_{L_1}(\C)
            \implies
            \det V_1 \neq 0
            \implies
            0 \neq \overline{\det V_1^\top} = \det V_1^\ast
            \implies
            V_1^\ast \in \GL_{L_1}(\C).
        \end{align*}

        $W_1 = (w_{1, 1}, \dots, w_{1, L_1})$ ist linear unabhängig, also eine Basis, weil

        \begin{align*}
            & \implies
            \alpha_1
            :=
            (\alpha_{1, 1}, \dots, \alpha_{1, L_1})
            =
            -M^{-1} (e_1, \dots, e_{L_1})
            =
            -M^{-1} I_{L_1}
            =
            -M^{-1}
            \in
            \GL_{L_1}(\C) \\
            & \implies
            W_1
            =
            \hat W_1 \alpha_1
            \in
            \GL_{L_1}(\C).
        \end{align*}

        \item Weil ja $\dim \C^\N < \infty$, genügt es, nachzurechnen, dass die rechte Seite eine Rechtsinverse ist.
        
        \begin{align*}
            (A - I_N \lambda)^{-1}
            & \stackrel{!}{=}
            \frac{1}{\lambda - \lambda_1} P_1^+
            +
            R_1(\lambda) \\
            \iff
            I_N
            & =
            (A - I_N \lambda)
            \pbraces
            {
                \frac{1}{\lambda - \lambda_1} P_1^+
                +
                R_1(\lambda)
            } \\
            & =
            (A - I_N \lambda)
            \pbraces
            {
                -\frac{1}{\lambda_1 - \lambda}
                \sum_{l=1}^{L_1}
                    v_{1, l} w_{1, l}^\ast
                +
                R_1(\lambda)
            } \\
            & =
            -\frac{1}{\lambda_1 - \lambda}
            \sum_{l=1}^{L_1}
                (
                    A v_{1, l}
                    -
                    \lambda v_{1, l}
                )
                w_{1, l}^\ast
            +
            (A - I_N \lambda) R_1(\lambda) \\
            & =
            -\frac{1}{\lambda_1 - \lambda}
            \sum_{l=1}^{L_1}
                (\lambda_1 - \lambda) v_{1, l} w_{1, l}^\ast
            +
            (A - I_N \lambda)
            R_1(\lambda) \\
            & =
            -\sum_{l=1}^{L_1}
                v_{1, l} w_{1, l}^\ast
            +
            (A - I_N \lambda)
            R_1(\lambda) \\
            & =
            P_1^-
            +
            (A - I_N \lambda)
            R_1(\lambda) \\
            \stackrel{!}{\iff}
            I_N - P_1^-
            & =
            (A - I_N \lambda) R_1(\lambda)
        \end{align*}

        Das motiviert folgende Definition des Residuums.
        Dabei ist $X$ so wie in \ref{spektrale_projektion}.

        \begin{multline*}
            R_1(\lambda)
            :=
            \begin{cases}
                (A - I_N \lambda)    ^{-1} (I_N - P_1^-), & \lambda \neq \lambda_1, \\
                (A - I_N \lambda) |_X^{-1} (I_N - P_1^-), & \lambda =    \lambda_1, \\
            \end{cases} \\
            \lambda
            \in
            U_1
            :=
            B
            (
                \lambda_1,
                \min
                \Bbraces
                {
                    \abs{\lambda - \lambda^\prime}:
                    \lambda, \lambda^\prime \in \sigma(A),
                    \lambda \neq \lambda^\prime
                }
            )
        \end{multline*}

        Für $\lambda \in U_1 \setminus \Bbraces{\lambda_1}$ ist $R(\lambda)$ wohldefiniert, weil $\lambda$ kein Eigenwert und somit $\ker (A - I_N \lambda) = \Bbraces{0}$, also $A - I_N \lambda$ invertierbar ist.
        $A - I_N \id$ ist holomorph.
        Wegen der Kofaktor-Darstellung der Inversen, als Matrix mit komponentenweise rationalen Polynomen, ist $\lambda \mapsto (A - I_N \lambda)^{-1}$, und auch somit $R_1$, in $\lambda$ holomorph.

        \begin{align*}
            (A - I_N \lambda)^{-1}
            =
            \frac{1}{\chi_A(\lambda)}
            \cof (A - I_N \lambda)
        \end{align*}

        Dies soll nun auch für $\lambda_1$ gelten.
        Laut Bemerkung \ref{semi_inverse}, ist $(A - I_N \lambda_1) |_X^{-1}$, also auch $R_1(\lambda_1)$, wohldefiniert.

        Weil $(A - I_N \lambda_1) |_X^{-1} (I_N - P_1^-) \in \Lin(\C^N, X)$, gilt

        \begin{align*}
            (A - I_N \lambda_1)
            (A - I_N \lambda_1) |_X^{-1}
            (I_N - P_1^-)
            =
            (A - I_N \lambda_1) |_X
            (A - I_N \lambda_1) |_X^{-1}
            (I_N - P_1^-)
            =
            (I_N - P_1^-)
        \end{align*}

        Darauf basierend, machen wir zunächst eine kleine Nebenrechnung, die wir mehrmals einsetzen werden.
        Dabei sollte man eigentlich immer $I_N - P_1^-$ links dazuschreiben.

        \begin{align*}
            (A - I_N \lambda)^{-1}
            -
            (A - I_N \lambda_1) |_X^{-1}
            & =
            (A - I_N \lambda)^{-1}
            \pbraces
            {
                (A - I_N \lambda_1) |_X
                -
                (A - I_N \lambda)
            }
            (A - I_N \lambda_1) |_X^{-1} \\
            & =
            (A - I_N \lambda)^{-1}
            \pbraces
            {
                (A - I_N \lambda_1)
                -
                (A - I_N \lambda)
            }
            (A - I_N \lambda_1) |_X^{-1} \\
            & =
            (A - I_N \lambda)^{-1}
            (\lambda - \lambda_1)
            (A - I_N \lambda_1) |_X^{-1}
        \end{align*}

        Wir versuchen jetzt, $R^\prime(\lambda_1)$ auszurechnen.

        \begin{align*}
            \frac{1}{\lambda - \lambda_1}
            (R(\lambda) - R(\lambda_1))
            & =
            \frac{1}{\lambda - \lambda_1}
            \pbraces
            {
                (A - I_N \lambda)^{-1}
                -
                (A - I_N \lambda_1) |_X^{-1}
            }
            (I_N - P_1^-) \\
            & =
            \frac{1}{\lambda - \lambda_1}
            (A - I_N \lambda)^{-1}
            (\lambda - \lambda_1)
            (A - I_N \lambda_1) |_X^{-1}
            (I_N - P_1^-) \\
            & =
            (A - I_N \lambda)^{-1}
            (A - I_N \lambda_1) |_X^{-1}
            (I_N - P_1^-) \\
            & \xrightarrow[\lambda \to \lambda_1]{!}
            (A - I_N \lambda_1) |_X^{-1}
            (A - I_N \lambda_1) |_X^{-1}
            (I_N - P_1^-)
        \end{align*}

        Letzterer Grenzwert muss noch gerechtfertigt werden.

        \begin{align*}
            &
            \norm[\C^N \to X]
            {
                (A - I_N \lambda)^{-1}
                (A - I_N \lambda_1) |_X^{-1}
                (I_N - P_1^-)
                -
                (A - I_N \lambda_1) |_X^{-1}
                (A - I_N \lambda_1) |_X^{-1}
                (I_N - P_1^-)
            } \\
            & \leq
            \norm[X \to X]
            {
                (A - I_N \lambda)^{-1}
                -
                (A - I_N \lambda_1) |_X^{-1}
            }
            \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}
            \norm[\C^N \to X]{I_N - P_1^-} \\
            & \leq
            \underbrace
            {
                \abs{\lambda - \lambda_1}
            }_{
                \xrightarrow[\lambda \to \lambda_1]{} 0
            }
            \underbrace
            {
                \norm[X \to X]{(A - I_N \lambda)^{-1}}
            }_{
                \stackrel{!}{\leq} C
            }
            \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}
            \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}
            \norm[\C^N \to X]{I_N - P_1^-}
        \end{align*}

        Die Beschränktheit durch $C > 0$, eine Schranke, die von $\lambda$ unabhängig ist, sieht man für $\abs{\lambda - \lambda_1}$ hinreichend klein.

        \begin{multline*}
            \norm[X \to X]{(A - I_N \lambda)^{-1}}
            \leq
            \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}
            +
            \norm[X \to X]
            {
                (A - I_N \lambda)^{-1}
                -
                (A - I_N \lambda_1) |_X^{-1}
            } \\
            \leq
            \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}
            +
            \abs{\lambda - \lambda_1}
            \norm[X \to X]{(A - I_N \lambda)^{-1}}
            \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}           
        \end{multline*}

        \begin{align*}
            \implies
            &
            \norm[X \to X]{(A - I_N \lambda)^{-1}}
            (
                1
                -
                \abs{\lambda - \lambda_1}
                \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}
            ) \\
            & =
            \norm[X \to X]{(A - I_N \lambda)^{-1}}
            -
            \abs{\lambda - \lambda_1}
            \norm[X \to X]{(A - I_N \lambda)^{-1}}
            \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}} \\
            & \leq
            \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}
        \end{align*}

        \begin{align*}
            \implies
            \norm[X \to X]{(A - I_N \lambda)^{-1}}
            \leq
            \frac
            {
                \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}
            }{
                1
                -
                \abs{\lambda - \lambda_1}
                \norm[X \to X]{(A - I_N \lambda_1) |_X^{-1}}
            }
            \leq
            C
        \end{align*}

    \end{enumerate}

\end{proof}

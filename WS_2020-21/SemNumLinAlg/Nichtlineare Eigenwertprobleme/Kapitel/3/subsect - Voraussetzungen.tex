\section{Voraussetzungen}

Man sei sich dabei bewusst, dass die einzigen Objekte, mit denen der Algorithmus \ref{alg:integral_methode_zusammenfassung} letztenendes operiert, $\hat V$, $A_0$, $A_1$, sowie $\tilde V$, $\Sigma$, $\tilde W$, voll bzw. reduziert, sind.
Bei der Herleitung des Algorithmus' haben wir jedoch auch auf $V$, $W$, $D$, und $S$ zurückgegriffen.
Ohne Kenntnis der gesuchten Eigenpaare sind diese aber sowieso nicht berechenbar.

Die Tatsache, dass sie nicht im Algorithmus \ref{alg:integral_methode_zusammenfassung} vorkommen, heißt aber nicht, dass ihre Eigenschaften (z.B. $S \in \GL_J(\C)$) unwichtig sind.
Wählt man z.B. eine zu kleine Matrix $\hat V$, so kann $S \not \in \GL_J(\C)$.
Die numerischen Resultate können dann in der Tat inkorrekt sein.

Wir fassen zusammen, was der Algorithmus \ref{alg:integral_methode_zusammenfassung} braucht, um korrekte Ergebnisse zu liefern.

\begin{enumerate}[label = \arabic*.]

    \item $\lambda_1, \dots, \lambda_k \in \Lambda$ seien allersamt halb-einfach und liegen im Inneren von $\Gamma$, d.h. insbesondere nicht darauf.
    Sonst könnten theoretisch Probleme bei der Approximationen von $A_0$ und $A_1$, vermöge einer der Quadraturformeln $Q_m$, $m \in \N$, entstehen.
    Sollte dies passieren, müsste man die Kurve $\Gamma$ anpassen.

    \item $V, W \in \C^{N \times J}$ haben vollen Rang $J$, d.h. deren Spalten seien linear unabhängig.
    Diese Annahme ist sinnvoll.

    $V, W$ bestehen ja schließlich aus Rechts- bzw. Links-Eigenvektoren.
    Die Voraussetzung gilt also zumindest im linearen Fall.

    \item $\hat V$ sei eine hinreichend große, gleichverteilt gewählte Zufallsmatrix, mit vollem Rang $j$.
    Diese Annahme ist sinnvoll.

    Zur Illustration, sei dazu $K$, anstelle von $\C$, ein endlicher Körper und $\hat V \in K^{j \times j}$.
    Durch Abzählen der möglichen Komponenten der Spalten der regulären Matrizen, kommt man auf folgende Wahrscheinlichkeit

    \begin{multline*}
        \mathbf{P}(\hat V \in \GL_j(K))
        =
        \frac
        {
            |\GL_j(K)|
        }{
            |K^{j \times j}|
        }
        =
        \frac{1}
        {
            |K|^{j \cdot j}
        }
        \prod_{i=1}^j
            \pbraces
            {
                |K|^j - |K|^{i-1}
            } \\
        =
        \prod_{i=1}^j
            \frac
            {
                |K|^j - |K|^{i-1}
            }{
                |K|^j
            }
        =
        \prod_{i=1}^j
            \pbraces
            {
                1 - \frac{1}{|K|^{j + 1 - i}}
            }
        \xrightarrow{|K| \to |\C|}
        1.
    \end{multline*}

    \item $W^\ast \hat V \in C^{J \times j}$ habe vollen Rang $J \leq j$.
    Diese Annahme ist sinnvoll.
    Es gilt nämlich

    \begin{multline*}
        W^\ast \hat V
        =
        (w_{1, 1}, \dots, w_{k, L_k})^\ast (\hat v_1, \dots, \hat v_j) \\
        =
        \begin{pmatrix}
            w_{1, 1}^\ast   \hat v_1 & \cdots & w_{1, 1}^\ast   \hat v_j \\
            \vdots                   & \ddots & \vdots                   \\
            w_{k, L_k}^\ast \hat v_1 & \cdots & w_{k, L_k}^\ast \hat v_j \\
        \end{pmatrix}
        =
        \begin{pmatrix}
            (\hat v_1, w_{1, 1})_2   & \cdots & (\hat v_j, w_{1, 1})_2   \\
            \vdots                   & \ddots & \vdots                   \\
            (\hat v_1, w_{k, L_k})_2 & \cdots & (\hat v_j, w_{k, L_k})_2 \\
        \end{pmatrix}.
    \end{multline*}

    Wir haben bereits vorausgesetzt, dass $\hat V \in \C^{N \times j}$ vollen Rang $j$ hat, d.h. $\hat v_1, \dots, \hat v_j$ linear unabhängig sind.
    Die Spalten von $W^\ast \hat V$ wären also genau dann linear abhängig, wenn eine davon $0$ ist, d.h.

    \begin{align*}
        \Exists i = 1, \dots, j:
            (\hat v_i, w_{1, 1})_2 = \cdots = (\hat v_i, w_{k, L_k})_2 = 0,
            \quad
            \text{d.h.}
            \quad
            \hat v_i \in (\Span W)^{\bot_2}.
    \end{align*}

    Das ist aber vernachlässigbar.
    Bei der Vektoriteration (Potenzmethode) geht man schließlich auch davon aus, dass der Startvektor (Zufallsvektor) nicht orthogonal auf den Eigenraum, des betrags-größten Eigenwerts, steht.
    Hier haben wir es zu tun, mit einer Zufallsmatrix $\hat V \in \C^{N \times j}$ aus $j$ Zufallsvektoren $\hat v_1, \dots, \hat v_j$, und der Summe aller Links-Eigenräume, d.h.

    \begin{align*}
        \Span W
        =
        \sum_{n=1}^k
            \ker A^\ast(\lambda_n).
    \end{align*}

    \item $S := \tilde V^\ast V \in \C^{J \times J}$ habe vollen Rang $J$, also sogar $S \in \GL_J(\C)$.
    Diese Annahme ist, analog zum vorigen Punkt, sinnvoll.
    Es gilt nämlich

    \begin{multline*}
        S^\ast
        =
        V^\ast \tilde V
        =
        (v_1, \dots, v_J)^\ast (\tilde v_1, \dots, \tilde v_J) \\
        =
        \begin{pmatrix}
            v_1^\ast \tilde v_1 & \cdots & v_1^\ast \tilde v_J \\
            \vdots              & \ddots & \vdots              \\
            v_J^\ast \tilde v_1 & \cdots & v_J^\ast \tilde v_J \\
        \end{pmatrix}
        =
        \begin{pmatrix}
            (\tilde v_1, v_1)_2 & \cdots & (\tilde v_J, v_1)_2 \\
            \vdots              & \ddots & \vdots              \\
            (\tilde v_1, v_J)_2 & \cdots & (\tilde v_J, v_J)_2 \\
        \end{pmatrix}.
    \end{multline*}

    $\tilde V \in C^{N \times J}$ hat, als Teilmatrix einer unitären $\in \U_N(\C)$, vollen Rang $J$, d.h. $\tilde v_1, \dots, \tilde v_J$ sind linear unabhängig.
    Die Spalten von $S^\ast$ wären also genau dann linear abhängig, wenn eine davon $0$ ist, d.h.

    \begin{align*}
        \Exists i = 1, \dots, J:
            (\tilde v_i, v_1)_2 = \cdots = (\tilde v_i, v_J)_2 = 0,
            \quad
            \text{d.h.}
            \quad
            \tilde v_i \in (\Span V)^{\bot_2}.
    \end{align*}

    Das ist aber vernachlässigbar.
    $\tilde V$ hängt schließlich von $\hat V$ ab, ist also in gewisser Weise zufällig.
    Hier haben wir es zu tun, mit einer (Quasi-)Zufallsmatrix $\tilde V \in \C^{N \times J}$ aus $J$ (Quasi-)Zufallsvektoren $\tilde v_1, \dots, \tilde v_J$, und der Summe aller Rechts-Eigenräume, d.h.

    \begin{align*}
        \Span V
        =
        \sum_{n=1}^k
            \ker A(\lambda_n).
    \end{align*}

\end{enumerate}

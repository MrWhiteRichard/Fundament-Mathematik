\section{Voraussetzungen}

Bei der Herleitung des Algorithmus' haben wir unter anderem auf die Matrizen $V$, $W$, $D$, und $S$ zurückgegriffen.
Man sollte sich aber bewusst sein, dass diese Matrizen bei der Implementierung des Algorithmus nicht verwendet werden, nachdem sie ohne Kenntnis der gesuchten Eigenpaare nicht berechenbar sind.

Die Tatsache, dass sie nicht im Algorithmus \ref{alg:integral_methode_zusammenfassung} vorkommen, heißt aber nicht, dass ihre Eigenschaften (z.B. $S \in \GL_J(\C)$) unwichtig sind.
Wählt man z.B. eine zu kleine Matrix $\hat V$, könnten wir unter Umständen die Regularität von $S$ verlieren.
Die numerischen Resultate werden dann in der Regel inkorrekt sein.

Wir fassen zusammen, was der Algorithmus \ref{alg:integral_methode_zusammenfassung} braucht, um korrekte Ergebnisse zu liefern.

\begin{enumerate}[label = \arabic*.]

    \item $\lambda_1, \dots, \lambda_k \in \Lambda$ seien allersamt halb-einfach und liegen im Inneren von $\Gamma$, d.h. insbesondere nicht darauf.
    Sonst könnten theoretisch Probleme bei der Approximationen von $A_0$ und $A_1$, vermöge einer der Quadraturformeln $Q_m$, $m \in \N$, entstehen.
    Sollte dies passieren, müsste man die Kurve $\Gamma$ anpassen.

    \item $V, W \in \C^{N \times J}$ haben vollen Rang $J$, d.h. deren Spalten seien linear unabhängig.

    $V, W$ bestehen ja schließlich aus Rechts- bzw. Links-Eigenvektoren.
    Die Voraussetzung gilt zumindest im linearen Fall, daher ist es durchaus plausibel sie auch im nichtlinearen Fall anzunehmen.

    \item $\hat V$ sei eine hinreichend große, gleichverteilt gewählte Zufallsmatrix, mit vollem Rang $j$.

    Zur Illustration, dass auch diese Annahme sinnvoll ist, sei dazu $K$ ein endlicher Körper und $\hat V \in K^{j \times j}$.
    Durch Abzählen der möglichen Komponenten der Spalten der regulären Matrizen, kommt man auf die Wahrscheinlichkeit

    \begin{align*}
        \mathbf P(\hat V \in \GL_j(K))
        & =
        \frac
        {
            |\GL_j(K)|
        }{
            |K^{j \times j}|
        }
        =
        \frac{1}
        {
            |K|^{j \cdot j}
        }
        \prod_{i=1}^j
            \pbraces
            {
                |K|^j - |K|^{i-1}
            } \\
        & =
        \prod_{i=1}^j
            \frac
            {
                |K|^j - |K|^{i-1}
            }{
                |K|^j
            }
        =
        \prod_{i=1}^j
            \pbraces
            {
                1 - \frac{1}{|K|^{j + 1 - i}}
            }
        \xrightarrow{|K| \to |\C|}
        1.
    \end{align*}

    \item $W^\ast \hat V \in C^{J \times j}$ habe vollen Rang $J \leq j$.
    Diese Annahme ist sinnvoll.
    Es gilt nämlich

    \begin{multline*}
        W^\ast \hat V
        =
        (w_{1, 1}, \dots, w_{k, L_k})^\ast (\hat v_1, \dots, \hat v_j) \\
        =
        \begin{pmatrix}
            w_{1, 1}^\ast   \hat v_1 & \cdots & w_{1, 1}^\ast   \hat v_j \\
            \vdots                   & \ddots & \vdots                   \\
            w_{k, L_k}^\ast \hat v_1 & \cdots & w_{k, L_k}^\ast \hat v_j \\
        \end{pmatrix}
        =
        \begin{pmatrix}
            (\hat v_1, w_{1, 1})_2   & \cdots & (\hat v_j, w_{1, 1})_2   \\
            \vdots                   & \ddots & \vdots                   \\
            (\hat v_1, w_{k, L_k})_2 & \cdots & (\hat v_j, w_{k, L_k})_2 \\
        \end{pmatrix}.
    \end{multline*}

    Wir haben bereits vorausgesetzt, dass $\hat V \in \C^{N \times j}$ vollen Rang $j$ hat, d.h. $\hat v_1, \dots, \hat v_j$ linear unabhängig sind.
    Die Spalten von $W^\ast \hat V$ wären also genau dann linear abhängig, wenn eine davon $0$ ist, d.h.

    \begin{align*}
        \Exists i = 1, \dots, j:
            (\hat v_i, w_{1, 1})_2 = \cdots = (\hat v_i, w_{k, L_k})_2 = 0,
            \quad
            \text{d.h.}
            \quad
            \hat v_i \in (\Span W)^{\bot_2}.
    \end{align*}

    In der Praxis wird dieser Fall zum Glück kaum eintreten.
    Hier sieht man auch den Grund, warum wir $\hat{V}$ als Zufallsmatrix gewählt haben.
    Würde man beispielsweise stattdessen als Spalten in $\hat{V}$ Einheitsvektoren wählen, könnte diese Bedingung in der Praxis durchaus verletzt werden.
    Bei der Vektoriteration (Potenzmethode) geht man beispielweise auch davon aus, dass der Startvektor (Zufallsvektor) nicht orthogonal auf den Eigenraum des betrags-größten Eigenwerts steht.

    \item $S := \tilde V^\ast V \in \C^{J \times J}$ habe vollen Rang $J$, also $S \in \GL_J(\C)$.
    Diese Annahme ist, analog zum vorigen Punkt sinnvoll, da

    \begin{align*}
        S^\ast
        &=
        V^\ast \tilde V
        =
        (v_1, \dots, v_J)^\ast (\tilde v_1, \dots, \tilde v_J) \\
        &=
        \begin{pmatrix}
            v_1^\ast \tilde v_1 & \cdots & v_1^\ast \tilde v_J \\
            \vdots              & \ddots & \vdots              \\
            v_J^\ast \tilde v_1 & \cdots & v_J^\ast \tilde v_J \\
        \end{pmatrix}
        =
        \begin{pmatrix}
            (\tilde v_1, v_1)_2 & \cdots & (\tilde v_J, v_1)_2 \\
            \vdots              & \ddots & \vdots              \\
            (\tilde v_1, v_J)_2 & \cdots & (\tilde v_J, v_J)_2 \\
        \end{pmatrix}.
    \end{align*}

    Die Matrix $\tilde V \in C^{N \times J}$ hat, als Teilmatrix einer unitären Matrix, vollen Rang $J$, d.h. $\tilde v_1, \dots, \tilde v_J$ sind linear unabhängig.
    Die Spalten von $S^\ast$ wären also genau dann linear abhängig, wenn eine davon $0$ ist, d.h.

    \begin{align*}
        \Exists i = 1, \dots, J:
            (\tilde v_i, v_1)_2 = \cdots = (\tilde v_i, v_J)_2 = 0,
            \quad
            \text{d.h.}
            \quad
            \tilde v_i \in (\Span V)^{\bot_2}.
    \end{align*}

    Wieder ist dieser Fall in der Praxis vernachlässigbar.

\end{enumerate}

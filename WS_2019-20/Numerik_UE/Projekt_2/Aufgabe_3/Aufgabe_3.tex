\subsection{Projektbeschreibung}

Sei $A$ eine symmetrische, positiv definite Matrix. Von einer \textit{Cholesky-Zerlegung} der Matrix spricht man, wenn eine untere Dreiecksmatrix
$L$ vorliegt mit $A = LL^{T}$; die Matrix $L$ und ihre Transponierte nennt man dann \textit{Choleskyfaktoren} von $A$. Ziel dieses Projekts ist die Optimierung von Algorithmen zur Berechnung einer solchen Zerlegung für schwach besetzte Matrizen (\textit{sparse matrices}).
\newline

\subsection{Lösung linearer Gleichungssysteme mit Vorwärts- und Rückwärtssubstitution}

Der Sinn dieser Zerlegung ist als Spezialfall einer LU-Zerlegung, dass sich das Problem $Ax=y$ auf die zwei Gleichungssysteme
\begin{align*}
    Lz = y \text{~~~und~~~} L^{T}x=z
\end{align*}
reduzieren lässt. Mit $L$ bzw. $L^{T}$ liegt nun eine untere bzw. obere Dreiecksmatrix vor, die wir bekannterweise mit Vorwärts- bzw. Rückwärtssubstitution effizient handhaben können.
Diese effizienten Algorithmen zur Lösung linearer Gleichungssysteme wollen wir nun speziell für den Choleskyfaktor $L$ implementieren. Da wir mit Python eine Programmiersprache nutzen, die Matrizen zeilenweise speichert, arbeiten wir auch mit dieser Art der Speicherung und nicht mit unteren Dreiecksmatrizen im Standardspaltenformat.

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Vorwärtssubstitution}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
def vorw(L,b):
    n = len(b)

    x = np.zeros(n)

    for i in range(n):
        sum = 0
        for j in range(i):
            sum += L[i][j]*x[j]
        x[i] = (b[i]-sum)/L[i][i]
    return x
\end{lstlisting}

Die Rückwärtssubstitution bekommt in unserem Fall auch die untere Dreiecksmatrix $L$ als Input, arbeitet dann allerdings mit der Tatsache, dass mit $L^{T}$ eine obere Dreiecksmatrix vorliegt.

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Rückwärtssubstitution}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
def rueckw(L,b):
    n = len(b)

    x = np.zeros(n)

    for i in [n-1-k for k in range(n)]:
        sum = 0
        for j in range(i,n):
            sum += L[j][i]*x[j]  ##L transponiert ist obere Dreiecksmatrix
        x[i] = (b[i]-sum)/L[i][i]

    return x
\end{lstlisting}

\subsection{Berechnung der Cholesky-Zerlegung}
\subsubsection{Zwei Varianten der Zerlegung}

Nun wollen wir für eine symmetrische, positiv definite Matrix $A$ ihren Choleskyfaktor $L$ berechnen. Hierfür stehen uns zwei Möglichkeiten zur Verfügung.

In der Vorlesung wurde ein Algorithmus in der folgenden Variante angegeben:
\begin{enumerate}
\itemsep0em
\item[] 1 \textbf{for} $k = 1,...,n$
\item[] 2 \qquad $L_{kk} = \sqrt{A_{kk} - \sum_{j=1}^{k-1} L_{kj}^2}$
\item[] 3 \qquad \textbf{for} $i = k+1,...,n$
\item[] 4 \qquad \qquad $L_{ik} = (A_{ik} - \sum_{j=1}^{k-1} L_{ij} L_{kj})/L_{kk}$.

\end{enumerate}


Diese Variante kann folgendermaßen implementiert werden, wobei die Summe $\sum_{j=1}^{k-1}L_{kj}^{2}$ aus Zeile $2$ dem Skalarprodukt der $k$-ten Zeile der bisherigen Matrix $L$ mit sich selbst entspricht:

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Zerlegung eintragsweise ohne Überschreiben}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
def chol1(A):
    n = len(A)
    L = np.zeros((n,n))
    for k in range(n):
        L[k][k] = np.sqrt((A[k][k] - L[k]@L[k]))
        for i in range(k+1,n):
            L[i][k] = (A[i][k] - L[i]@L[k])/L[k][k]
    return L
\end{lstlisting}

Eine zweite Möglichkeit für die Berechnung der Choleskyfaktoren ist folgende:

\begin{enumerate}
\itemsep0em
\item[] 1 \textbf{for} $k = 1,...,n$
\item[] 2 \qquad $A_{kk} = \sqrt{A_{kk}}$
\item[] 3 \qquad \textbf{for} $i = k+1,...,n$
\item[] 4 \qquad \qquad $A_{ik} = A_{ik}/A_{kk}$
\item[] 5 \qquad \textbf{for} $i = k+1,...,n$
\item[] 6 \qquad \qquad \textbf{for} $j = i,...,n$
\item[] 7 \qquad \qquad \qquad $A_{ji} = A_{ji} - A_{jk} A_{ik}$.

\end{enumerate}

Dieser Algorithmus überschreibt den linken unteren Teil der Matrix $A$ direkt mit ihrem Choleskyfaktor und agiert nur auf Spalten. Das ist so zu verstehen, dass die for-Schleife in Zeile 3-4 die Division der $k$-ten Spalte (unterhalb der Hauptdiagonale) durch den Eintrag $A_{kk}$ ist. Auch die for-Schleife in Zeile 6-7 beschreibt, dass von der $i$-ten Spalte (unterhalb der Hauptdiagonale) die entsprechende $k$-te Spalte mal $A_{ik}$ abgezogen wird.


\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Zerlegung spaltenweise mit Überschreiben}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
def chol2(A):
     n = len(A)
     J = nuller(A)
     for k in range(n):
         A[k][k] = np.sqrt(A[k][k])
         A[k+1:n,k] = A[k+1:n,k]/A[k][k]
         for i in range(k+1,n):
             A[i:,i] -= A[i,k]*A[i:,k]

     for i in range(n):
         for j in range(i+1,n):
             A[i][j] = 0
     return A
\end{lstlisting}

Es bleibt noch zu zeigen, dass der zweite Algorithmus das Gleiche leistet wie der erste. Diese Äquivalenz zeigen wir mithilfe von Induktion für die Spalten $1,...,n$ (die Zeilenangaben beziehen sich dabei auf den nummerierten Code):

Die erste Spalte wird im zweiten Algorithmus nur einmal verändert. Dabei wird der Eintrag $A_{11}$ mit seiner Wurzel überschrieben und alle Einträge unter der Hauptdiagonale werden durch das neue $A_{11}$ dividiert. Die gesamte erste Spalte von $A$ entspricht also nun der ersten Spalte von $L$ aus Algorithmus 1, da in diesem die Summen für $k=1$ jeweils leer sind.

Betrachten wir nun eine beliebige Spalte $m \leq n$ und nehmen als Induktionsvoraussetzung an, dass für jedes $k < m$ die $k$-te Spalte schon der $k$-ten Spalte von $L$ aus dem ersten Algorithmus entspricht (vor Anwendung auf die $m$-te Spalte). In jeder Schleife wird also von jedem Eintrag $A_{jm}$ der $m$-ten Spalte $L_{jk}L_{mk}$ abgezogen (Zeile 7). Diese $m-1$ Subtraktionen entsprechen $-\sum_{k=1}^{m-1}L_{jk}L_{mk}$ bzw. speziell für $j=m$ der Summe $-\sum_{k=1}^{m-1}L_{mk}^{2}$ aus Zeile 4 bzw. 2 des ersten Algorithmus.

In der $m$-ten Schleife wird nun wieder das bis dahin berechnete Diagonalelement $A_{mm}^{(1)} = A_{mm}-\sum_{k=1}^{m-1}L_{mk}^{2}$ mit der Wurzel überschrieben und die jeweiligen $A_{jm}^{(1)}$ durch das neue, korrekte Diagonalelement geteilt. Nun entspricht also auch die $m$-te Spalte von $A$ der $m$-ten Spalten des Choleskyfaktors.


\subsubsection{Vergleich des Aufwands}
Beim Vergleich der Effizienz beider Varianten stellt sich heraus, dass die erste Variante schneller ist. Würden wir mit Matrizen im Standardspaltenformat arbeiten, könnten wir uns eine verbesserte Effizienz durch die zweite Variante erwarten. Da uns aber wie bereits erwähnt mit Python zeilenweise gespeicherte Matrizen vorliegen, ist der Aufwand durch den Zugriff auf gesamte Spalten sogar größer.

Daher haben wir uns entschieden, das Projekt mit der ersten Variante der Cholesky-Zerlegung weiterzuführen.

\subsection{Steigerung der Effizienz bei gegebenen Blockmatrizen}

Unseren Code möchten wir nun testen, indem wir das Gleichungssystem $Mx=b$ für beliebige rechte Seiten $b$ mittels Choleskyzerlegung lösen. Dabei sei $M \in \R^{n^{2} \times n^{2}}$, $n \in \N$ eine Blockmatrix
\begin{align*}
    M = \left(\begin{array}{cccccc}
                A & B && \boldsymbol{0} \\
                B & \ddots & \ddots & \\
                & \ddots & \ddots & B \\
                \boldsymbol{0} && B & A
           \end{array}
     \right)
\end{align*}

bestehend aus den Blöcken $A,B \in \R^{n \times n}$ mit

\begin{align*}
    A = \left( \begin{array}{cccccc}
                4 & -1 && 0 \\
                -1 & \ddots & \ddots & \\
                & \ddots & \ddots & -1 \\
                0 && -1 & 4
         \end{array}
        \right)
\mathrm{~und~}
    B = \left(\begin{array}{cccccc}
                -1 & 0 && 0 \\
                0 & \ddots & \ddots & \\
                & \ddots & \ddots & 0 \\
                0 && 0 & -1
          \end{array}
        \right).
 \end{align*}

Da $M$ strikt diagonaldominant ist, ist sie auch positiv definit. Die Symmetrie ist offensichtlich.

%Die Tests legen nahe, dass unser bisheriger Code korrekt ist.
Bei der Berechnung der Cholesky-Zerlegung der Matrix $M$ sticht ein interessantes Muster ins Auge; für ein beliebiges $n \in \N$ ist der Choleskyfaktor $L$ stets von folgender Bauart:

\begin{align*}
    L = \left(\begin{array}{cccccc}
                C &&& \boldsymbol{0} \\
                \ast & \ast && \\
                & \ddots & \ddots & \\
                \boldsymbol{0} && \ast & \ast
           \end{array}
     \right)
\end{align*}

mit einer unteren Dreiecksmatrix $C \in \R^{n \times n}$.

Das bedeutet, dass es in der ersten Spalte nur $n+1$ Einträge ungleich 0 gibt. Insbesondere hat das nur aus Nullern bestehende linke untere Dreieck eine Schenkellänge von $n^{2}-n-1$; für große $n$ ist das der Großteil der gesamten Matrix. Wir beobachten, dass dieses Dreieck aus Nullern auch in der ursprünglichen Matrix $M$ zu finden ist. Wir müssen diese Einträge also nicht mehr berechnen und können so die Effizienz des Algorithmus steigern.

In der folgenden effizienteren Variante der Cholesky-Zerlegung nutzen wir also aus, dass wir in jeder Spalte nur maximal $n$ Einträge unter der Hauptdiagonale berechnen müssen (umgesetzt in der for-Schleife Zeile 7 und 8 -- falls weniger als $n$ Einträge unter der Hauptdiagonale vorhanden sind, wollen wir natürlich nur bis zum Ende der Matrix gehen).

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Effiziente Cholesky-Zerlegung der Blockmatrizen}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
def efficientCholBlock(n):
    A = M(n)
    L = np.zeros((n*n,n*n))
    for k in range(n*n):
        L[k][k] = np.sqrt((A[k][k] - L[k]@L[k]))
        m = min(k+n,n*n-1)
        for i in range(k+1,m+1):
             L[i][k] = (A[i][k] - L[i]@L[k])/L[k][k]
    return L
\end{lstlisting}

Wie man der folgenden Tabelle entnehmen kann, wirkt sich der Verzicht auf die Berechnung der unnötigen Einträge stark auf die Effizienz des Algorithmus aus:


\begin{table}[htb]
\centering
\caption{Vergleich des Aufwands beider Implementierungen}
\begin{tabular}{lccc}
\toprule
\textbf{n}
&\textbf{$\frac{\mathrm{\textbf{Aufwand verbesserter Algorithmus}}}{\mathrm{\textbf{Aufwand allgemeiner Algorithmus}}}$}  & \\
	        \midrule
4 & 0.895 \\
8 & 0.339 \\
16& 0.122 \\
32& 0.059 \\
64& 0.031 \\
\end{tabular}
\end{table}

Für $n=64$, also eine Blockmatrix mit ca. 16 Millionen Einträgen, reduziert sich der Aufwand demnach um fast 97 Prozent.

\subsection{Theoretische Überlegungen zu schwach besetzten Matrizen}

Die obigen Beobachtungen lassen vermuten, dass im Allgemeinen schwach besetzte Matrizen auch schwach besetzte Choleskyfaktoren besitzen. Dies gilt nicht notwendigerweise, allerdings ist es in bestimmten Fällen möglich, die Besetzungsstruktur der Matrix auszunutzen, um den Aufwand der Zerlegung zu reduzieren. Als theoretische Grundlage dazu dient folgende Überlegung:


\begin{lemma}
Sei $A$ eine symmetrische, positiv definite Matrix und die untere Dreiecksmatrix $L$ ihr Choleskyfaktor. Sei $J_i(A) := \mathrm{min}\{ j: A_{ij} \neq 0 \}$ der erste Spaltenindex einer Zeile, an dem $A$ nicht Null ist. Dann ist $L_{ij} = 0$ für $j < J_i(A)$. In $L$ bleiben also führende Nuller in Zeilen erhalten.
\end{lemma}

\begin{proof}
Es gilt $A_{ij} = \sum_{k=1}^j L_{ik} L_{kj}^T
= \sum_{k=1}^j L_{ik} L_{jk}$. Der Eintrag $A_{ij}$ kann also als kanonisches Skalarprodukt der $i$-ten und $j$-ten Zeile von $L$ interpretiert werden. $A$ ist als positiv definite Matrix insbesondere regulär; aus dem Multiplikationssatz für Determinanten folgt sofort die Regularität von $L$ und daraus die Tatsache, dass alle Diagonaleinträge von $L$ ungleich Null sind. Wir zeigen nun $L_{ij} = 0$ für alle $j < J_i(A)$.
%Für $j = 1$ gilt $0 = A_{i1} = L_{11} L_{i1}$ und aus $L_{11} \neq 0$ folgt $L_{i1} = 0.$
Ist $j < J_i(A)$ und ist die Behauptung bereits für alle $k < j$ erfüllt, so gilt
\begin{align*}
0 = A_{ij} = \sum_{k=1}^{j-1} L_{ik} L_{jk} + L_{ij} L_{jj} = 0 + L_{ij} L_{jj} = L_{ij} L_{jj}.
\end{align*}
Aus $L_{jj} \neq 0$ schließen wir $L_{ij} = 0$.
\end{proof}

\subsection{Cholesky-Zerlegung von "`Pfeil-Matrizen"'}

Gegeben seien die Matrizen $M,N \in \R^{n \times n}$ mit den Besetzungsstrukturen

\begin{align*}
    M =
\left(\begin{array}{cccccc}
                \ast & \ast & \hdots & \ast \\
                \ast & \ast && \\
                \vdots && \ddots & \\
                \ast &&& \ast
      \end{array}
\right)
\mathrm{~und~}
    N =
\left(\begin{array}{cccccc}
                \ast &&& \ast \\
                & \ddots && \vdots \\
                && \ast & \ast \\
                \ast & \hdots & \ast & \ast
      \end{array}
\right).
\end{align*}

Mithilfe unserer implementierten Zerlegung stellen wir fest, dass die Besetzungsstrukturen der jeweiligen Choleskyfaktoren unseren Überlegungen entsprechend aussehen:

\begin{align*}
    L_{M} =
\left(\begin{array}{cccccc}
                \ast &&&\boldsymbol{0} \\
                \vdots & \ddots && \\
                \vdots & \ddots & \ddots & \\
                \ast &\cdots &\cdots & \ast
      \end{array}
\right)
\mathrm{~und~}
    L_{N} =
\left(\begin{array}{cccccc}
                \ast &&& \boldsymbol{0} \\
                & \ddots && \\
                && \ast & \\
                \ast & \hdots & \ast & \ast
      \end{array}
\right).
\end{align*}

Während der Choleskyfaktor von $M$ vollbesetzt ist, ist der von $N$ von der Gestalt eines halben Pfeils und besteht nach wie vor zum Großteil aus Nullern.

Haben wir eine Matrix der Form $M$ gegeben, möchten wir also nicht ihre dünne Besetzungsstruktur verschwenden, indem wir direkt eine Cholesky-Zerlegung durchführen. Sinnvoll wäre es, die Matrix durch Permutationen auf die Form einer Matrix $N$ zu bringen und erst im Anschluss zu zerlegen.

Offensichtlich leistet die Permutationsmatrix $P =
\left(\begin{array}{cccccc}
                &&1 \\
                & \reflectbox{$\ddots$}& \\
                1  &  &
      \end{array}
\right) $ genau
$M=P^{-1}NP$.

Die Implementierung dieser Permutation mithilfe einer Matrix $P$ würde natürlich unnötigen Speicher verbrauchen, daher bemerken wir, dass die Permutation der Umnummerierung $N_{ij} = M_{n+1-i,n+1-j}$ entspricht.

\subsection{Cholesky-Zerlegung von beliebigen schwach besetzten Matrizen}

Sei $M$ eine beliebige schwach besetzte, positiv definite symmetrische Matrix. Ähnlich wie bei den Pfeilmatrizen suchen wir eine Strategie, um durch Permutationen möglichst viele der Nuller "`linksbündig"' zu machen, um schwach besetzte Choleskyfaktoren zu erhalten und so bei der Zerlegung Aufwand zu sparen.

Folgende Strategie erfüllt dieses Ziel sehr gut:

\begin{enumerate}
    \item Wir suchen die Spalte mit den meisten Nullern. Diese wird mit der ersten Spalte getauscht.
    \item Wir suchen die Spalte mit den meisten Nullern an den Stellen, an denen die neue erste Spalte auch eine Null hat. Diese wird mit der zweiten Spalte getauscht.
    \item Wir suchen die Spalte mit den meisten Nullern an den Stellen, an denen die zweite Spalte auch eine Null hat. Diese wird mit der dritten Spalte getauscht.

    $\vdots$
\end{enumerate}

Die jeweiligen Vertauschungen speichern wir als Tupel in einem Vektor ab, der als Äquivalent zur Permutationsmatrix gesehen werden kann.

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Permutation einer Matrix nach obigem Schema}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
def sort(A):
    p=[]
    n = len(A)
    for i in range(n):
        j = i # Spalte, die spaeter an die Stelle i gesetzt wird
        max = 0
        for k in range(i,n):
            c = 0
            for l in range(0,n):
                if(i != 0):
                    if(A[l][k] == 0 and A[l][i-1] == 0):
                        c += 1
                if(i == 0):
                    if(A[l][k] == 0):
                        c += 1

            if(c > max):
                max = c
                j = k

        if(i != j):
            p += [(i,j)]
        tmp = np.copy(A[:,i])
        A[:,i] = np.copy(A[:,j])
        A[:,j] = np.copy(tmp)
        tmp2 = np.copy(A[i,:])
        A[i,:] = np.copy(A[j,:])
        A[j,:] = np.copy(tmp2)
    return A, p
\end{lstlisting}

Offenbar ist die Inverse einer Permutationsmatrix $P$ genau ihre Transponierte.\newline Wegen $x^T (P^{-1}NP) x = x^T (P^TNP) x = (Px)^T N (Px)$ bleibt die Definitheit einer Matrix $N$ unter einer solchen Transformation erhalten (weil Permutationsmatrizen regulär sind, ist $Px \neq 0$ für $x \neq 0$).
Die permutierte Matrix ist wegen $(P^TNP)^T = P^TN^TP = P^TNP$ auch wieder symmetrisch.

Haben wir nun eine permutierte Version von $M$, die uns eine günstige Berechnung der Choleskyfaktoren ermöglicht, müssen wir uns noch überlegen, wie wir erreichen, dass nur die notwendigen Einträge berechnet werden.

Die Schwierigkeit ist, dass wir nicht wie bei den Blockmatrizen einfach nur bis zu einem gewissen Index der Spalte gehen können, da die linksbündigen Nuller nur zeilenweise geordnet sind. Da unser Algorithmus die Matrix allerdings spaltenweise durchläuft, benötigen wir eine Funktion, die uns für jede Spalte die Indizes liefert, deren Einträge nicht Null sind.

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Output ist eine Liste $I$, die zu jeder Spalte $k$ eine Liste $I_k$ mit den Zeilenindizes der Nichtnulleinträge von $k$ enthält}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
def nichtnuller(A):
    n = len(A)
    J = []
    for i in range(n):
        c = 0
        j = 0
        while(A[i][j] == 0):
            c += 1
            j += 1
        J += [c]
    I = []
    for k in range(n):
        Ik = []
        for j in range(k+1,n):
            if J[j] <= k:
                Ik += [j]
        I += [Ik]
    return I
\end{lstlisting}


Mithilfe dieser Funktion können wir also eine effiziente Variante entwickeln, eine bereits sortierte, schwach besetzte Matrix zu zerlegen:

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Berechnung des Choleskyfaktors unter Ausnützung führender Nuller}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
def efficientChol(A):
    n = len(A)
    L = np.zeros((n,n))
    I = nichtnuller(A)
    for k in range(n):
        L[k][k] = np.sqrt((A[k][k] - L[k]@L[k]))
        for i in I[k]:
             L[i][k] = (A[i][k] - L[i]@L[k])/L[k][k]
    return L
\end{lstlisting}
Der Code entscheidet sich vom ursprünglichen lediglich durch den Kopf der for-Schleife in Zeile 7.


Wir testen nun unsere Implementierungen für schwach besetzte Matrizen. Dazu generieren wir zufällige positiv definite, symmetrische $n\times n$-Matrizen, die ungefähr $2n$ Nichtnulleinträge haben. Die folgende Tabelle zeigt die gemittelten Werte von zehn Tests; in der dritten Spalte kann man den Anteil der benötigten Zeit des naiven Algorithmus an der Zeit, die der verbesserte Algorithmus benötigt hat, ablesen.


\begin{table}[htb]
\centering
\caption{Vergleich des Aufwands der beiden Algorithmen}
\begin{tabular}{lccc}
\toprule
\textbf{n}	& \textbf{$\frac{\mathrm{\textbf{Anzahl Nuller}}}{\mathrm{\textbf{n}}}$}
&\textbf{$\frac{\mathrm{\textbf{Aufwand verbesserter Algorithmus}}}{\mathrm{\textbf{Aufwand ursprünglicher Algorithmus}}}$}  & \\
	        \midrule

4 & 1.500 & 0.728 \\
8 & 2.500 & 0.734 \\
16 & 2.000 & 0.686 \\
32 & 2.250 & 0.628 \\
64 & 2.250 & 0.512 \\
128 & 2.172 & 0.516 \\
256 & 2.258 & 0.332 \\
512 & 2.355 & 0.418 \\
1024 & 2.248 & 0.425 \\
2048 & 2.234 & 0.406 \\
4096 & 2.213 & 0.364 \\
\end{tabular}
\end{table}

Der zweite Algorithmus schneidet also wie erwartet besser ab. Wir hätten auch folgende Strategie wählen können, um die Matrix zu permutieren:

\begin{enumerate}
    \item Wir suchen die Spalte mit den meisten Nullern. Diese wird mit der ersten Spalte getauscht.
    \item Wir suchen die Spalte mit den meisten Nullern an den Stellen, an denen die neue erste Spalte auch eine Null hat. Diese wird mit der zweiten Spalte getauscht.
    \item Wir suchen die Spalte mit den meisten Nullern an den Stellen, an denen \textbf{alle vorherigen} Spalten (in unserer Variante: nur die Spalte direkt davor) auch eine Null haben. Diese wird mit der dritten Spalte getauscht.

    $\vdots$
\end{enumerate}

In den Tests zeigt sich, dass dadurch eine noch deutlichere Steigerung der Effizienz bei der Berechnung der Choleskyzerlegung möglich ist, der größere Aufwand für das Sortieren macht diese aber wieder wett. Deshalb haben wir uns für die erste Strategie entschieden.
\newline
\newline
Zuletzt möchten wir noch zusammenfassen, wie ein ganzes lineares Gleichungssystem mithilfe der dargestellten Methoden nun gelöst werden kann:

Sei $M \in \R^{n \times n}$ eine beliebige schwach besetzte symmetrisch positiv definite Matrix. Gesucht ist die Lösung des linearen Gleichungssystems $Mx=b$. Wir finden eine Permutationsmatrix $P$, sodass gilt $M = P^{-1}NP$ mit $N$ geeignet für eine günstige Cholesky-Zerlegung. Wir können das Gleichungssystem also umschreiben in
\begin{align*}
    Mx = b \text{~~} \Leftrightarrow \text{~~} P^{-1}N\underbrace{Px}_{=\colon\widetilde{x}} = b \text{~~} \Leftrightarrow \text{~~} N\widetilde{x} = \underbrace{Pb}_{=\colon\widetilde{b}}.
\end{align*}

Die Gleichung $ N\widetilde{x} = \widetilde{b}$ können wir nun wie zu Beginn besprochen mithilfe der Cholesky-Zerlegung von $N$ und anschließender Vorwärts- und Rückwärtssubstitution lösen.\footnote{Die Vorwärts- und Rückwärtssubstitution könnte in diesem Fall mit unserem Wissen über die Besetzungsstruktur der Choleskyfaktoren natürlich auch noch effizienter implementiert werden. Darauf soll allerdings in diesem Projekt nicht eingegangen werden.} Den Lösungsvektor $\widetilde{x}$ müssen wir nun lediglich wieder mit der Matrix $P^{-1}$ rückpermutieren, um unsere gesuchte Lösung $x$ zu finden.

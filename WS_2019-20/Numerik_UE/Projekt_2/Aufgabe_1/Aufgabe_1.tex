Eine häufige Problemstellung in der Numerischen Mathematik lautet lineare Gleichungssysteme mit großen, dünn besetzten Matrizen zu lösen.
Dabei kommen meist iterative Verfahren zum Einsatz, die in diesem Projekt effizient implementiert werden.
\subsection*{a)}
Zuerst testen wir als Basis wie aufwändig der direkte Löser (numpy.linalg.solve) ist um später einen Maßstab für unsere
Effizienzsteigerungen zu haben.
Wir generieren eine symmetrisch positiv definite Zufallsmatrix $A \in \mathbb{R}^{n\times n}$, wobei pro Zeile eine fixe Anzahl an Einträgen
ungleich Null sind und testen für eine zufällige rechte Seite $b \in \mathbb{R}^n$ bis zu welcher Größe $n$ das lineare
Gleichungssystem
\begin{align*}
  Ax = b
\end{align*}
mit einem direkten Löser (numpy.linalg.solve) in akzeptabler Zeit gelöst werden kann.
\begin{lstlisting}[language=Python]

def zufallsmatrix(n, nonzeros):
	A = np.concatenate((np.zeros((n,nonzeros)), np.random.rand(n, n-nonzeros)), axis = 1)
	for i in range(n):
		np.random.shuffle(A[i])
	return A + A.T + np.diag(np.random.rand(n)*n)

A_base = zufallsmatrix(5000,100)

x = [i for i in range(400,n+1,200)]
y = []
for n in x:
	A = A_base[:n,:n]
	b = np.random.rand(n)
	start = time.process_time()
	z = np.linalg.solve(A,b)
	end = time.process_time()
	y.append(end-start)
\end{lstlisting}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Aufgabe_1/plot_a.png}
    \caption{Rechenzeit abhängig von der Problemgröße}
    \label{fig:my_label}
\end{figure}
Wie in Abbildung 1 ersichtlich verhält sich die Rechenzeit kubisch in Relation zur Problemgröße.
Zum Testen wurde eine Zufallsmatrix mit 100 Nicht-Null-Einträgen pro Zeile (nicht exakt, da die Symmetriesierung den Wert pro
Zeile verzerrt) und einer Gesamtgröße von 5000 erstellt. Der direkte Löser wurde schließlich auf die $(200k \times 200k)$-dimensionalen Ausschnitte der oberen rechten Ecke angewandt ($2 \leq k \leq 25$). Wie man sieht erreichen wir damit schon langsam die Grenze
des akzeptabel Berechenbaren, für die volle $5000\times5000$-Matrix braucht der Algorithmus schon über 10 Sekunden.
\FloatBarrier
\subsection*{b)}
Um die Effizienz der Problemlösung zu steigern greifen wir also nicht mehr auf den direkten Löser zurück,
sondern implementieren eine optimierte Version des CG-Verfahrens.
\begin{lstlisting}[language=Python]
def cg(A,b,x0,tol):
    xt = x0
    r0 = b - np.dot(A,xt)
    d = r0
    while(np.linalg.norm(r0) > tol):
        prod = np.dot(np.transpose(r0),r0)
        prod2 = np.dot(A,d)
        alpha = prod/np.dot(np.transpose(d),prod2)
        xt = xt + alpha*d
        r0 = r0 - alpha*prod2
        beta = np.dot(np.transpose(r0),r0)/prod
        d = r0 + beta*d
    return xt
\end{lstlisting} \label{cg}
Im Vergleich dazu Algorithmus 8.10 aus dem Numerik-Skript:
\begin{flalign*}
&1: t = 0 &\\
&2: r^{(0)} = b - Ax^{(0)} &\\
&3: d^{(0)} = r^{(0)} &\\
&4: \textbf{while}~ \norm{r^{(t)}} > \textbf{TOL}~ \text{do} &\\
&5:\quad \alpha_t = \frac{r^{(t)}*d^{(t)}}{d^{(t)}*Ad^{(t)}} &\\
&6:\quad x^{(t+1)} = x^{(t)} + \alpha_td^{(t)} &\\
&7:\quad t = t + 1 &\\
&8:\quad r^{(t)} = b - Ax^{(t)} &\\
&9:\quad \beta_{t-1} = -\frac{r^{(t)}*Ad^{(t-1)}}{d^{(t-1)}*Ad^{(t-1)}} &\\
&10:\quad d^{(t)} = r^{(t)} + \beta_{t-1}d^{(t-1)} &\\
&11: \textbf{end while}&
\end{flalign*}
Die Algorithmen ähneln sich in vielen Schritten, weisen aber durchaus einige Unterschiede auf.
Daher müssen wir noch beweisen, dass die beiden Algorithmen wirklich äquivalent sind.
Wir führen den Beweis mittels Induktion über die Anzahl der Iterationen. \\
Dabei bezeichnen wir mit * die Variablen aus unserem Algorithmus und ohne * die Variablen des Algorithmus
aus dem Skript. \\
Starten wir mit dem Induktionsanfang. \\
\begin{align*}
   &A = A^*, b = b^*, x_0 = x_0^* \\
   &r_0 = b - Ax_o = b^* - A^*x_0^* = r_0^* \\
   &d_0 = r_0 = r_0^* = d_0^* \\
   &\alpha_0 = \frac{r_0^Td_0}{d_0^TAd_0} = \frac{r_0^{*T}d_0^*}{d_0^{*T}Ad_0^*} = \frac{r_0^{*T}r_0^*}{d_0^{*T}Ad_0^*} = \alpha_0^* \\
   &x_1 = x_0 + \alpha_0d_0 = x_0^* + \alpha_0^*d_0^* = x_1^* \\
   &r_1 = b - Ax_1 = b^* - A^*x_1^* = b^* - A^*x_0^* - \alpha_0^*Ad_0^* = r_0^* - \alpha_0^*A^*d_0^* = r_1^* \\
   &\beta_0 = - \frac{r_1^TAd_0}{d_0^TAd_0} = \frac{-r_1^{*T}Ad_0^*}{d_0^{*T}Ad_0^*} = \frac{-r_1^{*T}Ar_0^*}{r_0^{*T}Ar_0^*}
\end{align*}
Unter Ausnutzung der Orthogonalität der Residuen erhalten wir: $r_1^{*T}r_0^* = 0$ und somit können wir den Bruch
folgendermaßen erweitern:
\begin{align*}
  \frac{-r_1^{*T}Ar_0^*}{r_0^{*T}Ar_0^*} = \frac{r_1^{*T}r_0^* -\alpha_0^*r_1^{*T}Ar_0^*}{\alpha_0^*r_0^{*T}Ar_0^*}
\end{align*}
Nun berechen wir
\begin{align*}
  r_1^{*T}r_1^* = r_1^{*T}(r_0^*-\alpha_0^*Ad_0^*) = r_1^{*T}r_0^* - \alpha_0^*r_1^{*T}Ar_0^*
\end{align*}
und setzen $\alpha_0^* = \frac{r_0^{*T}r_0^*}{d_0^{*T}Ad_0^*}$ ein:
\begin{align*}
  \frac{r_1^{*T}r_0^* -\alpha_0^*r_1^{*T}Ar_0^*}{\alpha_0^*r_0^{*T}Ar_0^*} = \frac{r_1^{*T}r_1^*}{\frac{r_0^{*T}r_0^*}{r_0^{*T}Ar_0^*}r_0^{*T}Ar_0^*}
  = \frac{r_1{*T}r_1^*}{r_0^{*T}r_0^*} = \beta_0^* \\
  d_1 = r_1 + \beta_0d_0 = r_1^* + \beta_0^*d_0^* = d_1^*
\end{align*}
Damit haben wir die Gleichheit der Variablen nach dem ersten Schleifendurchlauf gezeigt. \\
Sei nun nach $n$ Schleifendurchläufen die Gleichheit aller vorhergehenden Variablen vorausgesetzt: \\
\begin{align*}
  \alpha_{n-1} = \alpha_{n-1}*, x_n = x_n^*, r_n = r_n^*, \beta_{n-1} = \beta_{n-1}^*, d_n = d_n^* \\
  \alpha_n = \frac{r_n^Td_n}{d_n^TAd_n} = \frac{r_n^{*T}d_n^*}{d_n^{*T}A^*d_n^*} = \frac{r_n^{*T}(r_n^* + \beta_{n-1}^*d_{n-1}^*)}{d_n^{*T}A^*d_n^*} \\
\end{align*}
Jetzt nutzen wir die Eigenschaft: $\forall~ 0 \leq j < m: r_m^Td_j = 0$ und erhalten:
\begin{align*}
  \frac{r_n^{*T}(r_n^* + \beta_{n-1}^*d_{n-1}^*)}{d_n^{*T}A^*d_n^*} = \frac{r_n^{*T}r_n^*}{d_n^{*T}A^*d_n^*} = \alpha_n^* \\
  x_{n+1} = x_n + \alpha_nd_n = x_n^* + \alpha_n^*d_n^* = x_{n+1}^* \\
  r_{n+1} = b - Ax_{n+1} = b - A(x_n + \alpha_nd_n) = b - Ax_n - \alpha_nd_n = \\
  = r_n - \alpha_nAd_n = r_n^* - \alpha_n^*A^*d_n^* = r_{n+1}^* \\
  \beta_n = - \frac{r_{n+1}^TAd_n}{d_n^TAd_n} = \frac{r_{n+1}^{*T}r_n^* - \alpha_n^*r_{n+1}^{*T}A^*d_n^*}{d_n^{*T}A^*d_n^*}
  = \frac{r_{n+1}{*T}r_{n+1}^*}{r_n^{*T}r_n^*} = \beta_n^* \\
  d_{n+1} = r_{n+1} + \beta_nd_n = r_{n+1}^* + \beta_n^*d_n^* = d_{n+1}^*
\end{align*}
Und der Beweis ist vollständig. \\

Nun stellt sich natürlich die Frage, welcher der beiden Algorithmen zu bevorzugen ist.
Nachdem sie mathematisch äquivalent sind, bleibt nur noch die Frage nach dem Aufwand zu überprüfen.
Am aufwändigsten ist natürlich die Matrix-Vektor-Multiplikation, wovon wir in unserem Algorithmus
im Gegensatz zu jenem aus dem Skript nur eine pro Schleifendurchlauf benötigen.
Zusätzlich dazu brauchen wir 3 Vektor-Vektor-Multiplikationen und 4 Vektor-Skalar-Operationen. \\
Damit erhalten wir insgesamt $n^2+7n$ Flops pro Durchlauf. \\
Im Vergleich dazu verwendet der Algorithmus aus dem Numerik-Skript pro Iteration zwei Matrix-Vektor-Multiplikationen
und ist daher aus Effizienzgründen unterlegen, da wir damit alleine schon $2n^2$ Flops pro Durchlauf brauchen. \\

Nachdem geklärt ist, in welcher Version der Algorithmus implementiert werden soll, ist es noch essentiell sich mit der
Konvergenztheorie dahinter zu beschäftigen.
Die Theorie besagt, dass das CG-Verfahren spätentens nach $n$ Durchläufen die exakte Lösung liefert und für die Iterierten
folgende Fehlerabschätzung gilt:
\begin{align*}
  \Vbraces{x^{(t)}-A^{-1}b}_A \leq 2\left(\frac{1-1/\sqrt{\kappa}}{1+1/\sqrt{\kappa}}\right)^t\Vbraces{x^{(0)}-A^{-1}b}_A, \quad t \in \N,
\end{align*}
mit der spektralen Konditionszahl $\kappa = \text{cond}_2(A)$. \\
Also sollte das Verfahren exponentiell kovergieren ($\Landau{AB^t}$) mit Konstanten\\
$A = 2\Vbraces{x^{(0)}-A^{-1}b}_A, \quad B = \frac{1-1/\sqrt{\kappa}}{1+1/\sqrt{\kappa}}$\\
Wir haben das Verfahren mit diagonaldominanten , symmetrisch, positiv definiten Zufallsmatrizen getestet. Eine weitere
Möglichkeit wäre gewesen, eine Zufallsmatrix $A$ zu erstellen und das CG-Verfahren auf $A\cdot A^T$ anzuwenden. Allerdings ist
im letzteren Fall die Konditionszahl deutlich schlechter und teilweise erreichen wir die gewünschte Toleranz erst nach über $n$
Schritten, wo wir in der Theorie ohne Rechenfehler bereits exakt sein sollten. Also haben wir uns für erstere Methode entschieden.
Interessanterweise konvergiert der Fehler wesentlich schneller gegen 0 als die obere Schranke des theoretischen Fehlerschätzers
vermuten lassen würde und unsere vorgegebene Toleranz von $10^{-8}$ wird bereits
nach etwa $400$ Iterationen erreicht, noch weit vor dem theoretisch
(bis auf Rechenfehler) garantierten exakten Resultat nach $n = 5000$ Durchläufen.
Unsere Testwerte: \\

\begin{lstlisting}[language=Python]
n = 5000
A = zufallsmatrix(n,20)
b = np.random.rand(n)
tol = 10**(-8)
x0 = np.random.rand(n)
\end{lstlisting}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Aufgabe_1/c2.png}
    \caption{Residuum abhängig von der Anzahl an Iterationen}
    \label{fig:my_label}
\end{figure}
\FloatBarrier
\subsection*{c)}
Um die Effizienz weiter zu steigern, müssen wir natürlich noch die dünne Besetztheit ausnutzen.
Dies bewerkstelligen wir mit einer eigens geschrieben Klasse in Python, welche die Matrix-Vektor-Multiplikation
deutlich effizienter ausführen sollte als die Standard-Multiplikation.
Dünn besetzte Matrizen erlauben effizientere Implementierungen als voll besetzte, indem beim Speichern und Rechnen
nur Einträge die ungleich Null sind berücksichtigt werden. Eine Möglichkeit einer solchen Implementierung ist das sogenannte
\textit{compressed sparse row} Format. Anstelle aller Einträge $A_{ij}, i,j = 1,\dots,n$ einer Matrix $A \in \mathbb{R}^{n\times n}$
werden ein Vektor $v \in \mathbb{R}^{n\times n}$ aller Einträge ungleich Null, ein Vektor $J \in \mathbb{N}_0^m$ von Spaltenindizes
und ein Vektor $I \in \mathbb{N}_0^{n+1}$ von Zeigern gespeichert. Die $i$-te Zeile von $A$ ist dann gegeben durch
\begin{align*}
  A_{ij} = \begin{cases}
    v_{k(j)}, & \text{falls}~ j \in \{J_{I_i}, J_{I_i} + 1, \dots, J_{I_i + 1} - 1\} \\
    0, & \text{sonst}
  \end{cases}
\end{align*}
\begin{lstlisting}[language=Python]
class Sparse:

    def __init__(self,b,v, J = np.zeros(0), I = np.zeros(0)):
        if b:
            self.v = np.array(v)
            self.J = np.array(J)
            self.I = np.array(I)
            self.n = len(self.I)-1
        else:
            self.v, self.J, self.I = self.fromdense(v)
            self.n = len(self.I)-1

    def __matmult__(self,b):
        d = np.zeros(self.n)
        for i in range(self.n):
            x = np.array(self.J[self.I[i]:self.I[i+1]]).astype(int)
            d[i] = self.v[self.I[i]:self.I[i+1]]@b[x]
        return d


    def todense(self):
        A = np.zeros([self.n,self.n])
        for i in range(self.n):
            for j in range(self.I[i],self.I[i+1]):
                A[i][self.J[j]] = self.v[j]
        return A

    def fromdense(self,A):
        v,J = np.zeros(0), np.zeros(0)
        I = np.array([0])
        c = 0
        for i in range(np.shape(A)[0]):
            for j in range((np.shape(A))[0]):
                if A[i][j] != 0:
                    v = np.append(v,A[i][j])
                    J = np.append(J,j)
                    c += 1
            I = np.append(I,c)
        return v, J, I
\end{lstlisting}


\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Aufgabe_1/matmult_plot.png}
    \caption{Vergleich Numpy Matrixmultiplikation vs. Sparse Matrixmultiplikation}
    \label{mul}
\end{figure}
\FloatBarrier

In Abbildung \ref{mul} sehen wir, dass die Sparse-Matrix-Vektor-Multiplikation schneller ist, als die Implementierung in numpy.
Wir starten erst ab einer Matrixgröße $n=3000$, da der Messfehler bei einer kleineren Größe überwiegt und der Plot nicht aussagekräftig wäre.


\subsection*{d)}
Jetzt können wir unsere neue Klasse noch mit der bereits vorhandenen CG-Implementierung kombinieren. \\
\begin{lstlisting}[language=Python]
def Scg(A,b,x0,tol):
    xt = x0
    r0 = b - A.__matmult__(xt)
    d = r0
    while(np.linalg.norm(r0) > tol):
        prod = np.dot(np.transpose(r0),r0)
        prod2 = A.__matmult__(d)
        alpha = prod/np.dot(np.transpose(d),prod2)
        xt = xt + alpha*d
        r0 = r0 - alpha*prod2
        beta = np.dot(np.transpose(r0),r0)/prod
        d = r0 + beta*d
    return xt
\end{lstlisting}

Bei dieser CG-Implementierung verwenden wir anstelle der Numpy-Matrix-Vektor-Multiplikation die Implementierung von der Sparse-Klasse.
Zu beachten ist, dass die Matrix A ein Objekt der Klasse Sparse sein muss, damit die Funktion durchgeführt werden kann.
Ansonsten ist die Implementierung ident zum vorherigen cg-Verfahren.
\newpage
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Aufgabe_1/Cg_Scg.png}
    \caption{cg-Verfahren vs. cg-Verfahren mit Klasse Sparse}
    \label{scg}
\end{figure}
\FloatBarrier
Wie erhofft liefert die Kombinierung mit der Sparse-Klasse ab einer gewissen Größe noch einen zusätzlichen Effizienzschub.
Ab einer Matrixgröße von ca. 1000 ist das Scg-Verfahren effizienter als die übliche Implementierung.
Man sieht außerdem, dass beide Algorithmen eine Laufzeit von etwa $\Landau{n^2}$ (siehe Abb. \ref{scg}).

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Aufgabe_1/Scg_npsolve.png}
    \caption{Vergleich numpy.linalg.solve mit cg-Verfahren aus Klasse Sparse}
    \label{linalgvs}
\end{figure}
\FloatBarrier

In Abbildung \ref{linalgvs}  sieht man, dass bei kleiner Matrixgröße das Verfahren in numpy deutlich schneller als das cg-Verfahren ist, aber bei zunehmender Größe benötigt das cg-Verfahren weniger Zeit.

\subsection*{e)}
Der letzte Schritt zur Effizienz-Optimierung ist nun noch die Matrix selbst noch für unsere Zwecke zu verbessern.
Die Konvergenzgeschwindigkeit des CG-Verfahrens ist durch die spektrale Konditionszahl cond($A$) der Matrix $A$ bestimmt.
Um die Konvergenzgeschwindigkeit zu erhöhen löst man das vorkonditionierte System
\begin{align*}
  D^{-1}AD^{-T} = D^{-1}b
\end{align*}
und gewinnt die Lösung $x$ dann durch $x = D^{-T}y$. Die Matrix $D$ wird dabei so gewählt, dass
\begin{itemize}
\item für beliebige Vektoren $z \in \mathbb{R}^n$ der Vektor $D^{-T}D^{-1}z$ einfach zu berechen ist und
\item $\text{cond}(D^{-1}AD^{-T}) < \text{cond}(A)$.
\end{itemize}
Implementierung des vorkonditionierten CG-Verfahrens:
\begin{lstlisting}[language=Python]
    def vcg(A,b,x0,P,tol):
        r0 = b - A.__matmult__(x0)
        P_inv = np.linalg.inv(P)
        z0 = P_inv@r0
        d = z0
        while(np.linalg.norm(r0) > tol):
            prod = np.dot(np.transpose(z0),r0)
            prod2 = A.__matmult__(d)
            alpha = np.dot(np.transpose(r0),z0)/np.dot(np.transpose(d),prod2)
            x0 = x0 + alpha*d
            r0 = r0 - alpha*prod2
            z0 = P_inv@r0
            beta = np.dot(np.transpose(z0),r0)/prod
            d = z0 + beta*d
    return x0
\end{lstlisting}

\subsection*{f)}
Wenn wir schließlich das vorkonditionierte CG-Verfahren mit $P = \text{diag}(A_{11},\dots,A_{nn})$ an strikt diagonaldominanten
Zufallsmatrizen mit positiven Diagonaleinträgen und an beliebigen symmetrisch, positiv definiten Zufallsmatrizen testen, sehen
wir unseren finalen Effizienzschub in diesem Projekt. \\
Wie in untenstehenden Grafik ersichtlich konnte mit der Vorkonditionierung die Anzahl der benötigten Iterationen
zur Erreichung der erwünschten Toleranz massiv gesenkt werden. Bei bereits strikt diagonaldominanten Matrizen mit positiven
Diagonaleinträgen ist die Konditionszahl
bereits relativ niedrig und somit kann die Vorkonditionierung nicht mehr so viel verbessern wie bei den normalen symmetrisch,
postiv definiten Zufallsmatrizen. Der Zeitgewinn schlägt sich bei den s.s.d. Matrizen leider nicht ganz so deutlich wieder,
aber ist dennoch vorhanden und merkbar. Bei den bereits strikt diagonaldominanten Matrizen braucht das vorkonditionierte CG-Verfahren
sogar deutlich mehr Zeit, da die Ersparnis der geringeren Anzahl an Schleifendurchläufen in diesem Fall eher minimal ist und
allerdings durch die Vorkonditionierung in jedem Iterationsschritt mehr Aufwand steckt. \\
Unsere Testwerte: Strikt diagonaldominante Matrix mit positiven Diagonaleinträgen: \\
\begin{lstlisting}[language=Python]
A = np.zeros((n,n))
row_sum = []
for i in range(n):
    non_zeros = np.random.rand(z)
    zeros = np.zeros(n-z)
    A[i] = np.concatenate((non_zeros, zeros))
    np.random.shuffle(A[i])
    row_sum.append(abs(np.sum(A[i]))+1)
A = A + A.T + np.diag(row_sum)
\end{lstlisting}
Standard s.s.d. Matrix:
\begin{lstlisting}[language=Python]
A = np.zeros((n,n))
for i in range(n):
    non_zeros = np.random.rand(z)
    zeros = np.zeros(n-z)
    A[i] = np.concatenate((non_zeros, zeros))
    np.random.shuffle(A[i])
A = A + A.T + np.diag(np.random.rand(n)*10)
\end{lstlisting}
\FloatBarrier
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Aufgabe_1/f_strikt.png}
    \caption{strikt diagonaldominante Matrix}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Aufgabe_1/f.png}
    \caption{s.s.d. Matrix}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Aufgabe_1/vcg.png}
    \caption{Zeitvergleich s.s.d. Matrix}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Aufgabe_1/vcg_strikt.png}
    \caption{Zeitvergleich strikt diagonaldominante Matrix}
\end{figure}

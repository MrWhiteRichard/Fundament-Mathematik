Eine häufige Problemstellung in der Numerischen Mathematik lautet lineare Gleichungssysteme mit großen, dünn besetzten Matrizen zu lösen.
Dabei kommen meist iterative Verfahren zum Einsatz, die in diesm Projekt effizient implementiert werden.
\subsection*{a)}
Generieren Sie eine symmetrisch positiv definite Zufallsmatrix $A \in \mathbb{R}^{n\times n}$, wobei pro Zeile eine fixe Anzahl an Einträgen
ungleich Null sind. Testen Sie für eine zufällige rechte Seite $b \in \mathbb{R}^n$ bis zu welcher Größe $n$ das lineare
Gleichungssystem
\begin{align*}
  Ax = b
\end{align*}
mit einem direkten Löser (numpy.linalg.solve) in akzeptabler Zeit gelöst werden kann.
Welchen Aufwand erwarten Sie abhängig von n? Plotten Sie die Rechenzeit in Abhängigkeit von der Problemgröße.
\begin{lstlisting}[language=Python]

def zufallsmatrix(n, nonzeros):
	A = np.concatenate((np.zeros((n,nonzeros)), np.random.rand(n, n-nonzeros)), axis = 1)
	for i in range(n):
		np.random.shuffle(A[i])
	return A + A.T + np.diag(np.random.rand(n)*10)

A_base = zufallsmatrix(5000,100)
\end{lstlisting}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Aufgabe_1/plot_a.png}
    \caption{Rechenzeit abhängig von der Problemgröße}
    \label{fig:my_label}
\end{figure}
Wie in Abbildung 1 ersichtlich verhält sich die Rechenzeit kubisch in Relation zur Problemgröße.
Zum Testen wurde eine Zufallsmatrix mit 100 Nicht-Null-Einträgen pro Zeile (nicht exakt, da die Symmetriesierung den Wert pro
Zeile verzerrt) und einer Gesamtgröße von 500 erstellt. Der direkte Löser wurde schließlich auf die $(k200 \times k200)$-dimensionalen,
rechts oberen Untermatrizen angewandt ($2 \leq k \leq 25$).
\FloatBarrier
\subsection*{b)}
Implementierung des CG-Verfahrens:
\begin{lstlisting}[language=Python]
def cg(A,b,x0,tol):
    xt = x0
    r0 = b - np.dot(A,xt)
    d = r0
    count = 0
    while(np.linalg.norm(r0) > tol):
        prod = np.dot(np.transpose(r0),r0)
        prod2 = np.dot(A,d)
        alpha = prod/np.dot(np.transpose(d),prod2)
        xt = xt + alpha*d
        r0 = r0 - alpha*prod2
        beta = np.dot(np.transpose(r0),r0)/prod
        d = r0 + beta*d
        count += 1
    print(count)
    return xt
\end{lstlisting}
Beweis der Äquivalenz obigen Algorithmus zu Algorithmus 8.10 im Numerik-Skript: \\
Wir führen den Beweis mittels Induktion: \\
Dabei bezeichnen wir mit * die Variablen aus unserem Algorithmus und ohne * die Variablen des Algorithmus
aus dem Skript. \\
Induktionsanfang: \\
\begin{align*}
   &A = A^*, b = b^*, x_0 = x_0^* \\
   &r_0 = b - Ax_o = b^* - A^*x_0^* = r_0^* \\
   &d_0 = r_0 = r_0^* = d_0^* \\
   &\alpha_0 = \frac{r_0^Td_0}{d_0^TAd_0} = \frac{r_0^{*T}d_0^*}{d_0^{*T}Ad_0^*} = \frac{r_0^{*T}r_0^*}{d_0^{*T}Ad_0^*} = \alpha_0^* \\
   &x_1 = x_0 + \alpha_0d_0 = x_0^* + \alpha_0^*d_0^* = x_1^* \\
   &r_1 = b - Ax_1 = b^* - A^*x_1^* = b^* - A^*x_0^* - \alpha_0^*Ad_0^* = r_0^* - \alpha_0^*d_0^* = r_1^* \\
   &\beta_0 = - \frac{r_1^TAd_0}{d_0^TAd_0} = \frac{-r_1^{*T}Ad_0^*}{d_0^{*T}Ad_0^*} = \frac{-r_1^{*T}Ar_0^*}{r_0^{*T}Ar_0^*}
\end{align*}
Unter Ausnutzung der Orthogonalität der Residuen erhalten wir: $r_1^{*T}r_0^* = 0$ und somit können wir den Bruch
folgendermaßen erweitern:
\begin{align*}
  \frac{-r_1^{*T}Ar_0^*}{r_0^{*T}Ar_0^*} = \frac{r_1^{*T}r_0^* -\alpha_0^*r_1^{*T}Ar_0^*}{\alpha_0^*r_0^{*T}Ar_0^*}
\end{align*}
Nun berechen wir
\begin{align*}
  r_1^{*T}r_1^* = r_1^{*T}(r_0^*-\alpha_0^*Ad_0^*) = r_1^{*T}r_0^* - \alpha_0^*r_1^{*T}Ar_0^*
\end{align*}
und setzen $\alpha_0^* = \frac{r_0^{*T}r_0^*}{d_0^{*T}Ad_0^*}$ ein:
\begin{align*}
  \frac{r_1^{*T}r_0^* -\alpha_0^*r_1^{*T}Ar_0^*}{\alpha_0^*r_0^{*T}Ar_0^*} = \frac{r_1^{*T}r_1^*}{\frac{r_0^{*T}r_0^*}{r_0^{*T}Ar_0^*}r_0^{*T}Ar_0^*}
  = \frac{r_1{*T}r_1^*}{r_0^{*T}r_0^*} = \beta_0^* \\
  d_1 = r_1 + \beta_0d_0 = r_1^* + \beta_0^*d_0^* = d_1^*
\end{align*}
Damit haben wir die Gleichheit der Variablen nach dem ersten Schleifendurchlauf gezeigt. \\
Sei nun nach $n$ Schleifendurchläufen die Gleichheit aller vorhergehenden Variablen vorausgesetzt: \\
\begin{align*}
  \alpha_{n-1} = \alpha_{n-1}*, x_n = x_n^*, r_n = r_n^*, \beta_{n-1} = \beta_{n-1}*, d_n = d_n^* \\
  \alpha_n = \frac{r_n^Td_n}{d_n^TAd_n} = \frac{r_n^{*T}d_n^*}{d_n^{*T}A^*d_n^*} = \frac{r_n^{*T}(r_n^* + \beta_{n-1}^*d_{n-1}^*)}{d_n^{*T}A^*d_n^*} \\
\end{align*}
Jetzt nutzen wir die Eigenschaft: $\forall 0 \leq j < m: r_m^Td_j = 0$ und erhalten:
\begin{align*}
  \frac{r_n^{*T}(r_n^* + \beta_{n-1}^*d_{n-1}^*)}{d_n^{*T}A^*d_n^*} = \frac{r_n^{*T}r_n^*}{d_n^{*T}A^*d_n^*} = \alpha_n^* \\
  x_{n+1} = x_n + \alpha_nd_n = x_n^* + \alpha_n^*d_n^* = x_{n+1}^* \\
  r_{n+1} = b - Ax_{n+1} = b - A(x_n + \alpha_nd_n) = b - Ax_n - \alpha_nd_n = \\
  = r_n - \alpha_nAd_n = r_n^* - \alpha_n^*A^*d_n^* = r_{n+1}^* \\
  \beta_n = - \frac{r_{n+1}^TAd_n}{d_n^TAd_n} = \frac{r_{n+1}^{*T}r_n^* - \alpha_n^*r_{n+1}^{*T}A^*d_n^*}{d_n^{*T}A^*d_n^*}
  = \frac{r_{n+1}{*T}r_{n+1}^*}{r_n^{*T}r_n^*} = \beta_n^* \\
  d_{n+1} = r_{n+1} + \beta_nd_n = r_{n+1}^* + \beta_n^*d_n^* = d_{n+1}^*
\end{align*}
Und der Beweis ist vollständig.

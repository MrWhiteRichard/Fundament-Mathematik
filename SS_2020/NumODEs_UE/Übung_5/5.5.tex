\begin{exercise}
Sei $M \in \R^{n \times n}$ mit $n \in \N$ eine diagonalisierbare Matrix und sei
$y: [0,T] \rightarrow \R^n$ Lösung des Anfangswertproblems
\begin{align*}
  y^{\prime} = My, \qquad y(0) = y_0 \in \R^n.
\end{align*}
Zeigen Sie, dass ein Runge-Kutta-Verfahren angewendet auf dieses Anfangswertproblem
exakt dem gleichen Runge-Kutta-Verfahren angewendet auf die separierten Anfangswertprobleme
\begin{align*}
  \widetilde{y}_j: [0,T] \rightarrow \C: \qquad \widetilde{y}_j = \lambda_j\widetilde{y}_j,
  \qquad \widetilde{y}_j(0) = \widetilde{y}_{j,0} \in \C, \qquad j = 1,\dots,n,
\end{align*}
entspricht (in welchem Sinne?), wobei $\lambda_j$ für $j = 1,\dots,n$ die Eigenwerte
der Matrix $M$ gemäß ihrer Vielfachheit sind. Wie müssen die Anfangsdaten $\widetilde{y}_{j,0}$
gewählt werden?
\end{exercise}
\begin{solution}
Um die Separierung des Anfangswertproblems zu motivieren, betrachte man die Substitution
\begin{align*}
  \widetilde{y} = V^{-1}y.
\end{align*}
 Wir erhalten das äquivalente Anfangswertproblem
\begin{align*}
  \widetilde{y}^{\prime} = (V^{-1}y)^{\prime} = V^{-1}V\Lambda\underbrace{V^{-1}y}_{ = \widetilde{y}} = \Lambda\widetilde{y},
\end{align*}
bei dem die einzelnen Komponenten bereits separiert sind, wir können also genauso gut schreiben
\begin{align*}
  \widetilde{y}_j = \lambda_j\widetilde{y}_j, \qquad j = 1,\dots,n.
\end{align*}
Die Anfangsdaten ergeben sich also einfach durch
\begin{align*}
   \widetilde{y}(0) = V^{-1}y(0) = V^{-1}y_0.
\end{align*}
Sei also
\renewcommand{\arraystretch}{1.5}
\begin{align*}
  \begin{matrix}
    c & \vline & A \\
    \hline
    & \vline & b^{\top}
  \end{matrix}
\end{align*}
das Butcher-Schema eines beliebigen $m$-stufigen Runge-Kutta-Verfahrens.
Ein Schritt dieses Verfahrens angewendet auf das erste Problem liefert für $h_0$
hinreichend klein ein wohldefiniertes Ergebnis mit
\begin{align*}
  y_1 &= y_0 + h_0 \sum_{j=1}^m b_jf\left(t + c_jh, y_0 + h\sum_{\ell = 1}^m A_{j\ell}k_{\ell}\right)
  = y_0 + h_0 \sum_{j=1}^m b_jM\left(y_0 + h\sum_{\ell = 1}^m A_{j\ell}k_{\ell}\right)
\end{align*}
Das selbe Verfahren angewendet auf die separierten Anfangswertprobleme ergibt
\begin{align*}
  \widetilde{y}_{1} = \widetilde{y}_{0} + h_0 \sum_{j=1}^m b_j\Lambda\left(\widetilde{y}_{0} + h\sum_{\ell = 1}^m A_{j\ell}\widetilde{k_{\ell}}\right)
\end{align*}
Setzen wir nun $\widetilde{y}_{0} = V^{-1}y_0$ ein, erhalten wir
\begin{align*}
  \widetilde{y}_{1} &= V^{-1}y_0 + h_0 \sum_{j=1}^m b_j\Lambda\left(V^{-1}y_0 + h\sum_{\ell = 1}^m A_{j\ell}\widetilde{k_{\ell}}\right) \\
  &= V^{-1}y_0 + h_0 \sum_{j=1}^m b_j\Lambda\left(V^{-1}y_0 + h\sum_{\ell = 1}^m A_{j\ell}V^{-1}k_{\ell}\right)
\end{align*}
und mit der Rücksubstitution $y_1 = V\widetilde{y}_{1}$ kommen wir wieder auf
das originale Problem zurück.
\begin{align*}
  V\widetilde{y}_{1} &= V\left(V^{-1}y_0 + h_0 \sum_{j=1}^m b_j\Lambda\left(V^{-1}y_0 + h\sum_{\ell = 1}^m A_{j\ell}V^{-1}k_{\ell}\right)\right) \\
  &= y_0 + h_0 \sum_{j=1}^m b_jV\Lambda V^{-1}\left(y_0 + h\sum_{\ell = 1}^m A_{j\ell}k_{\ell}\right) \\
  &= y_0 + h_0 \sum_{j=1}^m b_jM\left(y_0 + h\sum_{\ell = 1}^m A_{j\ell}k_{\ell}\right) = y_1
\end{align*}
\end{solution}

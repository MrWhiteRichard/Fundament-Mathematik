---
title: "HW8"
author: "Christian Sallinger"
date: "18 5 2021"
output: pdf_document
---

## 1. Cramér-Rao lower bound - Simulation

In Homework 7 Exercise 2 a density $f(x \mid \theta) = \theta x^{\theta -1}$ for $0 < x < 1$ and  $\theta > 0$ was given. The goal was to find a suitable function $g$ of the parameter $\theta$ such that there exists an unbiased estimator of $g(\theta)$ which attains the Cramér-Rao lower bound.

A unbiased statistic which attains the Cramér-Rao lower bound is for $g(\theta)=\frac{1}{\theta}$ biven by

$$
S_n(X_1,\dots, X_n)
=
-\frac{1}{n}\sum_{i=1}^n \ln(X_i).
$$

Implement the following steps in $\texttt{R}$:

(a) Write pdf \texttt{dhw}, cdf \texttt{phw}, quantile \texttt{qhw} and random sampling function \texttt{rhw} for the above distribution parameterized by $\theta$ (s) (see for example \texttt{?runif, ?norm}).

    \textit{Hint: Given an strict monotone continous cdf $F$, then $F^{-1}(U)$ is distributed with cdf $F$ for $U \sim U(0,1)$}.

(b) Fix an arbitrary $\theta$ and perform a simulation with growing sample size $n = 500, 1000, 1500,\dots,10000$ each with $100$ replications for the estimation of $g(\theta)$ with the statistic $S_n$.

(c) Create a scatter plot of all the estimates over the sample size, add the sample mean and standard deviation aggregated over the sample size to the plot. Finally, add the theoretical mean and standard deviation of the statistic $S_n$.

\textbf{Solution:}

(a)
```{r}
dhw <- function(x,theta){theta*x^(theta-1)*ifelse(1>x,1,0)*ifelse(x>0,1,0)}
phw <- function(x,theta){ifelse(x>0,1,0)*ifelse(x<1,x^theta,1)}
qhw <- function(q,theta){ifelse(q>0,1,0)*ifelse(q<1,q^(1/theta),1)}
rhw <- function(n,theta){dhw(runif(n),theta)}
```

(b)
```{r}
theta = 2
S_n <- function(n,theta){-1/n*sum(log(rhw(n,theta)))}

n_list = seq(500,10000,500)

for (n in n_list){
  est = replicate(100,S_n(n,theta))
}
```

## 2. Sufficient statistic and point estimator statistics

Let $X_1,\dots,X_n$ be a random sample from a population with pdf

$$
f(x\mid \theta)
=
\begin{cases}
\frac{\theta}{x^2}, & \theta \leq x \\
0, & \text{otherwise}
\end{cases}
$$

with unknown $\theta > 0$. Use the Factorization theorem to obtain a sufficient statistic for $\theta$.

\textbf{Solution:}

The Factorization theorem tells us that if $f(x\mid\theta)$ is the pdf of $X$ then $T(X)$ is a sufficient statistic for $\theta$ if and only if there exist functions $g(t\mid \theta)$ and $h(x)$ such that

$$
f(x\mid\theta) = g(T(x)\mid \theta)\cdot h(x).
$$

So we write out the pdf

$$
f(\textbf{x} \mid \theta)
=
\prod_{i=1}^n \frac{\theta}{x_i^2} \textbf{1}_{[\theta,\infty)}(x_i)
=
\theta^n \textbf{1}_{[\theta,\infty)}\Big(\min_{i=1,\dots,n} x_i\Big)\prod_{i=1}^n \frac{1}{x_i^2}
$$
So if we define $g(t \mid \theta):=\theta^n \textbf{1}_{[\theta,\infty)}(t)$ and $h(\textbf{x}):=\prod_{i=1}^n \frac{1}{x_i^2}$ we can see that $T(X)=\min_{i=1,\dots,n} X_i$ is a sufficient statistic.

## 3. Minimal sufficient statistic 1

Let $X_1,\dots,X_n$ be a random sample from a population with $\mathcal{N}(\mu,\mu)$ distribution, where $\mu > 0$ is unknown.

(a) Show that the statistic $\sum X_i^2$ is minimal sufficient in the $\mathcal{N}(\mu,\mu)$ family.

(b) Show that the statistic $\big(\sum X_i, \sum X_i^2 \big)$ is sufficient but not minimal sufficient in the $\mathcal{N}(\mu,\mu)$ family.

\textbf{Solution:}

(a) We will use the Theorem on page 29 of Lecture 8, it states that $T(\textbf{X})$ is a minimal sufficient statistic if for every sample point $\textbf{x}$ and $\textbf{y}$ the ratio

$$
\frac{f(\textbf{x}\mid \theta)}{f(\textbf{y}\mid \theta)}
$$
is constant as a function of $\theta$ if and only if $T(\textbf{x}) = T(\textbf{y})$. The fraction of the pdf's is

\begin{align*}
\frac{f(\textbf{x}\mid \mu)}{f(\textbf{y}\mid \mu)}
&=
\frac{\exp\Big(-\frac{1}{2\mu}\sum_{i=1}^n (x_i - \mu)^2\Big)}{\exp\Big(-\frac{1}{2\mu}\sum_{i=1}^n (y_i - \mu)^2\Big)}\\
&=
\exp\Big(\frac{1}{2\mu}\sum_{i=1}^n y_i^2 -2y_i\theta - x_i^2 + 2x_i\theta\Big)
=
\exp\Big(\frac{1}{2\mu}\big(\sum_{i=1}^n y_i^2- \sum_{i=1}^n x_i^2\big)\Big)\exp\Big(\sum_{i=1}^n(x_i-y_i)\Big)
\end{align*}

We can easily see that this is constant in $\mu$ if and only if $T(x)=T(y)$, so the statistic really is a minimal sufficient statistic.

(b)
To show sufficiency we can use the Factorization theorem:

\begin{align*}
f(\textbf{x}\mid \theta)
&=
\frac{1}{(2\pi\mu)^{n/2}}\exp\Big(-\frac{1}{2\mu}\sum_{i=1}^n(x_i - \mu)^2 \Big)
=
\frac{1}{(2\pi\mu)^{n/2}}\exp\Big(-\frac{1}{2\mu}\sum_{i=1}^n \big(x_i^2 -2\mu x_i+\mu^2\big) \Big) \\
&=
\frac{1}{(2\pi\mu)^{n/2}}\exp\Big(-\frac{1}{2\mu}\big(\sum_{i=1}^n x_i^2 -2\mu \sum_{i=1}^n x_i \big)\Big)\exp(-\frac{n\mu^2}{2\mu})
=g(T(x))
\end{align*}

if we define $g((t_1,t_2)\mid \mu):= \frac{1}{(2\pi\mu)^{n/2}}\exp\Big(-\frac{t_2 -2\mu t_1}{2\mu}\Big)\exp(-\frac{n\mu^2}{2\mu})$ (and $h(x):=1$). To see that it is not a minimal sufficient statistic we take a sample where $\sum_{i=1}^n x_i \neq \sum_{i=1}^n y_i$ and $\sum_{i=1}^n x_i^2 = \sum_{i=1}^n y_i^2$ (e.g. $x_i = 3, y_i = -3$ for all $i$). It holds that $\bar{T}(\textbf{x})\neq\bar{T}(\textbf{y})$ but as we have seen in (a) the ratio of the pdf's is still constant in $\mu$, so our statistic is not minimal sufficient.

## 4. Minimal sufficient statistic 2

Let $X_1,\dots,X_n$ be a random sample from a population with pdf

$$
f(x\mid \theta)
=
\begin{cases}
\frac{2x}{\theta}, & 0<x<\theta \\
0, & \text{otherwise}
\end{cases}
$$

with unknown parameter $\theta >0$. Find a minimal sufficient statistic for $\theta$.

\textbf{Solution:}

## 5. Sufficiency, bias, Rao-Blackwell theorem

Let $X_1,\dots,X_n$ be i.i.d. $\textit{Poi}(\lambda)$, with unknown $\lambda > 0$.

(a) Show that $Y = \sum_{i =1}^n X_i$ is a sufficient statistic for $\lambda$.

(b) Find an unbiased estimator of $p_r = P(X = r)$, which depends only on $X_1$.

    Find $P(X_1 = r\mid Y=k)$ both for $k \geq r$ and $k<r$.
    
    Hence use the Rao-Blackwell theorem to improve your estimator of $p_r$.
    
\textbf{Solution:}

(a) We again use the Factorization theorem with $g(t \mid \lambda) := e^{-n \lambda}e^{\ln(\lambda)t}$ and $h(\textbf{k}) = \prod_{i=1}^n\frac{1}{k_i !}$:

$$
f(\textbf{k}\mid \lambda)
=
\prod_{i=1}^n \frac{\lambda^{k_i}e^{-\lambda}}{k_i !}
=
e^{-n\lambda}h(\textbf{k})\exp\Big(\ln\big(\prod_{i=1}^n \lambda^{k_i}\big)\Big)
=
h(\textbf{k})e^{-n\lambda}\exp\Big(\sum_{i=1}^n k_i \ln(\lambda)\Big)
=
h(\textbf{k})g(Y\mid \lambda)
$$

(b) We first note that 

$$
p_r = \frac{\lambda^r e^{-\lambda}}{r!}
=
\mathbb{P}(X_1 = r)
$$

An unbiased estimator is given by 

$$
W_r
=
\textbf{1}_{\{r\}}(X_1)
$$
since

$$
\mathbb{E}(W_r)
=
\sum_{k=0}^\infty \textbf{1}_{\{r\}}(X_1) \mathbb{P}(X=k)
=
\sum_{k=0}^\infty \textbf{1}_{\{r\}}(X_1) \mathbb{P}(X_1=k)
=
\mathbb{P}(X_1 = r)
=
p_r
$$


To find $\mathbb{P}(X_1 = r \mid Y =k)$ we first note that $Y \sim \textit{Poi}(n\lambda)$ as we have seen in Homework 4.

$$
\mathbb{P}(X_1 = r \mid Y =k)
=
\frac{\mathbb{P}(Y =k\mid X_1 = r)\mathbb{P}(X_1 = r)}{\mathbb{P}(Y = k)}
=
\frac{\mathbb{P}(\sum_{i=2}^n X_i =k-r)\mathbb{P}(X_1 = r)}{\mathbb{P}(Y = k)}
=
\cdots
$$

Here we can already see that the probability is zero if $r > k$, since $\sum_{i=2}^n \sim \textit{Poi}((n-1)\lambda)$ and the poisson distribution has its support on the natural numbers. For $k \geq r$ we calculate further

$$
\cdots
=
\frac{k!}{(k-r)!r(r)(n\lambda)^k}\lambda^k (n-1)^{k-r}
=
\binom{k}{r}(n-1)^{k-r}n^{-k}
$$

We can now use the Rao-Blackwell Theorem, since $W_r$ is an unbiased estimator of $p_r$ and $Y$ is a sufficient statistic for $\lambda$ then

$$
\phi(Y)
=
\mathbb{E}(W_r\mid Y)
=
\mathbb{P}(X_1 =r \mid Y)
=
\begin{cases}
\binom{Y}{r}(n-1)^{Y-r}n^{-Y}, & Y \geq r \\
0, & Y<r
\end{cases}
$$

is a uniformly better unbiased estimator of $p_r$.
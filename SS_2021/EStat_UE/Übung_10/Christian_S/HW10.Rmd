---
title: "HW10"
author: "Christian Sallinger"
date: "8 6 2021"
header-includes:
  - \usepackage{dsfont}
output: pdf_document
---
```{r,eval=FALSE,include=FALSE}
install.packages("ggplot2")
library(ggplot2)
```

## 1. Exponential family

Show that the one-parameter exponential family has a monotone likelihood ratio in a sufficient statistic $T(\textbf{X})$ if the natural parameter $w(\theta)$ is a non-decreasing function in $\theta$.

\textbf{Solution:}

We say a family of distributions $\{f_\theta , \theta \in \Theta\}$ with a one-dimensional parameter $\theta$ has a monotone likelihood ratio in a statistic $T(\textbf{X})$ if for any $\theta_1 < \theta_2$, the likelihood ratio $f_{\theta_2}(\textbf{x})/f_{\theta_1}(\textbf{x})$ is a non-decreasing function of $T(\textbf{x})$. We also remind that a family of pdfs is called an one-parameter exponential family (not curved!) if it can be represented in the form

$$
f(x\mid \theta)
=
h(x)c(\theta) e^{w(\theta)t(x)}
$$
where $h(x) > 0, t(x)$ are real valued functions that do not depend on $\theta$ and $c(\theta) > 0, w(\theta)$ are real valued functions that do not depend on $x$. The likelihood ratio is 

$$
\lambda(\textbf{x})
=
\frac{f_{\theta_2}(\textbf{x})}{f_{\theta_1}(\textbf{x})}
=
\prod_{i=1}^n \frac{h(x_i)c(\theta_2) \exp\Big({ w(\theta_2)t(x_i)}\Big)}{h(x_i)c(\theta_1) \exp\Big({ w(\theta_1)t(x_i)}\Big)}
=
\Big(\frac{c(\theta_2)}{c(\theta_1)}\Big)^n \exp\Big((w(\theta_2)-w(\theta_1))\cdot \sum_{i=1}^n t(x_i)\Big)
$$

We know form the Lecture (Lecture 8, page 48) that the statistic $T(\textbf{X}) = \sum_{i=1}^nt(X_i)$ is a sufficient statistic for $\theta$. Since the natural parameter $w(\theta)$ is non decreasing and $\theta_1 < \theta_2$ it holds that

$$
w(\theta_2)-w(\theta_1) \geq 0
$$
so the likelihood ratio is indeed a non decreasing function of this statistic.

## 2. Confidence interval 1

In the June 1986 issue of Consumer Reports, some data on the calorie content of beef hot dogs is given. Here are the numbers of calories in 20 different hot dog brands:

$$
186, 181, 176, 149,184,190,158,139,175,148,152,111,141,153,190,157,131,149,135,132.
$$

Assume that the numbers are from a normal distribution with mean $\mu$ and variance $\sigma^2$, both unknown. Use \texttt{R} to obtain a $90 \%$ confidence interval for the mean number of calories $\mu$.

\textbf{Solution:}

From the lecture (lecture 9, page 16) we know that a $100(1-\alpha)\%$ confidence interval for $\mu$ (when $\sigma^2$ is unknown) is given by

$$
\Big[\bar{X} - t_{\alpha/2}(n-1)\cdot\frac{S}{\sqrt{n}},\bar{X} + t_{\alpha/2}(n-1)\cdot\frac{S}{\sqrt{n}}\Big]
$$
We will calculate this interval with \texttt{R}:

```{r}
#Put the data into an array
data_arr = c(186, 181, 176, 149,184,190,158,139,175,148,152,111,141,153,190,157,131,149,135,132)

#n is the number of observations
n = length(data_arr)

#Calculate the mean
X_bar = mean(data_arr)

#Calculate our estimation for the variance, the subtraction in the sum happens element-wise
S = sqrt(sum((data_arr-X_bar)^2)*(1/(n-1)))

#The 1 - alpha/2 quantile of the t distribution with n-1 degrees of freedom
t_alpha = qt(1 - 0.05,n-1)

#Calculate the bounds of the CI and print them
lower_bound = X_bar - t_alpha*S/sqrt(n)
upper_bound = X_bar + t_alpha*S/sqrt(n)

paste("90% CI = [",lower_bound,",",upper_bound,"]")
```

## 3. Confidence interval 2

Suppose $X_1,\dots, X_n$ are i.i.d. with pdf

$$
f(x\mid \lambda, \eta)
=
\begin{cases}
\lambda e^{-\lambda(x-\eta)}, & x > \eta \\
0, & \text{otherwise}
\end{cases}
$$

where $\lambda$ and $\eta$ are positive parameters with $\eta$ known but $\lambda$ unknown. Find the MLE of $\lambda$ and construct a $(1-\alpha)100\%$ confidence interval for $\lambda$ when $n$ is assumed to be large.

\textbf{Solution:}

The likelihood function is

\begin{align*}
L(\lambda \mid \textbf{x} )
&=
\prod_{i=1}^n \lambda e^{-\lambda(x_i - \eta)} \mathds{1}_{(\eta,\infty)}(x_i) \\
&=
\lambda^n \exp\Big(-\lambda \sum_i(x_i-\eta)\Big) \mathds{1}_{(\eta,\infty)}(x_{(1)}) \\
&=
\exp\Big(-\lambda \sum_i(x_i-\eta) + n \ln(\lambda)\Big)  \mathds{1}_{(\eta,\infty)}(x_{(1)})
\end{align*}

To be able to take the logarithm we constrain ourselves to $\bf x$ for which $\mathds{1}_{(\eta,\infty)}(x_{(1)}) = 1$ otherwise the likelihood function obtains its maximum $0$ for any $\lambda$ anyways. The log-likelihood is then

$$
\ell(\lambda\mid \textbf{x} )
=
-\lambda \sum_i(x_i-\eta) + n \ln(\lambda)
$$

We calculate the derivative

$$
\frac{\partial}{\partial \lambda} \ \ell(\lambda\mid \textbf{x})|_{\lambda = \hat \lambda}
=
-\sum_i(x_i - \eta) + \frac{n}{\hat\lambda}
$$
and set it to zero

$$
-\sum_i(x_i - \eta) + \frac{n}{\hat\lambda}
\stackrel{!}{=}
0
\iff
\hat\lambda = \frac{n}{\sum_i(x_i - \eta)}
$$


To show that $\hat{\lambda} = \frac{n}{\sum_i(x_i - \eta)}$ is really our MLE we check the second derivative

$$
\frac{\partial^2}{\partial \lambda^2} \ \ell(\lambda\mid \textbf{x})|_{\lambda = \hat \lambda}
=
-\frac{n}{\hat\lambda^2} < 0
$$
so we have indeed found our MLE. We note that we can rewrite our MLE as 

$$
\hat{\lambda}
=
\frac{n}{\sum_i(x_i - \eta)}
=
\frac{n}{\sum_i x_i - n\eta}
=
\frac{1}{\frac{1}{n}\sum_i x_i - \eta}
=
\frac{1}{\bar{x}-\eta}
$$

To get the Fisher information we calculate the score function (for $x > \eta$)

$$
z(x,\lambda)
=
\frac{\partial}{\partial \lambda}
\log f(x\mid \lambda)
=
\frac{1}{\lambda} - x + \eta
$$
Then

$$
z^\prime(x,\lambda)
\frac{\partial^2}{\partial \lambda^2}
\log f(x\mid \lambda)
=
-\frac{1}{\lambda^2}
$$
So the Fisher information is
$$
I(\lambda)
=
-\mathbb{E}(z^\prime(x,\lambda))
=
\frac{1}{\lambda^2}
$$
By the theorem in lecture 7, page 12, it holds that 

$$
\frac{\hat \lambda -\lambda}{\frac{1}{\sqrt{n}}}
\stackrel{d}{\longrightarrow}
\mathcal{N}(0,\lambda^2)
$$

We know that the MLE is a consistent estimator and the Fisher information is continuous for $\lambda \in \mathbb{R}^+$, so with Slutsky's theorem we get

$$
\frac{\hat \lambda -\lambda}{\frac{1}{\sqrt{nI(\hat{\lambda})}}}
\stackrel{d}{\longrightarrow}
\mathcal{N}(0,1)
$$

If we denote the $1-\frac{\alpha}{2}$ quantile of the normal with $z_{\alpha/2}$ we then get our $100(1-\alpha)\%$ confidence interval, our so called Wald interval for $\lambda$ with

$$
\Big[\hat{\lambda} - z_{\alpha/2} \cdot\frac{1}{\sqrt{nI(\hat{\lambda})}},
\hat{\lambda} + z_{\alpha/2} \cdot\frac{1}{\sqrt{nI(\hat{\lambda})}}
\Big]
=
\Big[\hat{\lambda} - z_{\alpha/2} \cdot\frac{\hat{\lambda}}{\sqrt{n}},
\hat{\lambda} + z_{\alpha/2} \cdot\frac{\hat{\lambda}}{\sqrt{n}}
\Big]
$$

## 4. confidence interval 3

Use \texttt{R} to generate a random sample $X_1,\dots, X_n$ from $Pois(1)$ distribution (for $n = 30$ and $n = 100$). Compute the $90\%$ confidence interval for $\lambda$, check if it contains the true value of $\lambda = 1$, and repeat this $10000$ times. What is the fraction of simulations for which the confidence interval covers $\lambda$?

\textbf{Solution:}

We know from the lecture (lecture 9, page 9 where we can also find this HW) that the asymptotic $100(1-\alpha)\%$ confidence interval for $\lambda$ is

$$
\Big[\hat{\lambda} - z_{\alpha/2}\cdot\sqrt{\frac{\hat{\lambda}}{n}},\hat{\lambda} + z_{\alpha/2}\cdot\sqrt{\frac{\hat{\lambda}}{n}}\Big]
$$
where $\hat{\lambda} = \bar{X}$.

```{r}
n_1 = 30
n_2 = 100

#Create a random n Pois(lambda) sample, compute the 90% confidence
#interval for lambda, returns 1 if the confidence interval contains
#lambda and 0 otherwise
check_value = function(n,lambda = 1){
  c_sample = rpois(n,lambda)
  
  X_bar = mean(c_sample)
  
  z_alpha = qnorm(1-0.05)
  
  lower_bound = X_bar - z_alpha*sqrt(X_bar/n)
  upper_bound = X_bar + z_alpha*sqrt(X_bar/n)
  
  if(lambda <= upper_bound){
    if(lambda >= lower_bound){
        return(1)
    }
    else{
      return(0)
    }
  }
  else{
    return(0)
  }
}

print(sum(replicate(10000,check_value(n_1))))

print(sum(replicate(10000,check_value(n_2))))
```

## 5. Boxplots and quantiles

Two novel randomized algorithms (A and B) are to be compared regarding their runtimes. Both algorithms were executed $n$ times. The runtimes (in seconds) are stored in the file $\texttt{algorithms.Rdata}$

(a) Set the working directory and load the data using $\texttt{load}()$. Create a boxplot to compare the running times. Color the boxes and add proper notations (axes notations, title, etc.). More info via $\texttt{?boxplot}$

(b) Comment on the following statements / questions only using the graphic.

\quad \quad  (i) The first quartile of the times in A was about?

\quad \quad (ii) The interquartile range of the times in B is about trice the interquartile range     of A.

\quad \quad  (iii) Is $n = 100$?
 
\quad \quad (iv) More than half of the running times in B were faster than $3/4$ of the running    times in A.

\quad \quad (v) At least $50\%$ in A were faster than the $25\%$ slowest in B.

\quad \quad (vi) at least $60\%$ in A were faster than the $25\%$ slowest in B.

(c) Regarding the runtimes

$$
23.7, 13.7, 7.6, 9.0, 44.3, 3.5, 2.2, 34.2
$$

which are a subset of B, find all empirical (a) medians, (b) first quartiles and (c) $2/3$-quantiles (not using $\texttt{R}$).

\textbf{Solution:}

```{r}
runtimes = load("algorithms.Rdata")

# boxplot using base graphics package
boxplot(runningtimes,col=c("lightgreen", "lightblue"), xlab ="runtimes in sec", 
        ylab = "algorithm", horizontal = TRUE)
legend(x="topright",col=c("lightgreen", "lightblue"),pch=c(15,15),
       legend=c("algorithm A", "algorithm B"))
title("runtimes boxplots")

# much cooler way to draw the boxplots!
times = data.frame(times = c(runningtimes$algoA,runningtimes$algoB),group= c(rep("A",length(runningtimes$algoA)),rep("B",length(runningtimes$algoB))))

#ggplot(times, aes(x=group, y=times,fill=group)) + geom_boxplot()
```

(b)

\quad (i) 20

\quad (ii) No, more like twice

\quad (iii) Can not answer this with only the graphic (but yes it is)

\quad (iv) Yes

\quad (v) No 

\quad (vi) We can only say that at least $75\%$ in A were faster than the $25\%$ slowest in B.

(c) The median is $m = (9,13.7)$, the first quartile $q_1 = (3.5,7.6)$ and the $2/3$-quantile is $q_{2/3} = 23.7$.
% -------------------------------------------------------------------------------- %

\begin{exercise}[\textbf{The GLRT for the normal variance - simple hypotheses}]

Derive the generalized likelihood ratio test (GLRT) for the normal variance:
Assume $X_1,\dots,X_n$ are i.i.d. $\mathcal{N}(\mu,\sigma^2)$, where both $\mu$ and $\sigma$
are unknown. We want to test

\begin{align*}
  H_0: \sigma^2 = \sigma_0^2 \quad \text{vs.} \quad H_1: \sigma^2 \neq \sigma_0^2.
\end{align*}

\end{exercise}

% -------------------------------------------------------------------------------- %

\begin{solution}

Our hypotheses are $\Theta_0 = \{(\mu, \sigma_0^2): \mu \in \R$ and
$\Theta_1 = \{(\mu, \sigma^2): \mu \in \R, \sigma^2 \neq \sigma_0^2\}$.

We first find the general MLE of $(\mu, \sigma)$.

\begin{align*}
  L(\mu, \sigma^2 | \textbf{x}) &= \frac{1}{(\sqrt{2\pi}\sigma)^n}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2\right) \\
  \ell(\mu, \sigma^2 | \textbf{x}) &= \log(L(\sigma,\textbf{x})) 
  = -\frac{n}{2}(\log(2\pi) + \log(\sigma^2)) - \frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}
\end{align*}

We solve the likelihood equations

\begin{align*}
  \frac{\partial}{\partial \mu} \ell(\mu, \sigma^2 | \textbf{x})
  &= \frac{\sum_{i=1}^n (x_i - \mu)}{\sigma^2} \stackrel{!}{=} 0 \\
  \frac{\partial}{\partial \sigma^2} \ell(\mu, \sigma^2 | \textbf{x})
  &= -\frac{n}{2\sigma^2} + \frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^4}
  \stackrel{!}{=} 0
\end{align*}

and obtain

\begin{align*}
  \hat{\mu} &= \bar{X} \\
  \frac{\sum_{i=1}^n (X_i - \bar{X})^2}{2\hat{\sigma}^4} &= \frac{n}{2\hat{\sigma}^2}
  \iff \frac{\sum_{i=1}^n (X_i - \bar{X})^2}{n} = \hat{\sigma}^2.
\end{align*}

The Hesse-Matrix at $(\hat{\mu}, \hat{\sigma}^2)$ reads

\begin{align*}
  \begin{pmatrix}
    -\frac{n}{\hat{\sigma}^2} & 0 \\
    0 & - \frac{n}{2\hat{\sigma}^4}
  \end{pmatrix}
\end{align*}

and since its negative definite, $(\hat{\mu},\hat{\sigma})$ is a local
minimizer of $\ell(\mu,\sigma^2 | \textbf{x})$.
Furthermore it holds 

$\lim_{|\mu| \to \infty} L(\mu, \sigma^2 | \textbf{x}) = 0$
and $\lim_{\sigma^2 \to 0} L(\mu, \sigma^2 | \textbf{x}) = 0$ for any $\sigma^2 > 0$ and $\mu \in \R$
and therefore we conclude that it is indeed our MLE for $(\mu,\sigma^2)$.


The MLE under the restriction of the null hypothesis thus reads

\begin{align*}
  \hat{\sigma}_0^2 &= \sigma_0^2 \\
  \hat{\mu}_0 &= \bar{X}.
\end{align*}

Therefore the generalized likelihood ratio reads

\begin{align*}
  \lambda(x) &= \frac{\sup_{\theta \in \Theta} L(\theta | \textbf{x})}{\sup_{\theta \in \Theta_0} L(\theta | \textbf{x})}
  = \frac{L(\hat{\mu}, \hat{\sigma}^2 | \textbf{x})}{L(\hat{\mu}_0, \hat{\sigma}^2_0, \textbf{x})} 
  = \left(\frac{\sigma_0^2}{\hat{\sigma}^2}\right)^{n/2}\frac{\exp\left(-n\frac{\sum_{i=1}^n(x_i - \bar{x})^2}{2\sum_{i=1}^n (x_i - \bar{x})^2}\right)}
  {\exp\left(-\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{2\sigma_0^2}\right)} \\
  &= \left(\frac{\sigma_0^2}{\hat{\sigma}^2}\right)^{n/2}\exp\left(\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{2\sigma_0^2}-\frac{n}{2}\right) \\
  &= \left(\frac{\sigma_0^2}{\hat{\sigma}^2}\right)^{n/2}\exp\left(\frac{1}{2}\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{\sigma_0^2}-\frac{n}{2}\right) \\
  &= \left(\frac{\sigma_0^2}{\hat{\sigma}^2}\right)^{n/2}\exp\left(\frac{n}{2}\left(\frac{\hat{\sigma}^2}{\sigma_0^2}-1\right)\right) \\
  &= \exp\left(\frac{n}{2}\left(\log\left(\frac{\sigma_0^2}{\hat{\sigma}^2}\right) + \frac{\hat{\sigma}^2}{\sigma_0^2}-1\right)\right),
\end{align*}

which is a non-decreasing function of our test statistic

\begin{align*}
  T(\textbf{X}) = \sum_{i=1}^n \frac{(X_i - \bar{X})^2}{\sigma_0^2} = n\frac{\hat{\sigma}^2}{\sigma_0^2},
\end{align*}

since

\begin{align*}
  f(T(\textbf{x})) &= \exp\left(\frac{n}{2}\left( \log\left(\frac{n}{T(\textbf{x})}\right) +  \frac{T(\textbf{x})}{n} - 1\right)\right), \\
  0 &\stackrel{!}{=} f'(T(\textbf{x})) 
  = \left(-\frac{1}{T(\textbf{x})} + \frac{1}{n}\right)
  \exp\left(\frac{n}{2}\left( \log\left(\frac{n}{T(\textbf{x})}\right) +  
  \frac{T(\textbf{x})}{n} - 1\right)\right) \\
  \iff T(\textbf{x}) &= n \\
  f(T(\textbf{x}) = n) &= \exp\left(\frac{n}{2}\left( \log(1) +  1 - 1\right)\right) = 1.
\end{align*}

Therefore, for $f(T(\textbf{x})) > 1$ the function is monotonously non-decreasing in $T(\textbf{x})$.
We know from the lecture that $T(\textbf{X}) \sim  \chi^2(n-1)$.


Our test now rejects $H_0$ iff $T(\textbf{X}) \geq C$.
For the critical value $C$ we have to solve

\begin{align*}
  \alpha &\stackrel{!}{=} \sup_{(\mu, \sigma^2) \in \Theta_0} \P_{(\mu, \sigma^2)}(T(\textbf{X}) \geq C) \\
  &= \sup_{\mu \in \R} \P_{(\mu, \sigma_0^2)}(T(\textbf{X}) \geq C) \\
  &= 1 - F_{\chi^2}(C) \\
  \iff C &= \chi_{1 - \alpha}^2(n-1).
\end{align*}
\end{solution}

% -------------------------------------------------------------------------------- %

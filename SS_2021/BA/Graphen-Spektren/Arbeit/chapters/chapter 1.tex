\chapter{Einleitung}

    % -------------------------------------------------------------------------------------------------------------------------------- %

    \section{Allgemeines}

        In diesem Kapitel sollen Notationen festgelegt und bekannte Definitionen sowie Ergebnisse in Erinnerung gerufen werden.
        Nachdem Letztere nicht das zentrale Thema dieser Arbeit sind, werden die Beweise größtenteils zitiert. \\

        Wir lassen $0$ als natürliche Zahl zu.
        Das \textit{Kronecker-Delta} für $i, j \in \N$, sei notiert als

        \begin{align*}
            \delta_{i, j}
            :=
            \begin{cases}
                1, & \text{wenn} ~ i = j, \\
                0, & \text{sonst}.
            \end{cases}
        \end{align*}

        % ---------------------------------------------------------------- %

        \subsection{Multi-Mengen}

            \begin{definition}

                Sei $\dom M$ eine endliche Menge.
                Eine \textit{Multimenge} $M$ auf $\dom M$ ist eine Funktion von $\dom M$ in die Natürlichen Zahlen $\N$,

                \begin{align*}
                    M: \dom M \to \N.
                \end{align*}

                Für $x \in \dom M$, nennen wir $M(x)$ dessen \textit{Vielfachheit}.
                $M$ ist eine \textit{echte} Multimenge, wenn ihre Werte-Menge $\ran M$ ein Element mit Vielfachheit ungleich $1$ hat.
                Der \textit{Träger} $\supp M$ von $M$ sei die Menge aller Elemente aus $\dom M$, mit positiver Vielfachheit, i.Z.
                
                \begin{align*}
                    \supp M
                    :=
                    \Bbraces{x \in \dom M: M(x) > 0}.
                \end{align*}

                Die \textit{Mächtigkeit} $|M|$ von $M$ sei die Summe aller Vielfachheiten,

                \begin{align*}
                    |M| := \sum_{x \in \dom M} M(x).
                \end{align*}

            \end{definition}

            Der Hauptzweck hinter Multi-Mengen soll darin bestehen, einen Kompromiss zwischen Mengen und Tupeln zu machen.
            D.h. deren Elemente sollen ungeordnet sein, es muss aber klar sein, wie oft die Elemente vorkommen.
            Wir wollen mit Multi-Mengen aber möglichst so, wie mit Mengen arbeiten können.
            Dazu treffen wir folgende Vereinbarungen:

            \begin{itemize}

                \item Für $x \in \supp M$ schreiben wir auch $x \in M$.

                \item Multi-Mengen werden wir mit eckigen Klammern notieren.
                Ein Beispiel wäre die Multi-Menge
                
                \begin{align*}
                    \Bbraces{(x, 0), (y, 1), (z, 2)}
                    & =
                    M \\
                    & =
                    [y, z, z] = [z, y, z] = [z, y, y] \\
                    & \neq
                    [y, z].
                \end{align*}

                \item Um Fallunterscheidungen zu vermeiden, werden wir Mengen gelegentlich auch mit eckigen statt geschweiften Klammen schreiben.
                
                \item Sollten wir $M$ jemals als Funktion, d.h. als Menge von Paaren betrachten, wird darauf explizit hingewiesen.

            \end{itemize}

        % ---------------------------------------------------------------- %

        \subsection{Vektoren und Matrizen}

            Für $n, m \in \N \setminus \Bbraces{0}$, schreiben wir $(x_i)_{i=1}^n$ für das Tupel $(x_1, \dots, x_n)$ und $(x_{i, j})_{i, j = 1}^{n, m}$ für die Matrix

            \begin{align*}
                \begin{pmatrix}
                    x_{1, 1} & \cdots & x_{1, m} \\
                    \vdots   & \ddots & \vdots   \\
                    x_{n, 1} & \cdots & x_{n, m}
                \end{pmatrix}.
            \end{align*}

            Falls $n = m$, so schreiben wir auch $(x_{i, j})_{i, j = 1}^n$.
            Vektoren und Matrizen sind immer \textbf{fett} gedruckt.
            Die \textit{Einheits-Matrix} und \textit{(konstante) Eins-Matrix}, jeweils der Größe $n \in \N \setminus \Bbraces{0}$, schreiben wir als

            \begin{align*}
                \mathbf I_n := (\delta_{i, j})_{i, j = 1}^n \in \R^{n \times n}
                \quad
                \text{bzw.}
                \quad
                \mathbf J_n := (1)_{i, j = 1}^n \in \R^{n \times n}.
            \end{align*}

            Wenn es aus dem Kontext heraus klar ist, was $n$ ist, lassen wir den Index weg, schreiben also $\mathbf I$ bzw. $\mathbf J$.

            \begin{lemma}

                Sei $\mathbf A \in \R^{n \times n}$ eine symmetrische Matrix, d.h. $\mathbf A = \mathbf A^\top$.
                Dann enthält deren Spektrum $\sigma(\mathbf A) \subseteq \R$ nur reelle Zahlen.

            \end{lemma}

            \begin{proof}

                Sei $\lambda \in \sigma(\mathbf A)$ ein Eigenwert von $\mathbf A$, mit zugehörigem Eigenvektor $x \in \R^n \setminus \Bbraces{0}$.
                $\lambda$ ist zunächst bloß eine Nullstelle des charakteristischen Polynoms $P(\mu) = \det(\mathbf A - \mu \mathbf I_n)$, also $\lambda \in \C$.

                Das Eigenpaar $(\lambda, \mathbf x) \in \C \times \R^n \setminus \Bbraces{0}$ erfüllt aber auch die Gleichung $\mathbf A \mathbf x = \lambda \mathbf x$.
                Weil $\mathbf A$ symmetrisch ist, folgt daraus

                \begin{multline*}
                    \lambda |\mathbf x|^2
                    =
                    \lambda (\mathbf x, \mathbf x)
                    =
                    (\lambda \mathbf x, \mathbf x)
                    =
                    (\mathbf A \mathbf x, \mathbf x)
                    =
                    (\mathbf x, \mathbf A^\top \mathbf x) \\
                    =
                    (\mathbf x, \mathbf A \mathbf x)
                    =
                    (\mathbf x, \lambda \mathbf x)
                    =
                    \overline{(\lambda \mathbf x, \mathbf x)}
                    =
                    \overline \lambda \overline{(\mathbf x, \mathbf x)}
                    =
                    \overline \lambda \overline{|\mathbf x|^2}.
                \end{multline*}

                Dabei bezeichne $|\cdot|$ die Euklidsche Norm und $(\cdot, \cdot)$ das Euklidsche Skalarprodukt.
                Weil $x \neq 0$ als Eigenvektor, ist $|x|^2 \in \R \setminus \Bbraces{0}$ und wir erhalten $\lambda = \overline \lambda$.
                Das geht aber nur für $\lambda \in \R$.

            \end{proof}

            % -------------------------------- %

            \subsubsection{Permutations-Matrizen}

                \begin{definition}
                    
                    Sei $S_n$ die \textit{Symmetrische Gruppe} auf $\Bbraces{1, \dots, n}$.
                    Für eine \textit{Permutation} $\pi \in S_n$ sei $\mathbf P_\pi := (\delta_{\pi(i), j})_{i, j = 1}^n$ die zugehörige \textit{Permutations-Matrix}.
                    Sei $\mathbb P_n := \Bbraces{\mathbf P_\pi: \pi \in S_n}$ die Menge aller Permutations-Matrizen der Größe $n$.
                    Zwei Matrizen $\mathbf A, \mathbf B \in \R^{n \times n}$ heißen \textit{permutations-ähnlich}, wenn es eine Permutations-Matrix $\mathbf P \in \mathbb P_n$ gibt, sodass $\mathbf A = \mathbf P^{-1} \mathbf B \mathbf P$.
                    Die dadurch definierte Relation $\sim_{\mathbb P}$ auf $\R^{n \times n}$ nennen wir \textit{Permutations-Ähnlichkeit}.

                \end{definition}

                \begin{lemma} \label{lem:permutation_matrices}

                    \begin{enumerate}[
                        label = \arabic*.,
                        wide,
                        labelindent = 0pt
                    ]

                        \item Seien $\pi \in S_n$ und $(x_1, \dots, x_n)^\top \mathbf x \in \R^n$, so gilt

                        \begin{align*}
                            \mathbf P_\pi \mathbf x
                            =
                            \begin{pmatrix}
                                x_{\pi(1)} \\ \vdots \\ x_{\pi(n)}
                            \end{pmatrix},
                            \quad
                            \text{und}
                            \quad
                            \mathbf x^\top \mathbf P_\pi
                            =
                            (x_{\pi^{-1}(1)}, \dots, x_{\pi^{-1}(n)}).
                        \end{align*}

                        $\mathbb P_n$ ist eine Untergruppe der orthogonalen Matrizen $\operatorname O_n(\R)$ der Größe $n$.
                        $S_n$ ist anti-isomorph zu $\mathbb P_n$, vermöge $\pi \mapsto \mathbf P_\pi$.

                        \item Die Permutations-Ähnlichkeit ist eine Teil-Äquivalenz-Relation der herkömmlichen \\ Ähnlichkeit $\sim$.
                        Permutations-ähnliche Matrizen haben dasselbe charakteristische Polynom und dieselben Eigenwerte.
                    
                    \end{enumerate}

                \end{lemma}

                \begin{proof}

                    \begin{enumerate}[
                        label = {\texttt{ad} \arabic*.},
                        wide,
                        labelindent = 0pt
                    ]

                        \item Die ersten beiden Aussagen sind elementare Rechnungen.
                        Man kann sie benutzen, um die Homomorphie von $\pi \mapsto \mathbf P_\pi$ nachzuweisen, indem man die Permutations-Matrizen mit $\mathbf x$ testet.
                        Weil $\mathbb P_n$ ein homomorphes Bild der Gruppe $S_n$ folgt, dass es auch eine Gruppe sein muss.
                        Dass Permutations-Matrizen orthogonal sind, d.h. ihre Transponierten sind jeweils ihre Inversen, weist man auch rasch durch testen mit $\mathbf x$ nach.

                        \item Dass $\sim_{\mathbb P}$ eine Äquivalenz-Relation ist, sieht man analog zu $\sim$.
                        Die Reflexivität folgt daraus, dass $\mathbb P_n$ ein neutrales Element hat;
                        die Symmetrie daraus, dass $\mathbb P_n$ unter Inversen-Bildung abgeschlossen ist;
                        und die Transitivität gilt, weil $\mathbb P_n$ unter Verknüpfung abgeschlossen ist.
                        Die letzte Aussage folgt aus der vorletzten und der Tatsache, dass sie für ähnliche Matrizen gilt.

                    \end{enumerate}    

                \end{proof}

                \begin{remark} \label{rem:congruent_sums}
                    
                    Obere letztere Aussage lässt sich noch verfeinern.
                    Seien $\mathbf A_1, \dots, \mathbf A_m \in \R^{n \times n}$ jeweils kongruent zu $\mathbf B_1, \dots, \mathbf B_m \in \R^{n \times n}$ vermöge einer orthogonalen Matrix $\mathbf P \in \operatorname O_n(\R)$.
                    Dann sind auch deren Summen kongruent vermöge $\mathbf P$, denn

                    \begin{align*}
                        \sum_{i=1}^m \mathbf A_i
                        =
                        \sum_{i=1}^m \mathbf P^\top \mathbf B_i \mathbf P
                        =
                        \mathbf P^\top \sum_{i=1}^m \mathbf B_i \mathbf P.
                    \end{align*}

                    Sei nun $\mathbf P = \mathbf P_\pi \in \mathbb P$ sogar eine Permutations-Matrix zur Permutation $\pi \in S_n$.
                    Mit Lemma \ref{lem:permutation_matrices} sieht man, dass

                    \begin{align*}
                        \mathbf P_\pi^\top \mathbf I \mathbf P_\pi = \mathbf I,
                        \quad
                        \text{und}
                        \quad
                        \mathbf P_\pi^\top \mathbf J \mathbf P_\pi = \mathbf J.
                    \end{align*}

                    Seien nun weiters $m_\mathbf{I}, m_\mathbf{J} \in \N \setminus \Bbraces{0}$ und $\lambda_1, \dots, \lambda_{m_\mathbf{I}}, \mu_1, \dots, \mu_{m_\mathbf{J}} \in \R$, so gilt

                    \begin{align*}
                        \sum_{i=1}^m \mathbf A_i
                        +
                        \sum_{i=1}^{m_\mathbf{I}} \lambda_i \mathbf I
                        +
                        \sum_{i=1}^{m_\mathbf{J}} \mu_i \mathbf J
                        & =
                        \sum_{i=1}^m \mathbf P_\pi^\top \mathbf B_i \mathbf P_\pi
                        +
                        \sum_{i=1}^{m_\mathbf{I}} \lambda_i \mathbf P_\pi^\top \mathbf I \mathbf P_\pi
                        +
                        \sum_{i=1}^{m_\mathbf{J}} \mu_i \mathbf P_\pi^\top \mathbf J \mathbf P_\pi \\
                        & =
                        \mathbf P_\pi^\top
                        \pbraces{
                            \sum_{i=1}^m \mathbf B_i
                            +
                            \sum_{i=1}^{m_\mathbf{I}} \lambda_i \mathbf I
                            +
                            \sum_{i=1}^{m_\mathbf{J}} \mu_i \mathbf J
                        }
                        \mathbf P_\pi.
                    \end{align*}

                \end{remark}

            % -------------------------------- %

        % ---------------------------------------------------------------- %

    % -------------------------------------------------------------------------------------------------------------------------------- %

    \section{Graphen}

        \begin{definition} \label{def:graph}

            Als \textit{Graph} bezeichnen wir ein Paar $G = (V, E)$, das aus einer Menge $V$ von \textit{Ecken} und einer Menge $E$ von \textit{Kanten} besteht.
            $V$ sei stets endlich, $G$ also ein \textit{endlicher} Graph.
            $E$ kann Elemente, eines der folgenden Typen, enthalten:

            \begin{enumerate}

                \item $E$ kann aus Paaren $(v, w)$, mit $v, w \in V$, bestehen.
                $G$ nennen wir dann \textit{gerichtet}.
                Kanten der Form $(v, v)$, wobei $v \in V$, sind \textit{(gerichtete) Schleifen}.

                \item $E$ kann auch aus Paar-Mengen $\Bbraces{v, w}$, mit $v, w \in V$, wobei $v \neq w$, bestehen.
                $G$ nennen wir dann \textit{ungerichtet}.

                \item $E$ kann aber auch aus Multi-Mengen $[v, w]$, für $v, w \in V$, bestehen.
                In diesem Fall heißt $G$ ebenfalls \textit{ungerichtet}.
                \textit{(Ungerichtete) Schleifen} seien nun echte Multi-Mengen $[v, v]$ mit $v \in V$.

            \end{enumerate}

            Solche Graphen nennen wir auch \textit{schlicht}.
            $E$ kann eine Multi-Menge sein.
            $G$ ist dann ein (nicht mehr schlichter) \textit{Multi-Graph} und ein Element $e \in E$ eine \textit{Mehrfach-Kante} (oder, wenn $e$ eine Schleife ist, sogar eine \textit{Mehrfach-Schleife}).
            Allgemeiner sei, für eine Funktion $w: E \to M$ in eine Menge $M$ (z.B. $M = \N$), das Tripel $G = (V, E, w)$ ein \textit{gewichteter} Graph mit \textit{Gewichts-Funktion} $w$.

            Typischerweise bezeichnen wir mit $V(G)$ und $E(G)$ die Ecken bzw. Kanten von $G$.
            Falls keine Verwechslungsgefahr besteht, lassen wir $G$ gelegentlich auch weg, schreiben also $V$ bzw. $E$.

        \end{definition}

        \begin{definition}

            Sei $G$ ein (Multi-)Graph.
            Zwei Ecken $v, w \in V(G)$ heißen \textit{adjazent}, wenn sie durch eine Kante $e \in E(G)$ verbunden werden, i.Z.

            \begin{align*}
                v, w ~\text{adjazent}~
                :\iff
                \Exists e \in E(G):
                    \begin{cases}
                        e = (v, w) \lor e = (w, v), & \text{wenn} ~ G ~ \text{gerichtet},   \\
                        e = [v, w],                 & \text{wenn} ~ G ~ \text{ungerichtet}. \\
                    \end{cases}
            \end{align*}

            Eine Ecke $v \in V(G)$ und eine Kante $e \in E(G)$ heißen \textit{inzident}, wenn $e$ nach $v$ hinein- oder von $v$ hinaus-führt, falls $G$ gerichtet, und $v$ auf $e$ liegt, falls $G$ ungerichtet ist, i.Z.

            \begin{align*}
                v, e ~\text{inzident}~
                :\iff
                \begin{cases}
                    \Exists w \in V(G): e = (v, w) \lor e = (w, v), & \text{wenn} ~ G ~ \text{gerichtet},   \\
                    v \in e,                                        & \text{wenn} ~ G ~ \text{ungerichtet}. \\
                \end{cases}
            \end{align*}

        \end{definition}

        \begin{definition}

            Sei $G$ (Multi-)Graph.

            \begin{enumerate}[
                wide,
                labelindent = 0pt
            ]

                \item Sei $G$ weiters gerichtet.
                Die \textit{Adjazenz} zwischen zwei Ecken $v, w \in V(G)$ ist die Vielfachheit der Kanten $e \in E(G)$, die von $v$ nach $w$ führen, i.Z.
        
                \begin{align*}
                    \operatorname{adj}(v, w)
                    :=
                    |\Bbraces{e \in E(G): e = (v, w)}|.
                \end{align*}
        
                Der \textit{Eingangs-Grad} eines Knotens $v \in V(G)$ ist die Anzahl aller Kanten $e \in E(G)$, die in $v$ hineinführen, i.Z.
        
                \begin{align*}
                    \deg^- v
                    :=
                    |\Bbraces{e \in E(G): \Exists w \in V(G): (w, v) = e}|.
                \end{align*}
        
                Der \textit{(Ausgangs-)Grad} eines Knotens $v \in V(G)$ ist die Anzahl aller Kanten $e \in E(G)$, die aus $v$ herausführen, i.Z.
        
                \begin{align*}
                    \deg v
                    :=
                    \deg^+ v
                    :=
                    |\Bbraces{e \in E(G): \Exists w \in V(G): (v, w) = e}|.
                \end{align*}
        
                Die \textit{Inzidenz} eines Knotens $v \in V(G)$ und einer Kante $e \in E(G)$ ist
        
                \begin{align*}
                    \operatorname{inc}(v, e)
                    :=
                    \begin{cases}
                        1, & \text{wenn}~ \Exists w \in V(G): (v, w) = e, \\
                        -1, & \text{wenn}~ \Exists w \in V(G): (w, v) = e, \\
                        0, & \text{sonst}.
                    \end{cases}
                \end{align*}
        
                \item Sei $G$ nun ungerichtet.
                Die \textit{Adjazenz} zwischen zwei Ecken $v, w \in V(G)$ ist die Vielfachheit der Kanten $e \in E(G)$, die diese Verbindet, i.Z.
        
                \begin{align*}
                    \operatorname{adj}(v, w)
                    :=
                    |\Bbraces{e \in E(G): e = [v, w]}|.
                \end{align*}
        
                Der \textit{Grad} eines Knotens $v \in V(G)$ ist die Anzahl aller Kanten $e \in E(G)$, die auf ihm liegen, i.Z.
        
                \begin{align*}
                    \deg v
                    :=
                    |\Bbraces{e \in E(G): v \in e}|.
                \end{align*}
        
                Die \textit{Inzidenz} eines Knotens $v \in V(G)$ und einer Kante $e \in E(G)$ ist
        
                \begin{align*}
                    \operatorname{inc}(v, e)
                    :=
                    \begin{cases}
                        1, & \text{wenn}~ v \in e, \\
                        0, & \text{sonst}.
                    \end{cases}
                \end{align*}    

            \end{enumerate}

            Unabhängig davon, ob $G$ gerichtet oder ungerichtet ist, sei die \textit{Seidel-Adjazenz} zweier Ecken $v, w \in V(G)$
        
            \begin{align*}
                \operatorname{adj}^\text{Seidel}(v, w)
                =
                \begin{cases}
                        1, & \text{wenn}~ v \neq w, ~\text{und}~ v, w ~\text{adjazent}, \\
                    -1, & \text{wenn}~ v \neq w, ~\text{und}~ v, w ~\text{nicht adjazent}, \\
                        0, & \text{sonst}.
                \end{cases}
            \end{align*}

        \end{definition}

    % -------------------------------------------------------------------------------------------------------------------------------- %

    \section{Graphen-Spektren}

        \begin{definition} \label{def:listing}

            Sei $G$ ein (Multi-)Graph.
            Mögen
            
            \begin{align*}
                n = |V(G)|
                \quad
                \text{und}
                \quad
                m = |E(G)|
            \end{align*}
            
            die Anzahl der Knoten bzw. Kanten von $G$ bezeichnen.
            Seien

            \begin{align*}
                f: \Bbraces{1, \dots, n} \to V(G)
                \quad
                \text{und}
                \quad
                g: \Bbraces{1, \dots, m} \to E(G)
            \end{align*}
            
            Bijektionen und $v_i = f(i)$ sowie $e_k = g(k)$, für $i = 1, \dots, n$ bzw. $k = 1, \dots, m$.
            Als \textit{Auflistung} von $V(G)$ bzw. $E(G)$, bezeichnen wir
            
            \begin{align*}
                (v_1, \dots, v_n)
                \quad
                \text{bzw.}
                \quad
                (e_1, \dots, e_m).
            \end{align*}

        \end{definition}

        \begin{definition}

            Wir schließen an Definition \ref{def:listing} direkt an und nennen

            \begin{align*}
                \mathbf A(G, f)    & := (\operatorname{adj}(v_i, v_j))_{i, j = 1}^n               & \text{eine} & ~ \textit{Adjazenz-Matrix,}              \\
                \mathbf D(G, f)    & := \diag (\deg v_i)_{i=1}^n                                  & \text{eine} & ~ \textit{Grad-Matrix,}                  \\
                \mathbf L(G, f)    & := \mathbf D(G, f) - \mathbf A(G, f)                         & \text{eine} & ~ \textit{Laplace-Matrix,}               \\
                \mathbf B(G, f, g) & := (\operatorname{inc}(v_i, e_k))_{i, k = 1}^{n, m}          & \text{eine} & ~ \textit{Inzidenz-Matrix,} ~ \text{und} \\
                \mathbf S(G, f)    & := (\operatorname{adj}^\text{Seidel}(v_i, v_j))_{i, j = 1}^n & \text{eine} & ~ \textit{Seidel-Matrix}
            \end{align*}

            von $G$ unter der(n) Abzählung(en) $f$ (und $g$).

            Die durch die Abzählungen indizierten Familien all jener genannten \textit{Graphen-Matrizen}, bezeichnen wir mit $\mathcal A(G)$, $\mathcal D(G)$, $\mathcal L(G)$, $\mathcal B(G)$, bzw. $\mathcal S(G)$.
            Gelegentlich werden wir diese aber als Mengen auffassen und auf die Indizierung verzichten.

        \end{definition}

        Wenn wir verschiedene Graphen-Matrizen betrachten, und nichts Zusätzliches gesagt wird, sollen jene, die zu selben Graphen gehören, auch derselben Auflistung entspringen.

        \begin{remark} \label{rem:graph_matrix_similarity}

            Offenbar sind Graphen $G$ durch ihre Adjazenz-Matrix eindeutig bestimmt.
            Je nachdem, wie man $V(G)$ aufzählt, bekommt man aber möglicherweise, zum selben Graphen, verschiedene Adjazenz-Matrizen.
            Betrachte die Adjazenz-Matrizen,

            \begin{align*}
                \mathbf A_1 := \mathbf A(G, f_1) \in \mathcal A(G)
                \quad
                \text{und}
                \quad
                \mathbf A_2 :=  \mathbf A(G, f_2) \in \mathcal A(G).
            \end{align*}

            Solche sind immer permutations-ähnlich, vermöge jener Permutations-Matrix $\mathbf P_\pi$, die zur Permutation $\pi$ gehört, die die eine Aufzählung $f_1$ in die andere Aufzählung $f_2$ überführt, i.Z.

            \begin{align*}
                \mathbf A_1 = \mathbf P_\pi^{-1} \mathbf A_2 \mathbf P_\pi,
                \quad
                \pi = f_2 \circ f_1^{-1} \in S_n.
            \end{align*}

            Analoges gilt auch für die übrigen Graphen-Matrizen.

        \end{remark}

        \begin{definition} \label{def:graph_matrices}

            Sei $G$ ein (Multi-)Graph, $\mathbf A \in \mathcal A(G)$, $\mathbf D \in \mathcal D(G)$, und $\mathbf S \in \mathcal S(G)$ beliebige Adjazenz-, Grad-, bzw. Seidel-Matrizen.
            Wir definieren die \textit{charakteristischen Polynome}

            \begin{gather*}
                \chi_P(G, \lambda) := \det(\lambda \mathbf I - \mathbf A),
                \quad
                \chi_Q(G, \lambda) := \frac{1}{\det \mathbf D} \det(\lambda \mathbf D - \mathbf A), \\
                \chi_R(G, \lambda) := \det(\lambda \mathbf I - \mathbf D - \mathbf A),
                \quad
                \text{und}
                \quad
                \chi_S(G, \lambda) := \det(\lambda \mathbf I - \mathbf S).
            \end{gather*}

            Deren Nullstellen-Mengen $\sigma_P(G)$, $\sigma_Q(G)$, $\sigma_R(G)$, bzw. $\sigma_S(G)$ nennen wir \textit{(Graphen-)Spektren} von $G$.
            $P_G$ und $\sigma_P(G)$ bekommen die Namen \textit{gewönliches} charakteristisches Polynom bzw. Spektrum;
            $P_S$ und $\sigma_S(G)$ die Namen \textit{charakteristisches Seidel-Polynom} bzw. \textit{Seidel-Spektrum}.
            Falls keine Verwechslungsgefahr besteht, lassen wir $G$ gelegentlich auch weg, schreiben also $\chi_P(\lambda)$, $\chi_Q(\lambda)$, $\chi_R(\lambda)$ bzw. $\chi_S(\lambda)$.

            Bei der Definition von $Q_G$ setzen wir voraus, dass $\det \mathbf D \neq 0$.
            Laut Bemerkungen \ref{rem:congruent_sums} und \ref{rem:graph_matrix_similarity}, sind diese charakteristischen Polynome damit auch wohldefiniert. 

        \end{definition}

        % ---------------------------------------------------------------- %

        \subsection{Motivation}

            Die Definition \ref{def:graph_matrices} kann man, durch eine geometrische Überlegung, motivieren.

        % ---------------------------------------------------------------- %

    % -------------------------------------------------------------------------------------------------------------------------------- %

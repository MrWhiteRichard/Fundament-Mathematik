% --------------------------------------------------------------------------------

\begin{exercise}[Exercise 3.19]

The value of an action, $q_\pi(s,a)$ depends on the expected next reward and the
expected sum of the remaining rewards. Again we can think of this in terms of a
small backup diagram, this one rooted at an action (state-action pair) and branching
to the possible next states:

\includegraphicsboxed{3.19.png}

Give the equation corresponding to this intuition and diagram for the action value,
$q_\pi(s,a)$, in terms of the expected next reward, $R_{t+1}$, and the expected
next state value, $v_\pi(S_{t+1})$, given that $S_t = s$ and $A_t = a$.
This equation should include an expectation but not one conditioned on following
the policy. Then give a second equation, writing out the expected value explicitly
in terms of $p(s',r|s,a)$ defined by (3.2), such that no expected value notation
appears in the equation.

\end{exercise}

% --------------------------------------------------------------------------------

\begin{solution}

\begin{align*}
  q_\pi(s,a) &= \E_\pi[G_t | S_t = s, A_t = a] = \E_\pi[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
  &= \E[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s, A_t = a] \\
  &= \sum_{r, s'} p(s',r|s,a)\cdot [r + v_\pi(s')]. \\
\end{align*}

\end{solution}

% --------------------------------------------------------------------------------

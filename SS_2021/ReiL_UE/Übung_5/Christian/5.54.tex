% -------------------------------------------------------------------------------- %

\begin{exercise}

Recover a well-known result in stochastic approximation theory gives the conditions for convergence with probability $1$:

\begin{align*}
    \sum_{n=1}^\infty \alpha_n(a) = \infty
    \quad
    \text{and}
    \quad
    \sum_{n=1}^\infty \alpha_n^2(a) < \infty
\end{align*}

Play around with the step-size parameter of any past implementations and observe the impact on convergence and learning speed.
Were the above conditions met for previous examples? What do you notice?
For which types of scenarios constant step sizes are suitable and for which not?

\end{exercise}

% -------------------------------------------------------------------------------- %

\begin{solution}

ToDo!

\end{solution}

% -------------------------------------------------------------------------------- %

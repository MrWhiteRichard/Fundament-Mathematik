{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from random import choices\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on Github: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.reset()\n",
    "#for _ in range(1000):\n",
    "#    env.render()\n",
    "#    env.step(env.action_space.sample()) # take a random action\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    \n",
    "    \"\"\"\n",
    "    Includes the methods for action-selection as well as the three RL Algorithms:\n",
    "    REINFORCE\n",
    "    REINFORCE with baseline\n",
    "    one-step Actor-critic\n",
    "    unfortunately there seem to be some problems with convergence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,environment):\n",
    "        self.env = gym.make(environment)\n",
    "        self.env.reset()\n",
    "        \n",
    "    def select_action(self,theta):\n",
    "        action_list = self.env.action_space\n",
    "        weights = [policy(self.env.state, action, action_list, theta) for action in get_space_list(action_list)]\n",
    "        return choices([action for action in get_space_list(action_list)],weights = weights)[0]\n",
    "        \n",
    "    def Reinforce(self, alpha, gamma, nr_episodes):\n",
    "        theta = np.zeros(len(feature_vector(self.env.state,self.env.action_space.sample())))\n",
    "        q_arr = []\n",
    "        sum_rewards = []\n",
    "        \n",
    "        for k in range(nr_episodes):\n",
    "            q = 0\n",
    "            self.env.reset()\n",
    "            S = [self.env.state]\n",
    "            a = self.select_action(theta)\n",
    "            A = [a]\n",
    "            R = []\n",
    "            new_state, reward, done, info = self.env.step(a)\n",
    "            S.append(new_state)\n",
    "            R.append(reward)\n",
    "            \n",
    "            while not done:\n",
    "                q +=1\n",
    "                a = self.select_action(theta)\n",
    "                A.append(a)\n",
    "                new_state, reward, done, info = self.env.step(a)\n",
    "                S.append(new_state)\n",
    "                R.append(reward)\n",
    "            \n",
    "            sum_rewards.append(sum(R))\n",
    "            q_arr.append(q)\n",
    "            \n",
    "            for t in range(q):\n",
    "                G = sum([gamma**(k-t)*R[k] for k in range(t,q)])\n",
    "                \n",
    "                eligibility_vector = (\n",
    "                    feature_vector(S[t],A[t])-\n",
    "                    sum([policy(S[t],b,self.env.action_space,theta)*feature_vector(S[t],b) for b in get_space_list(self.env.action_space)],0)\n",
    "                )\n",
    "                theta += alpha*(gamma**t)*G*eligibility_vector\n",
    "        \n",
    "        print('average reward = {}'.format(np.average(sum_rewards)))\n",
    "        return theta, sum_rewards\n",
    "    \n",
    "    def Reinforce_baseline(self, alpha_w, alpha_t, gamma, nr_episodes):\n",
    "        theta = np.zeros(len(feature_vector(self.env.state,self.env.action_space.sample())))\n",
    "        w = np.zeros(len(feature_vector(self.env.state)))\n",
    "        q_arr = []\n",
    "        sum_rewards = []\n",
    "        \n",
    "        for k in range(nr_episodes):\n",
    "            q = 0\n",
    "            self.env.reset()\n",
    "            S = [self.env.state]\n",
    "            a = self.select_action(theta)\n",
    "            A = [a]\n",
    "            R = []\n",
    "            new_state, reward, done, info = self.env.step(a)\n",
    "            S.append(new_state)\n",
    "            R.append(reward)\n",
    "            \n",
    "            while not done:\n",
    "                q +=1\n",
    "                a = self.select_action(theta)\n",
    "                A.append(a)\n",
    "                new_state, reward, done, info = self.env.step(a)\n",
    "                S.append(new_state)\n",
    "                R.append(reward)\n",
    "            \n",
    "            sum_rewards.append(sum(R))\n",
    "            q_arr.append(q)\n",
    "            \n",
    "            for t in range(q):\n",
    "                G = sum([gamma**(k-t)*R[k] for k in range(t,q)])\n",
    "                delta = G - w@feature_vector(S[t])\n",
    "                eligibility_vector = (\n",
    "                    feature_vector(S[t],A[t])-\n",
    "                    sum([policy(S[t],b,self.env.action_space,theta)*feature_vector(S[t],b) for b in get_space_list(self.env.action_space)],0)\n",
    "                )\n",
    "                \n",
    "                w += alpha_w*delta*feature_vector(S[t])\n",
    "                theta += alpha_t*delta*(gamma**t)*G*eligibility_vector\n",
    "        \n",
    "        print('average reward = {}'.format(np.average(sum_rewards)))\n",
    "        return theta, sum_rewards\n",
    "    \n",
    "    def actor_critic(self, alpha_w, alpha_t, gamma, nr_episodes):\n",
    "        theta = np.zeros(len(feature_vector(self.env.state,self.env.action_space.sample())))\n",
    "        w = np.zeros(len(feature_vector(self.env.state)))\n",
    "        sum_rewards = []\n",
    "        q_arr = []\n",
    "        \n",
    "        for p in range(nr_episodes):\n",
    "            self.env.reset()\n",
    "            old_state = self.env.state\n",
    "            I = 1\n",
    "            q = 0\n",
    "            R = []\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                a = self.select_action(theta)\n",
    "                new_state, reward, done, info = self.env.step(a)\n",
    "                q += 1\n",
    "                R.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    v_prime = 0\n",
    "                else:\n",
    "                    v_prime = w@feature_vector(new_state)\n",
    "                \n",
    "                eligibility_vector = (\n",
    "                    feature_vector(old_state,a)-\n",
    "                    sum([policy(old_state,b,self.env.action_space,theta)*feature_vector(old_state,b) for b in get_space_list(self.env.action_space)],0)\n",
    "                )\n",
    "                \n",
    "                delta = reward + gamma*v_prime - gamma*w@feature_vector(old_state)\n",
    "                w += alpha_w*delta*feature_vector(old_state)\n",
    "                theta += alpha_t*I*delta*eligibility_vector\n",
    "                I = I*gamma\n",
    "                old_state = new_state\n",
    "            \n",
    "            q_arr.append(q)\n",
    "            sum_rewards.append(sum(R))\n",
    "        \n",
    "        print('average reward = {}'.format(np.average(sum_rewards)))\n",
    "        \n",
    "        return theta, sum_rewards\n",
    "\n",
    "#exponential soft-max distribution\n",
    "def policy(state,action,action_space,theta):\n",
    "    denom = sum([np.exp(theta@feature_vector(state,a)) for a in get_space_list(action_space)])\n",
    "    return np.exp(theta@feature_vector(state,action))/denom   \n",
    "\n",
    "#polynomial feature vectors for both states and state-action pairs, standard degree 2\n",
    "def feature_vector(state, action = None, n = 2):\n",
    "    s = state\n",
    "    \n",
    "    if action == None:\n",
    "        c = np.array(list(itertools.product(range(n), repeat = len(s))))\n",
    "        \n",
    "        return np.array(\n",
    "            [np.prod(\n",
    "                np.array([s[i] for i in range(len(s))])** c_) for c_ in c])\n",
    "    else:\n",
    "        c = np.array(list(itertools.product(range(n), repeat = len(s) + 1)))\n",
    "\n",
    "        return np.array(\n",
    "            [np.prod(\n",
    "                np.array(\n",
    "                    np.append([s[i] for i in range(len(s))],action))\n",
    "                ** c_) for c_ in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space_list(space):\n",
    "\n",
    "    \"\"\"\n",
    "    Converts gym space, constructed from types, to list space_list\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------------- #\n",
    "\n",
    "    types = [\n",
    "        gym.spaces.multi_binary.MultiBinary,\n",
    "        gym.spaces.discrete.Discrete,\n",
    "        gym.spaces.multi_discrete.MultiDiscrete,\n",
    "        gym.spaces.dict.Dict,\n",
    "        gym.spaces.tuple.Tuple,\n",
    "    ]\n",
    "\n",
    "    if type(space) not in types:\n",
    "        raise ValueError(f'input space {space} is not construdted from spaces of types:' + '\\n' + str(types))\n",
    "\n",
    "    # -------------------------------- #\n",
    "\n",
    "    if type(space) is gym.spaces.multi_binary.MultiBinary:\n",
    "        return [\n",
    "            np.reshape(np.array(element), space.n)\n",
    "            for element in itertools.product(\n",
    "                *[range(2)] * np.prod(space.n)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    if type(space) is gym.spaces.discrete.Discrete:\n",
    "        return list(range(space.n))\n",
    "\n",
    "    if type(space) is gym.spaces.multi_discrete.MultiDiscrete:\n",
    "        return [\n",
    "            np.array(element) for element in itertools.product(\n",
    "                *[range(n) for n in space.nvec]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    if type(space) is gym.spaces.dict.Dict:\n",
    "\n",
    "        keys = space.spaces.keys()\n",
    "        \n",
    "        values_list = itertools.product(\n",
    "            *[get_space_list(sub_space) for sub_space in space.spaces.values()]\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            {key: value for key, value in zip(keys, values)}\n",
    "            for values in values_list\n",
    "        ]\n",
    "\n",
    "        return space_list\n",
    "\n",
    "    if type(space) is gym.spaces.tuple.Tuple:\n",
    "        return [\n",
    "            list(element) for element in itertools.product(\n",
    "                *[get_space_list(sub_space) for sub_space in space.spaces]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # -------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = agent(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward = 36.18\n"
     ]
    }
   ],
   "source": [
    "theta_out, rewards = ag.Reinforce(0.1,0.9,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward = 51.26\n"
     ]
    }
   ],
   "source": [
    "theta_2, rewards_2 = ag.Reinforce_baseline(0.1,0.1,0.9,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward = 46.15\n"
     ]
    }
   ],
   "source": [
    "theta_3, rewards_3 = ag.actor_critic(0.1,0.1,0.9,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

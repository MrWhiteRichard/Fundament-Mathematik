% --------------------------------------------------------------------------------

\begin{exercise}[Exercise 3.19]

The value of an action, $q_\pi(s, a)$, depends on the expected next reward and the expected sum of the remaining rewards.
Again we can think of this in terms of a small backup diagram, this one rooted at an action (state-action pair) and branching to the possible next states:

\begin{figure}[H]
    \centering
    \includegraphics[width = 0.5 \textwidth]{2.19.png}
    \caption{}
    \label{fig:2.19}
\end{figure}

Give the equation corresponding to this intuition and diagram for the action value, $q_\pi(s, a)$, in terms of the expected next reward, $R_{t+1}$ , and the expected next state value, $v_\pi(S_{t+1})$, given that $S_t = s$ and $A_t = a$.
This equation should include an expectation but not one conditioned on following the policy.
Then give a second equation, writing out the expected value explicitly in terms of $p(s_0, r \mid s, a)$ defined by \eqref{eq:2.13}, such that no expected value notation appears in the equation.

\end{exercise}

% --------------------------------------------------------------------------------

\begin{solution}

ToDo!

\end{solution}

% --------------------------------------------------------------------------------

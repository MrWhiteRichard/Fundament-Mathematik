% --------------------------------------------------------------------------------

\begin{exercise}[Exercise 3.2]

Is the MDP framework adequate to usefully represent all goal-directed learning tasks?
Can you think of any clear exceptions?

\end{exercise}

% --------------------------------------------------------------------------------

\begin{solution}

Appart from Agent-Environment Interface, a crucial part of the MDP framework ist the \textit{Markov property} ...

\Quote
{
    In a \textit{Markov} decision process, the probabilities given by $p$ completely characterize the environment's dynamics.
    That is, the probability of each possible value for $S_t$ and $R_t$ depends on the immediately preceding state and action, $S_{t-1}$ and $A_{t_1}$, and, given them, not at all on earlier states and actions.
    This is best viewed as a restriction not on the decision process, but on the \textit{state}.
    The state must include information about all aspects of the past agentâ€“environment interaction that make a difference for the future.
    If it does, then the state is said to have the \textit{Markov property}.
}
\cite*[page 49]{SuttonRichardS2018Rl:a}

Various exceptions can be found in \cite{WhiteheadStevenD1995Rlon}.

\end{solution}

% --------------------------------------------------------------------------------

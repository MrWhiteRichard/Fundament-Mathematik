\begin{exercise}
Consider the continuing MDP shown below.
The only decision to be made is that in the top state, where two actions are available, \textbf{left} and \textbf{right}.
The numbers show the rewards that are received deterministically after each action.
There are exactly two deterministic policies, $\pi_\textbf{left}$ and $\pi_\textbf{right}$.
What policy is optimal if $\gamma = 0$?
If $\gamma = 0.9$?
If $\gamma = 0.5$?

\begin{figure}[H]
    \centering
    \includegraphics[width = 0.25 \textwidth]{2.20.png}
    \caption{}
    \label{fig:2.20}
\end{figure}

\end{exercise}

\begin{solution}
1. $\gamma = 0$: Since we discard all future rewards and only consider the immediate reward so we get

\begin{align*}
  v_{\pi_\textbf{right}}(\text{top}) = 0 \qquad
  v_{\pi_\textbf{left}}(\text{top}) = 1
\end{align*}

In both bottom states we get the same reward regardless of the policy we follow,
so in this case the optimal policy would be $\pi_\textbf{left}$.

2. $\gamma = 0.9$: We call the bottom states left and right and with the Bellman equation \eqref{eq:2.14} we get

\begin{align*}
  v_{\pi_\textbf{right}}(\text{top})
  =
  0.9 \cdot v_{\pi_\textbf{right}}(\text{right})
  =
  0.9 \cdot (2 + 0.9 \cdot v_{\pi_\textbf{right}}(\text{top}))
  \implies
  v_{\pi_\textbf{right}}(\text{top}) = \frac{180}{19} \\
  v_{\pi_\textbf{left}}(\text{top})
  =
  1 + (0.9 \cdot v_{\pi_\textbf{left}}(\text{left}))
  =
  1 + 0.81 \cdot v_{\pi_\textbf{left}}(\text{top})
  \implies
  v_{\pi_\textbf{left}}(\text{top}) = \frac{100}{19} \\
\end{align*}

For the bottom states we get:

\begin{align*}
  v_{\pi_\textbf{right}}(\text{right})
  =
  2 + 0.9 \cdot v_{\pi_\textbf{right}}(\text{top})
  =
  10\\
  v_{\pi_\textbf{right}}(\text{left})
  =
  0.9 \cdot v_{\pi_\textbf{right}}(\text{top})
  =
  \frac{162}{19}\\
  v_{\pi_\textbf{left}}(\text{right})
  =
  2 + 0.9 \cdot v_{\pi_\textbf{left}}(\text{top})
  =
  \frac{118}{19} \\
  v_{\pi_\textbf{left}}(\text{left})
  =
  0.9 \cdot v_{\pi_\textbf{left}}(\text{top})
  =
  \frac{90}{19}
\end{align*}
So in this case the optimal policy would be $\pi_\textbf{right}$.

3. $\gamma = 0.5$: With the Bellman equation \eqref{eq:2.14} we get:

\begin{align*}
v_{\pi_\textbf{right}}(\text{top})
=
0.5 \cdot v_{\pi_\textbf{right}}(\text{bottom})
=
0.5 \cdot (2 + 0.5 \cdot v_{\pi_\textbf{right}}(\text{top}))
\implies
v_{\pi_\textbf{right}}(\text{top}) = \frac{4}{3} \\
v_{\pi_\textbf{left}}(\text{top})
=
1 + (0.5 \cdot v_{\pi_\textbf{left}}(\text{bottom}))
=
1 + 0.25 \cdot v_{\pi_\textbf{left}}(\text{top})
\implies
v_{\pi_\textbf{left}}(\text{top}) = \frac{4}{3}
\end{align*}

For the bottom states we get:

\begin{align*}
v_{\pi_\textbf{right}}(\text{right})
=
2 + 0.5 \cdot v_{\pi_\textbf{right}}(\text{top})
=
\frac{8}{3}\\
v_{\pi_\textbf{right}}(\text{left})
=
0.5 \cdot v_{\pi_\textbf{right}}(\text{top})
=
\frac{2}{3}\\
v_{\pi_\textbf{left}}(\text{right})
=
2 + 0.5 \cdot v_{\pi_\textbf{left}}(\text{top})
=
\frac{8}{3} \\
v_{\pi_\textbf{left}}(\text{left})
=
0.5 \cdot v_{\pi_\textbf{left}}(\text{top})
=
\frac{2}{3}
\end{align*}

So in this case both policies are optimal.
\end{solution}

% --------------------------------------------------------------------------------

\begin{exercise}[Exercise 3.2]

Is the MDP framework adequate to usefully represent all goal-directed learning tasks?
Can you think of any clear exceptions?

\end{exercise}

% --------------------------------------------------------------------------------

\begin{solution}

Appart from Agent-Environment Interface, a crucial part of the MDP framework ist the \textit{Markov property} ...

\begin{displayquote}[{\cite*[page 49]{SuttonRichardS2018Rl:a}}]
    In a \textit{Markov} decision process, the probabilities given by $p$ completely characterize the environment's dynamics.
    That is, the probability of each possible value for $S_t$ and $R_t$ depends on the immediately preceding state and action, $S_{t-1}$ and $A_{t_1}$, and, given them, not at all on earlier states and actions.
    This is best viewed as a restriction not on the decision process, but on the \textit{state}.
    The state must include information about all aspects of the past agentâ€“environment interaction that make a difference for the future.
    If it does, then the state is said to have the \textit{Markov property}.    
\end{displayquote}

Consider an agent attempting to learn playing poker against some human opponent that occasionally bluffs (according his internal psychological state).

\begin{itemize}
    \item Actions:
    \enquote{call}, \enquote{raise} (by some amount), or \enquote{fold};
    \item Rewards:
    The amount of money (or game chips) won or lost after each action.
    \item States:
    Everything that a player is allowed to percieve by following the rules of poker!?
    This will be an issue \dots
\end{itemize}

The agent would have to try and approximate the probaility of the human bluffing.
Since modelling human behaviour is inherently non-trivial, this means bad news for fitting this situation in an MDP framework.
Let alone finding all defining all of the parameters that steer the human oppenent towards bluffing is near impossible.
Moreover, human emotions are too complex to accurately model due to the sheer amount of data required.

Other examples, similar to the one just given, may be constructed by introducing a human (unpredictable) component.
More generally, whenever it is not possible to adequately model a situation (e.g. due to the lack of proper sensors) it is refered to as \textit{hidden state tasks}.
More on that can be found in \cite{WhiteheadStevenD1995Rlon}.

\end{solution}

% --------------------------------------------------------------------------------

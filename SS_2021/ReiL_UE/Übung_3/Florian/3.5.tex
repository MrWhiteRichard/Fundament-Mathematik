% --------------------------------------------------------------------------------

\begin{exercise}[Exercise 4.5]

How would policy iteration be defined for action values? Give a complete algorithm
for computing $q_*$, analogous to that on page 80 for computing $v_*$. Please pay
special attention to this exercise, because the ideas involved will be used
throughout the rest of the book.

\end{exercise}

% --------------------------------------------------------------------------------

\begin{solution}

Policy Iteration for action values:

\FloatBarrier

\begin{algorithm}
    \caption{Initialization}
    \begin{algorithmic}[1]
      \State $Q(s,a) \in \R \text{ and } \pi(s) \in \mathcal{A}(s) \text{ arbitrarily for all } s \in \mathcal{S}$
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
    \caption{Policy Evaluation}
    \begin{algorithmic}[1]
      \State $\Delta = +\infty$
      \While{$\Delta >= \theta$}
        \State $\Delta \leftarrow 0$
        \For{$s \in \mathcal{S}$}
          \For{$a \in \mathcal{A}(s)$}
            \State $q \leftarrow Q(s,a)$
            \State $Q(s,a) \leftarrow
            \sum_{s',r} p(s',r|s,a)\left[r + \gamma \sum_{a'} \pi(a'|s')Q(s',a')\right]$
            \State $\Delta \leftarrow \max(\Delta, |q - Q(s,a)|)$
          \EndFor
        \EndFor
      \EndWhile
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{Policy improvement}
    \begin{algorithmic}[1]
      \State policy-stable $\leftarrow$ True
      \For{$s \in \mathcal{S}$}
        \State old-action $\leftarrow \pi(s)$
        \State $\pi(s) \leftarrow \argmax_a Q(s,a)$
        \If{old-action $\neq \pi(s)$}
          \State policy-stable $\leftarrow$ False
        \EndIf
      \EndFor
      \If{policy-stable = True}
        \State \Return $Q \approx q_*$ and $\pi \approx \pi_*$
      \Else
        \State Go to Algorithm 2.
      \EndIf
    \end{algorithmic}
\end{algorithm}

\FloatBarrier
\end{solution}

% --------------------------------------------------------------------------------

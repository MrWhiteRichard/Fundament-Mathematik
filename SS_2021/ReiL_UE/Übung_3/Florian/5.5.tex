% --------------------------------------------------------------------------------

\begin{exercise}[Exercise 6.11]

Why is Q-learning considered an off-policy control method?

\end{exercise}

% --------------------------------------------------------------------------------

\begin{solution}

Because only the observed rewards generated by the behavior policy
contribute to the update formula. The choice of the action $A_{t+1}$
plays no role in that update, as instead, the maximal value over all $a \in \mathcal{A}(S_{t+1})$
is chosen instead.
While the actual behavior policy might be just $\epsilon$-greedy,
Q-learning uses the action-values, which would have been selected
by a 100\% greedy policy regarding the current approximation of $Q$.

\end{solution}

% --------------------------------------------------------------------------------
